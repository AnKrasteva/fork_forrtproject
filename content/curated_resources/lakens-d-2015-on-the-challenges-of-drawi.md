
    {
        "Timestamp":"2020-05-31T18:36:46.879Z",
        "Title":"Lakens, D. (2015). On the challenges of drawing conclusions from p-values just below 0.05. PeerJ, 3, e1142.",
        "link_to_resource":"https:\/\/doi.org\/10.7717\/peerj.1142",
        "Creators":[
            "Daniel Lakens"
        ],
        "Material_Type":[
            "Primary Source",
            "Reading",
            "Paper"
        ],
        "Education_Level":[
            "College \/ Upper Division (Undergraduates)"
        ],
        "Abstract":"In recent years, researchers have attempted to provide an indication of the prevalence of inflated Type 1 error rates by analyzing the distribution of p-values in the published literature. De Winter & Dodou (2015) analyzed the distribution (and its change over time) of a large number of p-values automatically extracted from abstracts in the scientific literature. They concluded there is a \u2018surge of p-values between 0.041\u20130.049 in recent decades\u2019 which \u2018suggests (but does not prove) questionable research practices have increased over the past 25 years.\u2019 I show the changes in the ratio of fractions of p-values between 0.041\u20130.049 over the years are better explained by assuming the average power has decreased over time. Furthermore, I propose that their observation that p-values just below 0.05 increase more strongly than p-values above 0.05 can be explained by an increase in publication bias (or the file drawer effect) over the years (cf. Fanelli, 2012; Pautasso, 2010, which has led to a relative decrease of \u2018marginally significant\u2019 p-values in abstracts in the literature (instead of an increase in p-values just below 0.05). I explain why researchers analyzing large numbers of p-values need to relate their assumptions to a model of p-value distributions that takes into account the average power of the performed studies, the ratio of true positives to false positives in the literature, the effects of publication bias, and the Type 1 error rate (and possible mechanisms through which it has inflated). Finally, I discuss why publication bias and underpowered studies might be a bigger problem for science than inflated Type 1 error rates, and explain the challenges when attempting to draw conclusions about inflated Type 1 error rates from a large heterogeneous set of p-values.",
        "Language":"English",
        "Conditions_of_Use":"I don't see any of these",
        "Primary_User":[
            "Student"
        ],
        "Subject_Areas":[
            "Math & Statistics"
        ],
        "FORRT_Clusters":[
            "Conceptual and Statistical Knowledge"
        ],
        "Tags":[
            ""
        ]
    }
