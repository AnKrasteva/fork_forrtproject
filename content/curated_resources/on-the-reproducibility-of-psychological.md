
    {
        "Timestamp":"2020-05-27T17:17:33.853Z",
        "Title":"On the reproducibility of psychological science",
        "link_to_resource":"https:\/\/doi.org\/10.1080\/01621459.2016.1240079",
        "Creators":[
            "Valen E. Johnson",
            "Richard D. Payne",
            "Tianying Wang",
            "Alex Asher &Soutrik Mandal"
        ],
        "Material_Type":[
            "Primary Source",
            "Reading",
            "Paper"
        ],
        "Education_Level":[
            "College \/ Upper Division (Undergraduates)"
        ],
        "Abstract":"Investigators from a large consortium of scientists recently performed a multi-year study in which they replicated 100 psychology experiments. Although statistically significant results were reported in 97% of the original studies, statistical significance was achieved in only 36% of the replicated studies. This article presents a reanalysis of these data based on a formal statistical model that accounts for publication bias by treating outcomes from unpublished studies as missing data, while simultaneously estimating the distribution of effect sizes for those studies that tested non-null effects. The resulting model suggests that more than 90% of tests performed in eligible psychology experiments tested negligible effects, and that publication biases based on p-values caused the observed rates of nonreproducibility. The results of this reanalysis provide a compelling argument for both increasing the threshold required for declaring scientific discoveries and for adopting statistical summaries of evidence that account for the high proportion of tested hypotheses that are false. Supplementary materials for this article are available online.",
        "Language":"English",
        "Conditions_of_Use":"I don't see any of these",
        "Primary_User":[
            "Student"
        ],
        "Subject_Areas":[
            "Applied Science",
            "Social Science"
        ],
        "FORRT_Clusters":[
            "Reproducibility and Replicability Knowledge",
            "Replication Research"
        ],
        "Tags":[
            "Reproducibility Crisis and Credibility Revolution",
            "Open Science"
        ]
    }
