
    {
        "Timestamp":"2020-06-06T18:53:30.916Z",
        "Title":"Conservative tests under satisficing models of publication bias.",
        "link_to_resource":"https:\/\/doi.org\/10.1371\/journal.pone.0149590",
        "Creators":[
            "Justin McCrary",
            "Garret Christensen",
            "Daniele Fanelli"
        ],
        "Material_Type":[
            "Primary Source",
            "Reading",
            "Paper"
        ],
        "Education_Level":[
            "College \/ Upper Division (Undergraduates)"
        ],
        "Abstract":"Publication bias leads consumers of research to observe a selected sample of statistical estimates calculated by producers of research. We calculate critical values for statistical significance that could help to adjust after the fact for the distortions created by this selection effect, assuming that the only source of publication bias is file drawer bias. These adjusted critical values are easy to calculate and differ from unadjusted critical values by approximately 50%\u2014rather than rejecting a null hypothesis when the t-ratio exceeds 2, the analysis suggests rejecting a null hypothesis when the t-ratio exceeds 3. Samples of published social science research indicate that on average, across research fields, approximately 30% of published t-statistics fall between the standard and adjusted cutoffs.",
        "Language":"English",
        "Conditions_of_Use":"I don't see any of these",
        "Primary_User":[
            "Student"
        ],
        "Subject_Areas":[
            "Applied Science",
            "Social Science"
        ],
        "FORRT_Clusters":[
            "Reproducibility and Replicability Knowledge",
            "Conceptual and Statistical Knowledge",
            "Replication Research"
        ],
        "Tags":[
            "Reproducibility Crisis and Credibility Revolution",
            "Open Science"
        ]
    }
