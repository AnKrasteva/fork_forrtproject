{"summary_1": {"Title": "Trust Your Science? Open Your Data and Code (Stodden, 2011)\u25c8", "Id": "h.4ijq3k12goxa", "Main_Takeaways": ["Computational results suffer from problems of errors in final published conclusions.", "Release scripts and data files, graphical user interface to allow independent replication and reproducible work.", "Repeatability is the sensitivity of results when underlying measurements are re-taken.", "Standards for code quality: More precise definitions of verification, validation, and error quantification in scientific computing.", "Research workflow involves changes made to data, including analysis, that affects data interpretation.", "To conclude, open data is a prerequisite for verifiable research."], "Quote": "\u201cScience has never been about open data per se, but openness is something hard fought and won in the context of reproducibility\u201d (p. 22).", "Abstract": "This is a view on the reproducibility of computational sciences by Victoria Stodden. It contains information on the Reproducibility, Replicability, and Repeatability of code created by the other sciences. Stodden also talks about the rising prominence of computational sciences as we are in the digital age and what that means for the future of science and collecting data.", "Reference": "Stodden, V. C. (2011). Trust your science? Open your data and code. https://doi.org/10.7916/D8CJ8Q0P", "You_may_also_be_interested_in": [{"Relevant_ref": "Attitudes Toward Open Science and Public Data Sharing: A Survey Among Members of the German Psychological Society (Abele-Brehm et al., 2019)", "href": "#h.10dzxpbtal8"}, {"Relevant_ref": "Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011)", "href": "#h.1mu3kuufj5he"}, {"Relevant_ref": "Open Data in Qualitative Research (Chauvette et al., 2019)", "href": "#h.ao8p9ae1wr16"}, {"Relevant_ref": "CJEP Will Offer Open Science Badges (Pexman, 2017)", "href": "#h.qr7alql3zi3k"}, {"Relevant_ref": "Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency (Kidwell et al., 2016)", "href": "#h.ozpykj3tfhom"}, {"Relevant_ref": "Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014)", "href": "#h.6m58pstvoxmx"}, {"Relevant_ref": "Rein in the four horsemen of irreproducibility (Bishop, 2019)", "href": "#h.rrhza31fwxpw"}, {"Relevant_ref": "Seven Easy Steps to Open Science: An Annotated Reading List (Cr\u00fcwell et al., 2019)", "href": "#h.5slx325ilt8s"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "Using OSF to Share Data: A Step-by-Step Guide (Soderberg, 2018)", "href": "#h.1eatzffhrw3j"}]}, "summary_2": {"Title": "Publishing Research With Undergraduate Students via Replication Work: The Collaborative Replications and Education Project (CREP; Wagge et al., 2019)", "Id": "h.1bcavr6huszo", "Main_Takeaways": ["The Collaborative Replications and Education Project (CREP) allows undergraduates to participate in high-quality direct replication, using existing resources and providing structure for research projects.", "CREP samples seminal papers in 9 sub-disciplines published 3 years before the present year. Then alumni students rate papers based on time and level of interest.", "CREP teaches good scientific practices with direct replications using open science methods.", "CREP tells original authors of study selections and asks for materials and guidance for replication.", "The skills acquired from CREP can be applied to non-academic careers. For instance, teaching students the ability to evaluate scientific claims.", "CREP provides a forum and a community for replication results to be presented, the institutionalization of replications, thereby contributing to science.", "Students are invited to contribute to authorship, even if they do not involve lead authorship roles.", "CREP deems that most student projects are not adequately powered for publication, thus do not lead to publication.", "Working with CREP allows students to replicate/not replicate a seminal finding but also to provide them a publication."], "Quote": "\u201cCREP offers a supportive entry point for faculty\u2026new to open science and large-scale collaboration\u2026helps with fidelity and quality checks\u2026eliminates need for instructors to vet every hypothesis and design for student research projects\u2026not be experts in a topic\u2026do not need to learn new programs\u2026documentable experience blending teaching, scholarship, and close mentoring.\u201d (p. 4).", "Abstract": "The Collaborative Replications and Education Project (CREP; http://osf.io/wfc6u) is a framework for undergraduate students to participate in the production of high-quality direct replications. Staffed by volunteers (including the seven authors of this paper) and incorporated into coursework, CREP helps produce high-quality data using existing resources and provides structure for research projects from conceptualization to dissemination. Most notably, student research generated through CREP make an impact: data from these projects are available for meta-analyses, some of which are published with student authors.", "Reference": "Wagge, J. R., Brandt, M. J., Lazarevic, L. B., Legate, N., Christopherson, C., Wiggins, B., & Grahe, J. E. (2019). Publishing research with undergraduate students via replication work: The collaborative replications and education project. Frontiers in psychology, 10, 247. \u00a0https://doi.org/10.3389/fpsyg.2019.00247", "You_may_also_be_interested_in": [{"Relevant_ref": "Is science really facing a reproducibility crisis, and do we need it to? (Fanelli, 2018)", "href": "#h.43r13mkeborf"}, {"Relevant_ref": "Many hands make tight work(Silberzahn & Uhlmann, 2015)", "href": "#h.yc1aiom51ew9"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}]}, "summary_3": {"Title": "Early co-authorship with top scientist predicts success in academic careers (Li et al., 2019)", "Id": "h.rvz88tdscswt", "Main_Takeaways": ["Academic impact is complex and linked to citation number. \u00a0Junior researcher\u2019s output can be provided competitive advantage based on visibility.", "Present study asks whether a single event of interaction with \u2018top scientists\u2019 may alter junior researcher\u2019s future in academia.", "Hypothesis: the more co-authorship with \u2018top scientists\u2019, the more junior researchers have competitive advantage.", "Method: publication and citation data for four disciplines was indexed, since 1970 of selected journals for specific authors and institutions.", "The average prestige score of its authors\u2019 institution + Average prestige score of the researchers\u2019 papers = a paper\u2019s prestige score.", "Results: co-author with top scientists provide competitive advantage compared to peers of comparable early career profiles without top co-authors. Authors seem to suggest that students from less prestigious institutions would benefit junior researchers most.", "Discussion:"], "Abstract": "We examined the long-term impact of coauthorship with established, highly-cited scientists on the careers of junior researchers in four scientific disciplines. Here, using matched pair analysis, we find that junior researchers who coauthor work with top scientists enjoy a persistent competitive advantage throughout the rest of their careers, compared to peers with similar early career profiles but without top coauthors. Such early coauthorship predicts a higher probability of repeatedly coauthoring work with top-cited scientists, and, ultimately, a higher probability of becoming one. Junior researchers affiliated with less prestigious institutions show the most benefits from coauthorship with a top scientist. As a consequence, we argue that such institutions may hold vast amounts of untapped potential, which may be realised by improving access to top scientists.", "Reference": "Li, W., Aste, T., Caccioli, F., & Livan, G. (2019). Early coauthorship with top scientists predicts success in academic careers. Nature communications, 10(1), 1-9. https://doi.org/10.1038/s41467-019-13130-4\u00a0[ungated]", "You_may_also_be_interested_in": [{"Relevant_ref": "Prestige drives epistemic inequality in the diffusion of scientific ideas (Morgan et al., 2018)", "href": "#h.rzwvgpt4pcda"}, {"Relevant_ref": "Open Science Isn\u2019t Always Open to All Scientists (Bahlai et al., 2019)", "href": "#h.b6say8wj1wto"}, {"Relevant_ref": "The Matthew effect in science funding (Bol et al., 2018)", "href": "#h.bbkbxhmwh8ms"}]}, "summary_4": {"Title": "Is science really facing a reproducibility crisis, and do we need it to? (Fanelli, 2018)", "Id": "h.43r13mkeborf", "Main_Takeaways": ["Science is in crisis due to unreliable findings, poor research quality and integrity, publication practices due to pressure to publish.", "Issues of flawed research and publication practice are higher than outright scientific misconduct.", "There are several differences between subfields: magnitude of true effect size, research bias, prior probability, true effects that are false positives and reproducibility of results and inferences.", "There is a strong decline in science such that strong initial findings were later contradicted by later studies.", "It is important to mention that published studies get longer, more complex, richer in data and null findings are placed in these long publications. This is done to remain accessible for researchers who are curious about these findings.", "We are going through a reproducibility crisis due to biased, fabricated, falsified, irreproducible, selective and underpowered findings."], "Quote": "\u201cScience always was and always will be a struggle to produce knowledge for the benefit of all of humanity against the cognitive and moral limitations of individual human beings, including the limitations of scientists themselves.\u201d\u00a0(p.2630)\u201cThe second element of historical novelty is the rising power of information and communication technologies, which are transforming scientific practices in all fields\u2026 to make research more accurate, powerful, open, democratic, transparent, and self-correcting than ever before. At the same time, this technological revolution creates new expectations and new challenges that meta researchers are striving to address.\u201d\u00a0(p.2630)", "Abstract": "Efforts to improve the reproducibility and integrity of science are typically justified by a narrative of crisis, according to which most published results are unreliable due to growing problems with research and publication practices. This article provides an overview of recent evidence suggesting that this narrative is mistaken, and argues that a narrative of epochal changes and empowerment of scientists would be more accurate, inspiring, and compelling.", "Reference": "Fanelli, D. (2018). Opinion: Is science really facing a reproducibility crisis, and do we need it to?. Proceedings of the National Academy of Sciences, 115(11), 2628-2631. https://doi.org/10.1073/pnas.1708272114", "You_may_also_be_interested_in": [{"Relevant_ref": "Publishing Research With Undergraduate Students via Replication Work: The Collaborative Replications and Education Project (Wagge et al., 2019)", "href": "#h.rvz88tdscswt"}, {"Relevant_ref": "Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014)", "href": "#h.6m58pstvoxmx"}, {"Relevant_ref": "Rein in the four horsemen of irreproducibility (Bishop, 2019)", "href": "#h.rrhza31fwxpw"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "On the persistence of low power in psychological science (Vankov et al., 2014)", "href": "#h.9784fcrnhxbi"}]}, "summary_5": {"Title": "Six principles for assessing scientists for hiring, promotion, and tenure (Naudet et al, 2018)", "Id": "h.1j8n5dtwa2fz", "Main_Takeaways": ["Academic work is usually quantified by the quantity of publications. However, this is not a reliable measure.", "An alternative measure is impact factor: the average number of citations to research articles over the preceding two years. This is an imperfect measure that does not capture the ethos of an academic institution.", "Impact factor provides information about citation influence for a few papers but is less informative about an individual publication and the authors involved in the publication (cf. Goodhart\u2019s Law - a valid measurement becomes useless when it becomes an optimisation target).", "Promotions are based on questionable research practices that promote the quantity of publications, but reproducible research does not receive such support.", "The incentive structure in academia is problematic, as the high impact factor is taken to be similar to high societal impact. This is not the case!", "High impact factor leads to more funding, more citations and further funding (cf. Matthew\u2019s Effect), whereas the opposite is observed for papers with low impact factor. Papers with high societal impact seem to fit the papers with low impact factor.", "We need to provide a more inclusive evaluation scheme that allows researchers and research to focus more on open science practices.", "We need to consider societal and broader impact for promotions."], "Abstract": "The negative consequences of relying too heavily on metrics to assess research quality are well known, potentially fostering practices harmful to scientific research such as p-hacking, salami science, or selective reporting. The \"flourish or perish\" culture defined by these metrics in turn drives the system of career advancement in academia, a system that empirical evidence has shown to be problematic and which fails to adequately take societal and broader impact into account. To address this systemic problem,", "Reference": "Naudet, F., Ioannidis, J., Miedema, F., Cristea, I. A., Goodman, S. N., & Moher, D. (2018). Six principles for assessing scientists for hiring, promotion, and tenure. Impact of Social Sciences Blog. http://eprints.lse.ac.uk/90753/", "You_may_also_be_interested_in": [{"Relevant_ref": "The Nine Circles of Scientific Hell (Neuroskeptic, 2012)", "href": "#h.al58h2hjrsil"}, {"Relevant_ref": "Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014)", "href": "#h.6m58pstvoxmx"}, {"Relevant_ref": "A user\u2019s guide to inflated and manipulated impact factor (Ioannidis & Thombs, 2019)", "href": "#h.w1fgusfwh4me"}, {"Relevant_ref": "Publication metrics and success on the academic job market (Van Dijk et al., 2014)", "href": "#h.ugiw8p46vedd"}, {"Relevant_ref": "Rewarding Research Transparency (Gernsbacher, 2018)", "href": "#h.jg9mj2pk5e0m"}, {"Relevant_ref": "Faculty promotion must assess reproducibility (Flier, 2017) \u233a", "href": "#h.c3k3blbo0exl"}]}, "summary_6": {"Title": "Psychologists Are Open to Change, yet Wary of Rules (Fuchs et al., 2012)", "Id": "h.5oevlpnau8wr", "Main_Takeaways": ["How research is conducted and reported by psychologists must change?", "Present study investigated whether psychologists support concrete changes to data collection, reporting and publication processes? If not, what are their reasons?", "Method: 1292 psychologists from 42 countries were surveyed to assess whether each of Simmons et al.\u2019s (2011) requirements and guidelines should be followed as a measure of good practice and whether they should be placed as mandatory conditions for publication in psychological journals.", "Results: \u00a098% of psychologists are open to change and agree at least one requirement should be placed as a condition for publication, especially authors must report all conditions, including failed manipulations.", "Results: Reasons for not including a condition was too rigorous, do not agree with the argument or it was not appropriate for all studies.", "Psychologists are open to change for reporting and conducting research and agree with guidelines. However, some requirements are rigid and questionable."], "Quote": "\u201cResearchers and editorial staff alike must also ensure that standards are enforceable so as to avoid punishing honest researchers. The psychological community should capitalize on the current openness to change in order to develop and implement appropriate changes and thus improve the quality of published psychological research.\u201d (p. 641).", "Abstract": "Psychologists must change the way they conduct and report their research\u2014this notion has been the topic of much debate in recent years. One article recently published in Psychological Science proposing six requirements for researchers concerning data collection and reporting practices as well as four guidelines for reviewers aimed at improving the publication process has recently received much attention (Simmons, Nelson, & Simonsohn, 2011). We surveyed 1,292 psychologists to address two questions: Do psychologists support these concrete changes to data collection, reporting, and publication practices, and if not, what are their reasons? Respondents also indicated the percentage of print and online journal space that should be dedicated to novel studies and direct replications as well as the percentage of published psychological research that they believed would be confirmed if direct replications were conducted. We found that psychologists are generally open to change. Five requirements for researchers and three guidelines for reviewers were supported as standards of good practice, whereas one requirement was even supported as a publication condition. Psychologists appear to be less in favor of mandatory conditions of publication than standards of good practice. We conclude that the proposal made by Simmons, Nelson & Simonsohn (2011) is a starting point for such standards.", "Reference": "Fuchs, H. M., Jenny, M., & Fiedler, S. (2012). Psychologists are open to change, yet wary of rules. Perspectives on Psychological Science, 7(6), 639-642.", "You_may_also_be_interested_in": [{"Relevant_ref": "The Nine Circles of Scientific Hell (Neuroskeptic, 2012)", "href": "#h.al58h2hjrsil"}, {"Relevant_ref": "Six principles for assessing scientists for hiring, promotion, and tenure (Naudet et al, 2018)", "href": "#h.1j8n5dtwa2fz"}, {"Relevant_ref": "CJEP Will Offer Open Science Badges (Pexman, 2017)", "href": "#h.qr7alql3zi3k"}, {"Relevant_ref": "Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency (Kidwell et al., 2016)", "href": "#h.ozpykj3tfhom"}, {"Relevant_ref": "Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014)", "href": "#h.6m58pstvoxmx"}, {"Relevant_ref": "Quality Uncertainty Erodes Trust in Science (Vazire, 2017)", "href": "#h.6gb96p9e79ii"}, {"Relevant_ref": "Rein in the four horsemen of irreproducibility (Bishop, 2019)", "href": "#h.rrhza31fwxpw"}, {"Relevant_ref": "Seven Easy Steps to Open Science: An Annotated Reading List (Cr\u00fcwell et al., 2019)", "href": "#h.5slx325ilt8s"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "Signalling the trustworthiness of science (Jamieson et al., 2020)", "href": "#h.yecsgzczx6oz"}]}, "summary_7": {"Title": "Registered Reports: Realigning incentives in scientific publishing (Chambers et al., 2015)", "Id": "h.4q6lmxfiq5cj", "Main_Takeaways": ["Registered allows peer review to focus on the quality and rigour of the experimental design instead of ground-breaking results. This should reduce questionable research practices such as selective reporting, post-hoc hypothesising and low statistical power.", "Registered reports are reviewed and revised prior to data collection.", "Cortex editorial sub-team triages submissions with one week to reject manuscripts, invite revision to meet necessary standards or send out for Stage 1 in-depth review.", "Stage 1 has 8-10 weeks to move from initial review to in-principle acceptance.", "Stage 2 review has 4 weeks to final editorial decision.", "Registered report is not a one-shot cure for reproducibility problems in science and poses no threat to exploratory analyses."], "Abstract": "This is a view on registered reports in Cortex by Chris Chambers and colleagues. It contains information on Registered Reports and the length of duration for submission and review. They discuss the editorial process and that a registered report is not a threat to exploratory research and is not a panacea to cure reproducibility problems.", "Reference": "Chambers, C. D., Dienes, Z., McIntosh, R. D., Rotshtein, P., & Willmes, K. (2015). Registered reports: realigning incentives in scientific publishing.\u00a0Cortex, 66, A1-A2.", "You_may_also_be_interested_in": [{"Relevant_ref": "Registered Reports: A new publishing initiative at Cortex (Chambers, 2013)", "href": "#h.d0r37aqo7vhl"}, {"Relevant_ref": "Registered Reports: A step change in scientific publishing (Chambers, 2014)", "href": "#h.lopn1u3ro9l1"}, {"Relevant_ref": "Registered reports : a method to increase the credibility of published results (Nosek & Lakens, 2014)", "href": "#h.2azmqwvaq115"}, {"Relevant_ref": "Registered reports (Jamieson et al., 2019)", "href": "#h.sc7lko114tkn"}, {"Relevant_ref": "Rein in the four horsemen of irreproducibility (Bishop, 2019)", "href": "#h.rrhza31fwxpw"}, {"Relevant_ref": "Seven Easy Steps to Open Science: An Annotated Reading List (Cr\u00fcwell et al., 2019)", "href": "#h.5slx325ilt8s"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "On the persistence of low power in psychological science (Vankov et al., 2014)", "href": "#h.9784fcrnhxbi"}]}, "summary_8": {"Title": "Registered Reports: A new\u00a0publishing initiative at Cortex (Chambers, 2013)", "Id": "h.d0r37aqo7vhl", "Main_Takeaways": ["We value novel and eye-catching findings over genuine findings, thus increasing questionable research practices.", "Editorial decisions are one cause of questionable research practices, as they make decisions based on results.", "Science undergraduates are taught about data analysis and hypothesis generation before the data is collected, ensuring the observer is independent of observation.", "Cortex provides registered reports to allow null results and replications be encouraged.", "Registered reports are manuscripts submitted before the experiment begins. This includes the introduction, hypotheses, procedures, analysis pipeline, power analysis and pilot data, if possible.", "Following peer review, the article is rejected or accepted in principle for publication, irrespective of the obtained results.", "Authors have to submit a finalised manuscript for re-review, share raw data and laboratory log.", "Pending quality checks and a sensible interpretation of findings, the manuscript is, in essence, accepted.", "Registered reports are immune to publication bias and need authors to adhere to pre-approved methodology and analysis pipeline to prevent questionable research practices from being used.", "A priori power analysis is required and the criteria for a registered report is seen as providing the highest truth value.", "Registered reports does not exclude exploratory analyses but must be distinguished from the analyses that were planned, also not all modes of scientific investigation fits registered reports."], "Abstract": "This is an editorial by Chris Chambers who encouraged Registered Reports in Cortex as a viable initiative to reduce questionable research practices, its benefits, limitations and what information to include in a registered report.", "Reference": "Chambers, C. D. (2013). Registered reports: a new publishing initiative at Cortex. Cortex, 49(3), 609-610. https://doi.org/10.1016/j.cortex.2012.12.016\u00a0[ungated]", "You_may_also_be_interested_in": [{"Relevant_ref": "Registered Reports: A step change in scientific publishing (Chambers, 2014)", "href": "#h.lopn1u3ro9l1"}, {"Relevant_ref": "Registered Reports: Realigning incentives in scientific publishing (Chambers et al., 2015)", "href": "#h.4q6lmxfiq5cj"}, {"Relevant_ref": "Registered reports : a method to increase the credibility of published results (Nosek & Lakens, 2014)", "href": "#h.2azmqwvaq115"}, {"Relevant_ref": "Registered reports (Jamieson et al., 2019)", "href": "#h.sc7lko114tkn"}, {"Relevant_ref": "Rein in the four horsemen of irreproducibility (Bishop, 2019)", "href": "#h.rrhza31fwxpw"}, {"Relevant_ref": "Seven Easy Steps to Open Science: An Annotated Reading List (Cr\u00fcwell et al., 2019)", "href": "#h.5slx325ilt8s"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "On the persistence of low power in psychological science (Vankov et al., 2014)", "href": "#h.9784fcrnhxbi"}]}, "summary_9": {"Title": "Registered Reports: A step change in scientific publishing (Chambers, 2014)", "Id": "h.lopn1u3ro9l1", "Main_Takeaways": ["Registered reports foster clarity and replication prior to experiments being conducted.", "Readers feel more confident that work is replicable with initial study predictions and analysis plans were independently reviewed.", "Registered reports are a departure from peer review.", "Low power, high rate of cherry picking, post-hoc hypothesising, lack of data sharing, journal culture marked by publication bias and few replication studies contributes to reproducibility crisis.", "Allows us to publish positive, negative or null findings, thus producing a true picture of the literature.", "We will not suffer from publication bias, as manuscript is worthy of publication, editors and reviewers are driven by quality of methods, as opposed to results.", "Registered reports are not an innovation but closer to restoration-reinvention of publication and peer review mechanisms.", "Registered reports allow creativity, flexibility and reporting of unexpected findings."], "Quote": "\u201cUltimately, it is up to all of us to determine the future of any reform, and if the community continues to support Registered Reports then that future looks promising. Each field that adopts this initiative will be helping to create a scientific literature that is free from publication bias, that celebrates transparency, that welcomes replication as well as novelty, and in which the reported science will be more reproducible.\u201d (p. 3)", "Abstract": "Professor Chris Chambers, Registered Reports Editor of the Elsevier journal Cortex and one of the concept\u2019s founders, on how the initiative combats publication bias.", "Reference": "Chambers, C. (2014). Registered reports: A step change in scientific publishing. Reviewers\u2019 Update. November, 13, 2014. https://www.elsevier.com/reviewers-update/story/innovation-in-publishing/registered-reports-a-step-change-in-scientific-publishing", "You_may_also_be_interested_in": [{"Relevant_ref": "Registered Reports: A new publishing initiative at Cortex (Chambers, 2013)", "href": "#h.d0r37aqo7vhl"}, {"Relevant_ref": "Registered Reports: Realigning incentives in scientific publishing (Chambers et al., 2015)", "href": "#h.4q6lmxfiq5cj"}, {"Relevant_ref": "Registered reports : a method to increase the credibility of published results (Nosek & Lakens, 2014)", "href": "#h.2azmqwvaq115"}, {"Relevant_ref": "Registered reports (Jamieson et al., 2019)", "href": "#h.sc7lko114tkn"}, {"Relevant_ref": "Rein in the four horsemen of irreproducibility (Bishop, 2019)", "href": "#h.rrhza31fwxpw"}, {"Relevant_ref": "Seven Easy Steps to Open Science: An Annotated Reading List (Cr\u00fcwell et al., 2019)", "href": "#h.5slx325ilt8s"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "On the persistence of low power in psychological science (Vankov et al., 2014)", "href": "#h.9784fcrnhxbi"}]}, "summary_10": {"Title": "Registered reports: a method to increase the credibility of published results (Nosek & Lakens, 2014)", "Id": "h.2azmqwvaq115", "Main_Takeaways": ["This editorial discusses the value of pre-registration and replication, as not all articles are published.", "Direct replication adds data that increases the precision of effect size estimate for meta-analytic research. No direct replication, means it is difficult to identify false positives.", "Conceptual replications are more popular than direct replications, as the original operationalised is placed as a phenomenon.", "Direct replication encourages generalisability of effects, providing evidence that indicates the effect was not due to sampling, procedural or contextual error.", "Direct replication produces negative results negative results, thus improving the identification of boundary conditions for real effects.", "The benefit of a registered report is that the feedback provided from peer review on design improves the methodology and can be resubmitted for review and acceptance or rejection based on feedback.", "Successful proposals can be high-powered, high quality and faithful replication designs. This can be all done before the research is conducted.", "Conflict of interest is reduced in order to ensure a fair test, allowing reviewers to focus on methodological quality of research.", "Replication can provide additional questions than answers. Effect sizes can be more genuine, as opposed to being exaggerated as a result of larger sample size.", "Registered reports enable exploratory and confirmatory analyses, but a distinction is required. However, more trust can be placed in confirmatory analyses, as it follows a plan and ensures interpretability of reported p value."], "Quote": "\u201cNo single replication provides the definitive word for or against the reality of an effect, just as no original study provides definitive evidence for it. Original and replication research each provides a piece of accumulating evidence for understanding an effect and the conditions necessary to obtain it. Following this special issue, Social Psychology will publish some commentaries and responses by original and replication authors of their reflections on the inferences from the accumulated data, and questions that could be addressed in follow-up research.\u201d (p. 139)", "Abstract": "Professor Daniel Laken and Professor Brian Nosek provide an editorial on how pre-registration and registered reports can be used for the journal of Social Psychology to increase credibility of individual results and findings.", "Reference": "Nosek, B. A., & Lakens, D. (2014). Registered reports : a method to increase the credibility of published results. Social Psychology, 45(3), 137-141. https://doi.org/10.1027/1864-9335/a000192", "You_may_also_be_interested_in": [{"Relevant_ref": "Registered Reports: A new publishing initiative at Cortex (Chambers, 2013)", "href": "#h.d0r37aqo7vhl"}, {"Relevant_ref": "Registered Reports: A step change in scientific publishing (Chambers, 2014)", "href": "#h.lopn1u3ro9l1"}, {"Relevant_ref": "Registered Reports: Realigning incentives in scientific publishing (Chambers et al., 2015)", "href": "#h.4q6lmxfiq5cj"}, {"Relevant_ref": "Registered reports (Jamieson et al., 2019)", "href": "#h.sc7lko114tkn"}, {"Relevant_ref": "Rein in the four horsemen of irreproducibility (Bishop, 2019)", "href": "#h.rrhza31fwxpw"}, {"Relevant_ref": "Seven Easy Steps to Open Science: An Annotated Reading List (Cr\u00fcwell et al., 2019)", "href": "#h.5slx325ilt8s"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "On the persistence of low power in psychological science (Vankov et al., 2014)", "href": "#h.9784fcrnhxbi"}]}, "summary_11": {"Title": "Registered reports (Jamieson et al., 2019)", "Id": "h.sc7lko114tkn", "Main_Takeaways": ["Stage I article is submitted with introduction, methods, analyses and conclusions of a study before carrying out research.", "Stage I includes the article and a cover letter, confirming all support and approval is in place, timeline for completing this study, statement confirming authors share raw data, digital materials, analysis and statements confirming authors register Stage I article.", "Stage I article includes title page, abstract, introduction, methods and analysis plan.", "Sent to peer review to judge if it is of sufficient quality. Peer reviewers assess importance of research question, introduction, plausibility, \u00a0quality of hypotheses and methodological quality and appropriateness of data analysis plan, validity of inferential conclusions based on data.", "Method includes justification of sample sizes compared to question, description of participants, problems investigated ,a priori justification, procedures to deduce inclusion and exclusion criteria and clear protocol of experimental procedures.", "Data analysis: how data is treated and justified including all pre-processing steps.", "If approved, authors submit a Stage II registered report. Stage II is accepted for publication, if no problems arise if Stage II is consistent with the approved Stage I proposal.", "Stage II provides a complete and final report of the approved Stage I article, which also includes raw data, digital materials and analyses. Stage II focuses on quality of data reported, soundness of conclusions drawn from data and consistency with arguments and reasoning.", "Are data sufficiently resolved to support conclusions? Does the data answer authors\u2019 proposed hypotheses? Does the introduction, analyses match Stage I submission? Any unregistered and post-hoc analyses justified, methodologically sound and informative? Are conclusions consistent with collected data.", "Editor can ask for revisions or reject Stage II articles."], "Abstract": "Professor Randall K. Jamieson provides an editorial on registered reports for the journal Canadian Journal of Psychology and how it works in this specific journal.", "Reference": "Jamieson, R. K., Bodner, G. E., Saint-Aubin, J., & Titone, D. (2019) Editorial: Registered reports. Canadian Journal of Experimental Psychology, 73, 3-4.", "You_may_also_be_interested_in": [{"Relevant_ref": "Registered Reports: A new publishing initiative at Cortex (Chambers, 2013)", "href": "#h.d0r37aqo7vhl"}, {"Relevant_ref": "Registered Reports: A step change in scientific publishing (Chambers, 2014)", "href": "#h.lopn1u3ro9l1"}, {"Relevant_ref": "Registered Reports: Realigning incentives in scientific publishing (Chambers et al., 2015)", "href": "#h.4q6lmxfiq5cj"}, {"Relevant_ref": "Registered reports : a method to increase the credibility of published results (Nosek & Lakens, 2014)", "href": "#h.2azmqwvaq115"}, {"Relevant_ref": "Rein in the four horsemen of irreproducibility (Bishop, 2019)", "href": "#h.rrhza31fwxpw"}, {"Relevant_ref": "Seven Easy Steps to Open Science: An Annotated Reading List (Cr\u00fcwell et al., 2019)", "href": "#h.5slx325ilt8s"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "On the persistence of low power in psychological science (Vankov et al., 2014)", "href": "#h.9784fcrnhxbi"}]}, "summary_12": {"Title": "Don\u2019t let transparency damage science (Lewandowsky & Bishop, 2016)", "Id": "h.9irgzcteicjg", "Main_Takeaways": ["Scientific communities have launched initiatives to increase transparency, open critique and data sharing.", "Good researchers include all perspectives but their openness can be abused by opponents who aim to stall inconvenient research.", "Science is prone to attacks and research requires rigour but also transparency to help responses of scientists and their institutions to correct criticisms.", "Open data and scientists should not regard all requests for data as harassment.", "Researchers should explain why they cannot share their research. Confidentiality issues need to be considered, also researchers need control over how data is used if the participant agrees to the sharing of this data.", "Social media can be used to remove biased, incorrect or misleading information.", "Engagement with critics is a fundamental part of scientific practice, researchers may feel obliged to respond even to trolls but can ignore abusive or illogical critics that make the same points.", "Minor corrections and clarifications after publications should not be seen as a stigma against fellow researchers.", "Publications are living documents with corrigenda are unwelcome but are accepted as part of scientific progress.", "Self-censorship affects academic freedom and discussion. Publication retractions should be reserved for fraud or grave errors.Call of retraction is coming from people who do not like a paper\u2019s conclusion.", "Complaints may undervalue researchers for legal but contentious science. Harassed scientists feel alone. They should not tolerate harassment dependent on race or gender nor if it is based on controversial science.", "Training and support should be used to aid researchers in the ability to cope with harassment."], "Abstract": "Stephan Lewandowsky and Dorothy Bishop explain how the research community should protect its members from harassment, while encouraging the openness that has become essential to science.", "Reference": "Lewandowsky, S., & Bishop, D. (2016). Research integrity: Don't let transparency damage science.\u00a0Nature, 529(7587), 459-461.http://dx.doi.org/10.1038/529459a", "You_may_also_be_interested_in": [{"Relevant_ref": "Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014)", "href": "#h.6m58pstvoxmx"}, {"Relevant_ref": "Rein in the four horsemen of irreproducibility (Bishop, 2019)", "href": "#h.rrhza31fwxpw"}, {"Relevant_ref": "Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018)", "href": "#h.syrrsgeyw2yr"}]}, "summary_13": {"Title": "The Nine Circles of Scientific Hell (Neuroskeptic, 2012)", "Id": "h.al58h2hjrsil", "Main_Takeaways": ["There are nine circles of hell and Neuroskeptic explains which level of hell relates to questionable research practice.", "The first circle: Limbo- not a place of punishment but regret as scientists ignore or encourage scientists by awarding them grants and promotions.", "The second circle: overselling- the scientist exaggerates the importance of their work in order to attain grants or write better papers.", "The third circle: post-hoc storytelling- The scientist fires arrows at random, if a finding is noticed, a demon will explain at length or ramble that it aimed for this precise finding.", "The fourth circle: p-value fishing- obtain the result desired by ensuring that the p\u00a0value is less than .05.", "The fifth circle: creative use of outliers- those who clean their results and exclude any data point without clear, explicit and a priori \u00a0justification.", "The sixth circle: plagiarism- presenting another individual\u2019s work as their own work.", "The seventh circle: non-publication of data-each desk with file drawer stuffed with articles but drawers are locked.", "The eighth circle: partial publication of data - choose which group to chase at random-group is matched for age, gender, height and weight.", "The ninth circle: inventing data- data is made up."], "Abstract": "In the spirit of Dante Alighieri\u2019s Inferno, this paper takes a humorous look at the fate that awaits scientists who sin against best practice.", "Reference": "Neuroskeptic. (2012). The nine circles of scientific hell. Perspectives on Psychological Science, 7(6), 643-644. https://doi.org/10.1177/1745691612459519", "You_may_also_be_interested_in": [{"Relevant_ref": "Psychologists Are Open to Change, yet Wary of Rules (Fuchs et al., 2012)", "href": "#h.5oevlpnau8wr"}, {"Relevant_ref": "Six principles for assessing scientists for hiring, promotion, and tenure (Naudet et al, 2018)", "href": "#h.1j8n5dtwa2fz"}, {"Relevant_ref": "Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014)", "href": "#h.6m58pstvoxmx"}, {"Relevant_ref": "Rein in the four horsemen of irreproducibility (Bishop, 2019)", "href": "#h.rrhza31fwxpw"}]}, "summary_14": {"Title": "Attitudes Toward Open Science and Public Data Sharing: A Survey Among Members of the German Psychological Society (Abele-Brehm et al., 2019)", "Id": "h.10dzxpbtal8", "Main_Takeaways": ["Open science is the idea that scientific knowledge of all kinds should be openly shared: open access to published research, open methodology and open data. Public data sharing is the only topic not discussed.", "We should make data accessible for re-analyses in a secure, reliable and competently managed repository. \u00a0A positive attitude towards open science but reservation whether data sharing will benefit young researchers\u2019 careers.", "The present study investigated the attitude towards open science in general and public data sharing, as attitudes not only contribute to an individual\u2019s research practice but also undergraduate, and postgraduate students, post-doctoral students, colleagues and the wider scientific community.", "Method: 337 people were given scales and open-ended questions with 14 items measured attitudes toward open science and toward public data sharing (e.g. what are the long-term consequences if a researcher shares raw data as part of a publication?).", "Method: Attitudes towards open science were separated into hopes and fears.", "Results: More hopes were related to open science and data sharing attitudes than fears. Both hopes and fears were highest among early-career researchers and lowest among professors. Positive attitudes toward data sharing is reduced by cost/benefit consideration.", "Attitudes towards open science and public data sharing was positive but fears that sharing data may have negative consequences for an individual\u2019s career, specifically if not all researchers participate or research parasites profit from data sharing if incentives remain unchanged.", "Professors exhibited least positive attitudes concerning consequences of open science and cost-benefit ratio of data sharing. In addition, they express less fear and hope linked to public data sharing than pre- and post-doctoral researchers."], "Quote": "\u201cThis is, of course, true, but the idea of OS is transparency, and the question whether transparency and a higher commitment to data sharing and OS practices will eventually decrease QRPs and, thus, increase the robustness and replicability of psychological effects remains to be determined empirically.\u201d (p.259).", "Abstract": "Central values of science are, among others, transparency, verifiability, replicability, and openness. The currently very prominent Open Science (OS) movement supports these values. Among its most important principles are open methodology (comprehensive and useful documentation of methods and materials used), open access to published research output, and open data (making collected data available for re-analyses). We here present a survey conducted among members of the German Psychological Society (N = 337), in which we applied a mixed-methods approach (quantitative and qualitative data) to assess attitudes toward OS in general and toward data sharing more specifically. Attitudes toward OS were distinguished into positive expectations (\u201chopes\u201d) and negative expectations (\u201cfears\u201d). These were uncorrelated. There were generally more hopes associated with OS and data sharing than fears. Both hopes and fears were highest among early career researchers and lowest among professors. The analysis of the open answers revealed that generally positive attitudes toward data sharing (especially sharing of data related to a published article) are somewhat diminished by cost/benefit considerations. The results are discussed with respect to individual researchers\u2019 behavior and with respect to structural changes in the research system.", "Reference": "Abele-Brehm, A. E., Gollwitzer, M., Steinberg, U., & Sch\u00f6nbrodt, F. D. (2019). Attitudes toward open science and public data sharing. Social Psychology, 50, 252-260. https://doi.org/10.1027/1864-9335/a000384", "You_may_also_be_interested_in": [{"Relevant_ref": "Trust Your Science? Open Your Data and Code (Stodden, 2011)", "href": "#h.8iegxoy9v5eq"}, {"Relevant_ref": "Open Data in Qualitative Research (Chauvette et al., 2019)", "href": "#h.ao8p9ae1wr16"}, {"Relevant_ref": "Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014)", "href": "#h.6m58pstvoxmx"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "Using OSF to Share Data: A Step-by-Step Guide (Soderberg, 2018)", "href": "#h.1eatzffhrw3j"}]}, "summary_15": {"Title": "Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011)", "Id": "h.1mu3kuufj5he", "Main_Takeaways": ["The American Psychological Association asks authors to sign a contract that data is available for individuals who wish to re-analyse the data to verify claims put forth in the paper.", "There has been no published research to assess this scenario in reality. The present study examined the willingness to share data for re-analysis linked to strength of evidence and quality of reporting of statistical results.", "Method: Wicherts et al. contacted corresponding authors of 141 papers published in the second half of 2004 in one of the four high-ranking journals published by the American Psychological Association and determined whether the effects of outliers contributed to statistical outliers.", "Method: They included authors from journal of personality and social psychology and journal of experimental psychology: learning, memory and cognition, as authors are more willing to share data than other journals.", "Method: They included tests results that were complete and reported as significant effects.", "Results: Reluctance to share was linked with weaker evidence and higher prevalence of apparent errors to report results. An unwillingness to share data was linked to reporting errors that affected statistical significance.", "The authors seem to suggest that a reluctance to share data was linked to more errors in reporting of results and with weaker evidence.The unwillingness to share data was more pronounced when errors concerned significance.", "Statistically rigorous researchers archive data better and are more attentive to statistical power than less statistically rigorous researchers."], "Quote": "\u201cBest practices in conducting analyses and reporting statistical results involve, for instance, that all co-authors hold copies of the data, and that at least two of the authors independently run all the analyses (as we did in this study). Such double-checks and the possibility for others to independently verify results later should go a long way in dealing with human factors in the conduct of statistical analyses and the reporting of results\u201d (pp.6-7).", "Abstract": "The widespread reluctance to share published research data is often hypothesized to be due to the authors\u2019 fear that reanalysis may expose errors in their work or may produce conclusions that contradict their own. However, these hypotheses have not previously been studied systematically. We related the reluctance to share research data for reanalysis to 1148 statistically significant results reported in 49 papers published in two major psychology journals. We found the reluctance to share data to be associated with weaker evidence (against the null hypothesis of no effect) and a higher prevalence of apparent errors in the reporting of statistical results. The unwillingness to share data was particularly clear when reporting errors had a bearing on statistical significance.Our findings on the basis of psychological papers suggest that statistical results are particularly hard to verify when reanalysis is more likely to lead to contrasting conclusions. This highlights the importance of establishing mandatory data archiving policies.", "Reference": "Wicherts, J. M., Bakker, M., & Molenaar, D. (2011). Willingness to share research data is related to the strength of the evidence and the quality of reporting of statistical results. PloS one, 6(11), e26828. https://doi.org/10.1371/journal.pone.0026828", "You_may_also_be_interested_in": [{"Relevant_ref": "Trust Your Science? Open Your Data and Code (Stodden, 2011)", "href": "#h.8iegxoy9v5eq"}, {"Relevant_ref": "Attitudes Toward Open Science and Public Data Sharing: A Survey Among Members of the German Psychological Society (Abele-Brehm et al., 2019)", "href": "#h.10dzxpbtal8"}, {"Relevant_ref": "Open Data in Qualitative Research (Chauvette et al., 2019)", "href": "#h.ao8p9ae1wr16"}, {"Relevant_ref": "CJEP Will Offer Open Science Badges (Pexman, 2017)", "href": "#h.qr7alql3zi3k"}, {"Relevant_ref": "Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency (Kidwell et al., 2016)", "href": "#h.ozpykj3tfhom"}, {"Relevant_ref": "Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014)", "href": "#h.6m58pstvoxmx"}, {"Relevant_ref": "Rein in the four horsemen of irreproducibility (Bishop, 2019)", "href": "#h.rrhza31fwxpw"}, {"Relevant_ref": "Seven Easy Steps to Open Science: An Annotated Reading List (Cr\u00fcwell et al., 2019)", "href": "#h.5slx325ilt8s"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "Using OSF to Share Data: A Step-by-Step Guide (Soderberg, 2018)", "href": "#h.1eatzffhrw3j"}]}, "summary_16": {"Title": "Constraints on Generality (COG): A Proposed Addition to All Empirical Papers (Simons et al., 2017)", "Id": "h.2c556rf0rgig", "Main_Takeaways": ["When a paper identifies a target population and specifies constraints on generality (COG) of findings, researchers conduct direct replications that sample from the target population, leading to more appropriate tests of reliability of the original claim.", "A COG statement indicates why the sample and target population is representative, justifying why subjects, materials and procedures are representative of broader populations.", "A COG statement does not limit the claim but leads the reader to correctly infer these findings limit to the groups of populations being tested such as undergraduate students.", "A COG statement inspires follow-up studies building on results by testing generality populations not originally tested.", "A COG statement encourages reviewers and editors more receptive to next-step studies to test constraints identified.", "A COG statement should be included in all papers, so editors support manuscripts with well-justified constraint on generality statements explicitly ground claims of generality.", "Editors can evaluate whether claims are sufficiently important to justify publication.", "A COG statement incentivises cumulative follow-up research, leading to greater reliability, influence and increased citations.", "This COG statement values rigor, honesty, accuracy and supports the conclusion justified by evidence and theory, allowing readers to understand the limits of generalisability.", "If science was more cumulative and self-correcting, broad generalisation might be justifiable.", "A COG statement describes known or anticipated limits on finding and not mediation by unknown factors. It asks how our sample is representative of a broader population."], "Abstract": "Psychological scientists draw inferences about populations based on samples\u2014of people, situations, and stimuli\u2014from those populations. Yet, few papers identify their target populations, and even fewer justify how or why the tested samples are representative of broader populations. A cumulative science depends on accurately characterizing the generality of findings, but current publishing standards do not require authors to constrain their inferences, leaving readers to assume the broadest possible generalizations. We propose that the discussion section of all primary research articles specify Constraints on Generality (i.e., a \u201cCOG\u201d statement) that identify and justify target populations for the reported findings. Explicitly defining the target populations will help other researchers to sample from the same populations when conducting a direct replication, and it could encourage follow-up studies that test the boundary conditions of the original finding. Universal adoption of COG statements would change publishing incentives to favor a more cumulative science.", "Reference": "Simons, D. J., Shoda, Y., & Lindsay, D. S. (2017). Constraints on generality (COG): A proposed addition to all empirical papers. Perspectives on Psychological Science, 12(6), 1123-1128. \u00a0https://doi.org/10.1177/1745691617708630\u00a0[ungated]", "You_may_also_be_interested_in": [{"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "Most people are not WEIRD (Henrich et al., 2010)", "href": "#h.pxy9j3hosvxg"}]}, "summary_17": {"Title": "Most people are not WEIRD (Henrich et al., 2010)", "Id": "h.pxy9j3hosvxg", "Main_Takeaways": ["Much research on human behaviour is based on Western Educated, Industralised, rich and democratic people (WEIRD).", "They are the most unusual and psychological distinct individuals in the world.", "Most research ignores the importance of generalizability assuming it will be the same across cultures.", "Across cultures, there are differences in terms of perceptual illusions, cultural biases and stereotypes.", "There is a need for cross-cultural evidence in order to have a better understanding of cognition and behaviour."], "Quote": "\u201cRecognizing the full extent of human diversity does not mean giving up on the quest to understand human nature. To the contrary, this recognition illuminates a journey into human nature that is more exciting, more complex, and ultimately more consequential than has previously been suspected\u201d (p.29)", "Abstract": "To understand human psychology, behavioural scientists must stop doing most of their experiments on Westerners, argue Joseph Henrich, Steven J. Heine and Ara Norenzayan.", "Reference": "Henrich, J., Heine, S. & Norenzayan, A. (2010) Most people are not WEIRD. Nature 466, 29. https://doi.org/10.1038/466029a", "You_may_also_be_interested_in": [{"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "Constraints on Generality (COG): A Proposed Addition to All Empirical Papers (Simons et al., 2017)", "href": "#h.2c556rf0rgig"}]}, "summary_18": {"Title": "Open Data in Qualitative Research\u00a0(Chauvette et al., 2019)", "Id": "h.ao8p9ae1wr16", "Main_Takeaways": ["This article argues that as a result of epistemological, methodological, legal and ethical issues, not all qualitative data is appropriate for open access.", "Open data allows researchers to test or refute new theories by validating research findings.", "Although data is becoming more available, we need to consider hotly debated issues concerning open data and that not all data is created equally, especially qualitative research.", "Qualitative research is not equally useful when decontextualized and requires contextualisation. Secondary analyses occur in teams or between collaborators when insider knowledge is shared.", "Qualitative research design is not beneficial to secondary analysis. Researchers become part of the research and may bias the data. Also, preconceptions should not be removed from the analyses.", "Personal knowledge is important for phenomenological research.", "Open data is not captured in transcripts and participants may conduct research to become active contributors to the research process. Field notes are written by researchers.", "Blanket consent form is used that discusses their data is kept indefinitely and reused by anyone. Confidentiality and anonymity becomes an issue for participants with open data.", "This becomes more problematic with small sample sizes, nature of questions, disclosure of information about sensitive issues that may be harmful to the individual and researcher."], "Quote": "\u201cRequirements for data access must consider the uniqueness and context of the data in each qualitative study. Consideration should be given to policies that grant the original research team adequate opportunities for involvement in publication of secondary analyses, perhaps with the rights to authorship to future publications if circumstances warrant. Alternatively, opportunities to comment on the new analysis and interpretation, considering the investigators\u2019 understanding of the unique context of the study, would provide some additional accountability\u201d (p.4).", "Abstract": "There is a growing movement for research data to be accessed, used, and shared by multiple stakeholders for various purposes. The changing technological landscape makes it possible to digitally store data, creating opportunity to both share and reuse data anywhere in the world for later use. This movement is growing rapidly and becoming widely accepted as publicly funded agencies are mandating that researchers open their research data for sharing and reuse. While there are numerous advantages to use of open data, such as facilitating accountability and transparency, not all data are created equally. Accordingly, reusing data in qualitative research present some epistemological, methodological, legal, and ethical issues that must be addressed in the movement toward open data. We examine some of these challenges and make a case that some qualitative research data should not be reused in secondary analysis.", "Reference": "Chauvette, A., Schick-Makaroff, K., & Molzahn, A. E. (2019). Open data in qualitative research. International Journal of Qualitative Methods, 18,\u00a01609406918823863. \u00a0https://doi.org/10.1177/1609406918823863", "You_may_also_be_interested_in": [{"Relevant_ref": "Trust Your Science? Open Your Data and Code (Stodden, 2011)", "href": "#h.8iegxoy9v5eq"}, {"Relevant_ref": "Attitudes Toward Open Science and Public Data Sharing: A Survey Among Members of the German Psychological Society (Abele-Brehm et al., 2019)", "href": "#h.10dzxpbtal8"}, {"Relevant_ref": "Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011)", "href": "#h.1mu3kuufj5he"}, {"Relevant_ref": "Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014)", "href": "#h.6m58pstvoxmx"}]}, "summary_19": {"Title": "How scientists can stop fooling themselves (Bishop, 2020b)", "Id": "h.vmo2rkmiz4ms", "Main_Takeaways": ["Lab scientists should not be allowed to handle dangerous substances without safety training, researchers should not be allowed to be near a p value or similar measure of probability until we demonstrate they understand what it means.", "We ignore contradicting views when confronted with new data and preconceived notions may make us see a structure not there.", "People under-estimate how noisy small samples can be and conduct studies that lack the necessary power to detect an effect.", "More variables investigated, the more likely a significant value is significant.", "Basic statistical training is insufficient or counterproductive, providing misplaced confidence.", "Students discover how easy it is to find false results that are significant via simulation data. Students learn with simulation that small sample sizes are useless to show a moderate difference.", "Researchers need to build lifelong habits to avoid being led astray by specific confirmation bias.", "It is easy to forget papers that counter our own instacts, albeit the papers had no flaws.It enables us to understand the blind spots and how to avoid them."], "Abstract": "Sampling simulated data can reveal common ways in which our cognitive biases mislead us.", "Reference": "Bishop, D. (2020). How scientists can stop fooling themselves over statistics. Nature, 584(7819), 9. https://doi.org/10.1038/d41586-020-02275-8", "You_may_also_be_interested_in": [{"Relevant_ref": "The Statistical Crisis in Science (Gelman & Loken, 2014)", "href": "#h.rc4vbzxkf0ax"}, {"Relevant_ref": "Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014)", "href": "#h.6m58pstvoxmx"}, {"Relevant_ref": "False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011)", "href": "#h.sxh34fk82cfh"}, {"Relevant_ref": "Publication Prejudices: An Experimental Study of Confirmatory Bias in the Peer Review System (Mahoney, 1977)", "href": "#h.e0omhrfflv5j"}, {"Relevant_ref": "Quality Uncertainty Erodes Trust in Science (Vazire, 2017)", "href": "#h.6gb96p9e79ii"}, {"Relevant_ref": "Rein in the four horsemen of irreproducibility (Bishop, 2019)", "href": "#h.rrhza31fwxpw"}, {"Relevant_ref": "Seven Easy Steps to Open Science: An Annotated Reading List (Cr\u00fcwell et al., 2019)", "href": "#h.5slx325ilt8s"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "A consensus-based transparency checklist (Aczel et al., 2020)", "href": "#h.mht7gz1mhdkh"}, {"Relevant_ref": "Tell it like it is (Anon, 2020)", "href": "#h.coptdfafnjrh"}, {"Relevant_ref": "Is pre-registration worthwhile? (Szollosi et al., 2020)", "href": "#h.9ogie999427n"}, {"Relevant_ref": "The life of p: \u201cJust significant\u201d results are on the rise (Leggett et al., 2013)", "href": "#h.b910lefd5hzk"}, {"Relevant_ref": "Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019)", "href": "#h.jwujkwh9t0yi"}]}, "summary_20": {"Title": "CJEP Will Offer Open Science Badges (Pexman, 2017)", "Id": "h.qr7alql3zi3k", "Main_Takeaways": ["Open data- the data is digitally shareable and made publicly available to reproduce results. Information necessary for replications must be included.", "Open materials: all materials necessary to reproduce reported results digitally shareable with descriptions of non-digital materials necessary for replication.", "Pre-registration: provide planned sample size, motivated research questions or hypotheses, outcome and predictor variables, including controls, co-variates and independent variables. This is provided prior to the data being collected.", "Pre-register + analysis: design a pre-register study with an analysis plan for research and results are recorded to plan."], "Quote": "\u201cIndeed, in most cases, authors who wish to apply for badges will do so only after the editorial decision has been made. I understand that there are many reasons why it may not be possible to share data or materials, or to preregister a study, and so I certainly do not expect all authors to apply for badges. Nonetheless, I hope that many authors will devote the time required to make their data, materials, or research plans publicly available; these efforts are an important step toward improving our science.\u201d \u00a0(p.1).", "Abstract": "This is a view on open science badges in Canadian Journal of Psychology by Professor Penny Pexman. It contains information about the badges, \u00a0how to apply for them and that it is not a mandatory requirement.", "Reference": "Pexman, P. M. (2017). CJEP will offer open science badges. Canadian Journal of Experimental Psychology= Revue Canadienne de Psychologie Experimentale, 71(1), 1-1.\u00a0https://doi.org/10.1037/cep0000128", "You_may_also_be_interested_in": [{"Relevant_ref": "Trust Your Science? Open Your Data and Code (Stodden, 2011)", "href": "#h.8iegxoy9v5eq"}, {"Relevant_ref": "Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011)", "href": "#h.1mu3kuufj5he"}, {"Relevant_ref": "Psychologists Are Open to Change, yet Wary of Rules (Fuchs et al., 2012)", "href": "#h.5oevlpnau8wr"}, {"Relevant_ref": "Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency (Kidwell et al., 2016)", "href": "#h.ozpykj3tfhom"}, {"Relevant_ref": "Quality Uncertainty Erodes Trust in Science (Vazire, 2017)", "href": "#h.6gb96p9e79ii"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "Signalling the trustworthiness of science (Jamieson et al., 2020)", "href": "#h.yecsgzczx6oz"}]}, "summary_21": {"Title": "Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency (Kidwell et al., 2016)", "Id": "h.ozpykj3tfhom", "Main_Takeaways": ["Incentives in academia focus on publications and grants. More common are ineffective policies to encourage or require sharing on request.", "Researchers are not likely to share data and materials unless there are incentives. Badges is one means to signal and incentivise desirable behaviours.", "Badges acknowledge open practice signals-journal values transparency, authors met transparency standards for research and immediate signal of accessible data, materials, or pre-registration to readers.", "The present study investigated the influence of adopting badges by comparing data and material sharing rates before badges (i.e. 2012-2013) and after adoption (2014-May 2015) in Psychological Science and across the same time period in comparison journals from the same discipline.", "Method: 2478 articles based on experiment or observations measure data accessibility and research materials. Variables included were open data or open material badge, availability statement of data and material.", "Method: Whether data or materials are available at publicly accessible location. Correct data/material-if data or materials could be retrieved, whether it was linked to what was reported.", "Results: There was an increase of reporting open data after badges were introduced. However reporting openness does not guarantee openness. When badges are earned, available data is provided, correct, usable and complete than when it was not earned.", "Results: Open materials increased but not to the same extent.", "Psychological science adopts badges, report sharing rates increases 10-fold to 40%. Without badges- small percentage of reported sharing is a gross exaggeration of sharing.", "Sharing data was larger when a badge was earned than when it was not earned.", "Effects on sharing research materials were similar sharing data but weaker with badges producing only three times more sharing."], "Quote": "\u201cHowever, actual evidence suggests that this very simple intervention is sufficient to overcome some barriers to sharing data and materials. Badges signal a valued behavior, and the specifications for earning the badges offer simple guides for enacting that behavior. Moreover, the mere fact that the journal engages authors with the possibility of promoting transparency by earning a badge may spur authors to act on their scientific values. Whatever the mechanism, the present results suggest that offering badges can increase sharing by up to an order of magnitude or more. With high return coupled with comparatively little cost, risk, or bureaucratic requirements, what\u2019s not to like?\u201d \u00a0(p.13).", "Abstract": "Beginning January 2014, Psychological Science gave authors the opportunity to signal open data and materials if they qualified for badges that accompanied published articles. Before badges, less than 3% of Psychological Science articles reported open data. After badges, 23% reported open data, with an accelerating trend; 39% reported open data in the first half of 2015, an increase of more than an order of magnitude from baseline. There was no change over time in the low rates of data sharing among comparison journals. Moreover, reporting openness does not guarantee openness. When badges were earned, reportedly available data were more likely to be actually available, correct, usable, and complete than when badges were not earned. Open materials also increased to a weaker degree, and there was more variability among comparison journals. Badges are simple, effective signals to promote open practices and improve preservation of data and materials by using independent repositories.", "Reference": "Kidwell, M. C., Lazarevi\u0107, L. B., Baranski, E., Hardwicke, T. E., Piechowski, S., Falkenberg, L. S., ... & Errington, T. M. (2016). Badges to acknowledge open practices: A simple, low-cost, effective method for increasing transparency. PLoS biology, 14(5), e1002456. https://doi.org/10.1371/journal.pbio.1002456", "You_may_also_be_interested_in": [{"Relevant_ref": "Trust Your Science? Open Your Data and Code (Stodden, 2011)", "href": "#h.8iegxoy9v5eq"}, {"Relevant_ref": "Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011)", "href": "#h.1mu3kuufj5he"}, {"Relevant_ref": "Psychologists Are Open to Change, yet Wary of Rules (Fuchs et al., 2012)", "href": "#h.5oevlpnau8wr"}, {"Relevant_ref": "CJEP Will Offer Open Science Badges (Pexman, 2017)", "href": "#h.qr7alql3zi3k"}, {"Relevant_ref": "Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014)", "href": "#h.6m58pstvoxmx"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "Signalling the trustworthiness of science (Jamieson et al., 2020)", "href": "#h.yecsgzczx6oz"}]}, "summary_22": {"Title": "Signalling the trustworthiness of science should not be a substitute for direct action against research misconduct (Kornfeld & Titus, 2020)", "Id": "h.rgc9h1qzouij", "Main_Takeaways": ["Truth is undermined by misconduct, fraud, failure to replicate and rise in the number of retractions and the public media.", "Fraudulent behaviour does not decrease trust in science.", "Fraudulent behaviour is a result of the fraudulent scientist, not untrustworthy science.", "Reports indicate failure to publish will prevent academic appointment, tenure and ensuring funding of laboratories as main concerns.", "Educating the public about high standards of science and scientists will not reduce outrage concerning fraudulent research."], "Quote": "\u201cWhen then will these leaders of the scientific community finally direct their talents and energy to the culprit per se, research misconduct, and its perpetrators\u201d \u00a0(p.41).", "Abstract": "This is a response to the paper by Jamieson et al. (2019) on signalling trustworthiness in science. It contains information that the trust in science from the public and scientific community contributes to misconduct and fraudulent behaviour.", "Reference": "Kornfeld, D. S., & Titus, S. L. (2020). Signaling the trustworthiness of science should not be a substitute for direct action against research misconduct. Proceedings of the National Academy of Sciences of the United States of America, 117(1), 41. https://doi.org/10.1073/pnas.1917490116", "You_may_also_be_interested_in": [{"Relevant_ref": "Reply to Kornfeld and Titus: No distraction from misconduct (Jamieson et al., 2020)", "href": "#h.1iofh0niziw4"}, {"Relevant_ref": "Stop ignoring misconduct (Kornfeld & Titus, 2016)", "href": "#h.iw4mcqqi8ce9"}, {"Relevant_ref": "Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018)", "href": "#h.syrrsgeyw2yr"}, {"Relevant_ref": "Publication Pressure and Scientific Misconduct in Medical Scientists (Tijdink et al., 2014)", "href": "#h.6mzquzro2mzm"}, {"Relevant_ref": "Signalling the trustworthiness of science (Jamieson et al., 2020)", "href": "#h.yecsgzczx6oz"}]}, "summary_23": {"Title": "Reply to Kornfeld and Titus: No distraction from misconduct (Jamieson et al., 2020)", "Id": "h.1iofh0niziw4", "Main_Takeaways": ["Funders should make research ethics a condition of support.", "Institutions should provide education to investigate misconduct fairly, rapidly and transparently, while protecting whistle-blowers.", "Scientists and outlets that publish their work need to provide methods (e.g. statistical checks, plagiarism checks, badges, checklists) used to honour science\u2019s integrity-protecting norms but when and how they have completed this task.", "These methods should uncover and increase awareness of biases that undermine the ability to fairly interpret their findings.", "These indicators of trustworthiness indicate that the honor of scientific integrity is protected and institutions can protect its integrity but signal how to protect itself."], "Abstract": "This is a response to the commentary by Kornfeld and Titus (2020). It contains information about the importance of research ethics for funders, how institutions should protect whistleblowers and provide education to prevent misconduct and how scientists and outlets can provide evidence they honour scientific integrity.", "Reference": "Jamieson, K. H., McNutt, M., Kiermer, V., & Sever, R. (2020). Reply to Kornfeld and Titus: No distraction from misconduct. Proceedings of the National Academy of Sciences of the United States of America, 117(1), 42. https://doi.org/10.1073/pnas.1918001116", "You_may_also_be_interested_in": [{"Relevant_ref": "Signalling the trustworthiness of science should not be a substitute for direct action against research misconduct (Kornfeld & Titus, 2020)", "href": "#h.rgc9h1qzouij"}, {"Relevant_ref": "Stop ignoring misconduct (Kornfeld & Titus, 2016)", "href": "#h.iw4mcqqi8ce9"}, {"Relevant_ref": "Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018)", "href": "#h.syrrsgeyw2yr"}, {"Relevant_ref": "Publication Pressure and Scientific Misconduct in Medical Scientists (Tijdink et al., 2014)", "href": "#h.6mzquzro2mzm"}, {"Relevant_ref": "Signalling the trustworthiness of science (Jamieson et al., 2020)", "href": "#h.yecsgzczx6oz"}]}, "summary_24": {"Title": "Stop ignoring misconduct (Kornfeld & Titus, 2016)", "Id": "h.iw4mcqqi8ce9", "Main_Takeaways": ["History of science shows irreproducibility is not a product of our times. These problems result from inadequate research practices and fraud. Current initiatives to improve science ignores fraudulent behaviour.", "Reducing irreproducibility is a wasted opportunity, if dishonesty is not being given much attention. Scientific leaders are trying to reduce questionable research practices but choose to ignore, not confront, the issue.", "These ethical issues and practices occurred long before people entered science.", "We need to consider reasons for misconduct: some are perfectionists and unable to cope with failure.", "Funders should craft policies to ensure mentors are advisers, teachers and role models, while limiting the number of trainees per mentor by discipline.", "Established scientists are less likely to commit misconduct if they were more concerned about being detected and punished.", "Whistle-blowers need to come forward and be protected. One method is to provide research integrity officers in the university who will protect them from retaliation.", "Research funds should be given only when current certification is provided by the institution. Those that fail to establish and execute these policies to ensure integrity will be made accountable when misconduct occurs."], "Quote": "\u201cWe believe that these system-wide interventions are essential to have an impact on the irreproducibility produced by research misconduct.\u201d \u00a0(p.30).", "Abstract": "This is an editorial by Kornfeld and Titus (2016) who discusses that misconduct needs to be taken seriously and discussed. It contains solutions to resolve matters concerning research integrity for both the scientist and research institute.", "Reference": "Kornfeld, D. S., & Titus, S. L. (2016). Stop ignoring misconduct. Nature, 537(7618), 29-30.https://doi.org/10.1038/537029a", "You_may_also_be_interested_in": [{"Relevant_ref": "Signalling the trustworthiness of science should not be a substitute for direct action against research misconduct (Kornfeld & Titus, 2020)", "href": "#h.rgc9h1qzouij"}, {"Relevant_ref": "Reply to Kornfeld and Titus: No distraction from misconduct (Jamieson et al., 2020)", "href": "#h.1iofh0niziw4"}, {"Relevant_ref": "Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018)", "href": "#h.syrrsgeyw2yr"}, {"Relevant_ref": "Publication Pressure and Scientific Misconduct in Medical Scientists (Tijdink et al., 2014)", "href": "#h.6mzquzro2mzm"}]}, "summary_25": {"Title": "The Statistical Crisis in Science (Gelman & Loken, 2014)", "Id": "h.rc4vbzxkf0ax", "Main_Takeaways": ["Scientists argued that p\u00a0values are seen as the perceived result of random variation. The value of p\u00a0is a measure of the extent the dataset provides evidence against the null hypothesis.", "It is appropriate to look at data and create rules for data exclusion, coding and analysis to lead to statistical significance. This error is risky in small effect sizes, small sample sizes, large measurement errors and large variability.", "Statistically significant p values cannot be taken at face value even if linked to comparison consistent with existing theory.", "Paper is not published in a high-impact journal without a significant p < .05 result.", "There is a garden of forking paths. Put simply, you make multiple routes and determine this route leads to a significant result but the choices to reach this decision are done implicitly.", "It is good scientific practice to refine one\u2019s research hypotheses in light of the data. However, we need to be aware of data dredging, using both confidence intervals and p values to avoid getting fooled by noise.", "There is an issue of multiple comparisons that emerge as different choices about combining variables, inclusion and exclusion of cases, transformations of variables, tests for interactions in absence of main effects and other steps could occur with different data.", "Pre-registration is practical but cannot be a general solution. Researchers should be made aware of choices involved in data analysis.", "We can perform two experiments: exploratory and confirmatory with its own pre-registered protocol.", "We should move toward an analysis of all data instead of focusing on a single comparison or a small set of comparison."], "Abstract": "Data-dependent analysis\u2014 a \"garden of forking paths\" \u2014 explains why many statistically significant comparisons don't hold up.", "Reference": "Gelman, A., & Loken, E. (2014). The statistical crisis in science: data-dependent analysis--a\" garden of forking paths\"--explains why many statistically significant comparisons don't hold up. American scientist, 102(6), 460-466. [ungated]", "You_may_also_be_interested_in": [{"Relevant_ref": "How scientists can stop fooling themselves (Bishop, 2020b)", "href": "#h.vmo2rkmiz4ms"}, {"Relevant_ref": "Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014)", "href": "#h.6m58pstvoxmx"}, {"Relevant_ref": "False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011)", "href": "#h.sxh34fk82cfh"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "The life of p: \u201cJust significant\u201d results are on the rise (Leggett et al., 2013)", "href": "#h.b910lefd5hzk"}, {"Relevant_ref": "Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019)", "href": "#h.jwujkwh9t0yi"}]}, "summary_26": {"Title": "Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014)", "Id": "h.6m58pstvoxmx", "Main_Takeaways": ["To get an h index for promotion, hiring and funding, there is a publish or perish culture.", "The h index depends on the number of publications, citations and productivity. They are cited due to faults in methodology or lack of replicability? Does this mean citations are a good measure? No! (cf. Goodhart\u2019s Law).", "Academia provides short-term contracts to exploit without wasting resources.", "Publication aims for newsworthy results, leading to false positives and less integration with the literature, leading to only positive findings to be produced with unethical behaviours.", "Questionable research practices are seen as unethically as it distorts data to support the researchers\u2019 hypotheses and scientists are unwilling to self-correct.", "Scientists need to be open about their results. Many scientists subscribe to norm of communality.", "There is data sharing but not many people share their data. Scientists are assumed to self-regulate, but this assumption is erroneous.", "Incentives need to change and focus on quality, reproducibility, data sharing and impact on society.", "Pre-registration can help with publication biases and questionable research practice. Study should be published irrespective of findings.", "Fraud could occur but workload will increase, evaluation of methodology, data collection to evaluate adherence to pre-registration plan.", "Pre-registration could backfire, as editors may require revisions to protocols, study is complete and changes may be impossible.", "Scientists prioritise their own research over scientific inquiry or credibility."], "Quote": "\u201cThe success of science is often attributed to its objectivity: surely science is an impartial, transparent, and dispassionate method for obtaining the truth? In fact, there is growing concern that several aspects of typical scientific practice conflict with these principles and that the integrity of the scientific enterprise has been deeply compromised.\u201d (p.1)", "Abstract": "It is becoming increasingly clear that science has sailed into troubled waters. Recent revelations about cases of serious research fraud and widespread \u2018questionable research practices\u2019 have initiated a period of critical self-reflection in the scientific community and there is growing concern that several common research practices fall far short of the principles of robust scientific inquiry. At a recent symposium, \u2018Improving Scientific Practice: Dealing with the Human Factors\u2019 held at The University of Amsterdam, the notion of the objective, infallible, and dispassionate scientist was firmly challenged. The symposium was guided by the acknowledgement that scientists are only human, and thus subject to the desires, needs, biases, and limitations inherent to the human condition. In this article, five post-graduate students from University College London describe the issues addressed at the symposium and evaluate proposed solutions to the scientific integrity crisis.", "Reference": "Hardwicke, T E et al 2014 Only Human: Scientists, Systems, and Suspect", "You_may_also_be_interested_in": [{"Relevant_ref": "Trust Your Science? Open Your Data and Code (Stodden, 2011)", "href": "#h.8iegxoy9v5eq"}, {"Relevant_ref": "Is science really facing a reproducibility crisis, and do we need it to? (Fanelli, 2018)", "href": "#h.43r13mkeborf"}, {"Relevant_ref": "Six principles for assessing scientists for hiring, promotion, and tenure (Naudet et al, 2018)", "href": "#h.1j8n5dtwa2fz"}, {"Relevant_ref": "The Nine Circles of Scientific Hell (Neuroskeptic, 2012)", "href": "#h.al58h2hjrsil"}, {"Relevant_ref": "Don\u2019t let transparency damage science (Lewandowsky & Bishop, 2016)", "href": "#h.9irgzcteicjg"}, {"Relevant_ref": "Signalling the trustworthiness of science should not be a substitute for direct action against research misconduct (Kornfeld & Titus, 2020)", "href": "#h.rgc9h1qzouij"}, {"Relevant_ref": "Attitudes Toward Open Science and Public Data Sharing: A Survey Among Members of the German Psychological Society (Abele-Brehm et al., 2019)", "href": "#h.10dzxpbtal8"}, {"Relevant_ref": "Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011)", "href": "#h.1mu3kuufj5he"}, {"Relevant_ref": "Open Data in Qualitative Research (Chauvette et al., 2019)", "href": "#h.ao8p9ae1wr16"}, {"Relevant_ref": "How scientists can stop fooling themselves (Bishop, 2020b)", "href": "#h.vmo2rkmiz4ms"}, {"Relevant_ref": "Reply to Kornfeld and Titus: No distraction from misconduct (Jamieson et al., 2020)", "href": "#h.1iofh0niziw4"}, {"Relevant_ref": "Stop ignoring misconduct (Kornfeld & Titus, 2016)", "href": "#h.iw4mcqqi8ce9"}, {"Relevant_ref": "The Statistical Crisis in Science (Gelman & Loken, 2014)", "href": "#h.rc4vbzxkf0ax"}, {"Relevant_ref": "False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011)", "href": "#h.sxh34fk82cfh"}, {"Relevant_ref": "Publication Prejudices: An Experimental Study of Confirmatory Bias in the Peer Review System (Mahoney, 1977)", "href": "#h.e0omhrfflv5j"}, {"Relevant_ref": "Rein in the four horsemen of irreproducibility (Bishop, 2019)", "href": "#h.rrhza31fwxpw"}, {"Relevant_ref": "Seven Easy Steps to Open Science: An Annotated Reading List (Cr\u00fcwell et al., 2019)", "href": "#h.5slx325ilt8s"}, {"Relevant_ref": "Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018)", "href": "#h.syrrsgeyw2yr"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "The life of p: \u201cJust significant\u201d results are on the rise (Leggett et al., 2013)", "href": "#h.b910lefd5hzk"}, {"Relevant_ref": "Signalling the trustworthiness of science (Jamieson et al., 2020)", "href": "#h.yecsgzczx6oz"}, {"Relevant_ref": "Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019)", "href": "#h.jwujkwh9t0yi"}]}, "summary_27": {"Title": "False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011)", "Id": "h.sxh34fk82cfh", "Main_Takeaways": ["Hypotheses are made, data is collected and data is compared to the hypothesis. However, errors will frequently occur in the form of false positives. Null results result from several causes such as a failure to replicate previous research.", "High-impact journals do not discuss the lack of reproducibility and researchers have little incentive to replicate.", "False positives waste resources and inspire investment in research programs and ineffective policy changes.", "Data exclusion can contribute to false positive findings (e.g. why are some observations excluded, why are conditions combined and which condition is being compared, which is the control variable and should measures be combined, transformed or both?)", "We need to consider how outliers are treated. What is too fast and what is too slow (e.g. 1.5/2.5/3SD outlier removal or 1000-5000ms).", "Authors should decide a stopping rule before data collection begins. Power calculations should be reported or recruit as many students as possible prior to the end of the semester.", "Authors should provide cost-of-data-collection justification. Smaller samples reflect interim data and flexible termination rules.", "Authors should provide an exhaustive list on all variables collected in the study.", "Authors should report all experimental conditions, especially the failed manipulations.", "If observations are eliminated, statistical results need to be reported and any eliminations of the data need to be justified.", "If a covariate is included, authors should include analyses with and without the covariate- this is to ensure the results are due to the covariate instead of random assignment.", "Reviewers need to ensure authors follow the requirements and exclude alternative explanations to make sure the findings do not result from chance alone.", "Imperfections in the findings be tolerated by reviewers.", "Reviewers should require authors to show findings not due to an arbitrary analytical decision.", "If justification or data collection is not compelling, reviewers should conduct an exact replication.", "Authors should not be too selective.", "Bayesian approach increases researcher degrees of freedom and offers new analyses and flexibly try out on data."], "Abstract": "In this article, we accomplish two things. First, we show that despite empirical psychologists\u2019 nominal endorsement of a low rate of false-positive findings (\u2264 .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.", "Reference": "Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological science, 22(11), 1359-1366.\u00a0https://doi.org/10.1177/0956797611417632\u00a0[ungated]", "You_may_also_be_interested_in": [{"Relevant_ref": "How scientists can stop fooling themselves (Bishop, 2020b)", "href": "#h.vmo2rkmiz4ms"}, {"Relevant_ref": "The Statistical Crisis in Science (Gelman & Loken, 2014)", "href": "#h.rc4vbzxkf0ax"}, {"Relevant_ref": "Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014)", "href": "#h.6m58pstvoxmx"}, {"Relevant_ref": "A 21 Word Solution (Simmons et al., 2012)", "href": "#h.hxo3saai5nvg"}, {"Relevant_ref": "Rein in the four horsemen of irreproducibility (Bishop, 2019)", "href": "#h.rrhza31fwxpw"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "The life of p: \u201cJust significant\u201d results are on the rise (Leggett et al., 2013)", "href": "#h.b910lefd5hzk"}, {"Relevant_ref": "Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019)", "href": "#h.jwujkwh9t0yi"}]}, "summary_28": {"Title": "Publication Prejudices: An Experimental Study of Confirmatory Bias in the Peer Review System (Mahoney, 1977)", "Id": "h.e0omhrfflv5j", "Main_Takeaways": ["Confirmatory bias is humans seek out experiences to confirm beliefs. Cognitive bias is more prevalent in scientific publication. A piece of research is threatened by human decision making (i.e. the journal editor and reviewer).", "The present study investigated confirmation bias as problems of current review practices.", "To what extent do editors and referees weigh various components in evaluation?", "Although the ideal publication review system might focus on methodological quality and relevance over data outcome and interpretation, writing styles and conclusion also affect editorial decisions.", "What contributes to this review system and how can we reduce confirmatory bias?", "Method: five groups of referees read manuscripts that had data consistent or inconsistent with the reviewer\u2019s theoretical perspective.", "Method:\u00a0Reviewers had to evaluate manuscript based on relevance and methodology.", "Method: two final groups of reviewers received mixed findings, supporting one perspective of the reviewer and the second was contradictory to the reviewer\u2019s perspective.", "Results: There was poor inter-rater reliability. \u00a0Reviewers were more likely to show confirmation bias, thus were more supportive of manuscripts in favour of their theoretical perspective and strongly against manuscripts that contradict their perspective.", "Referees should be asked to evaluate relevance and methodology of an experiment without seeing its results or interpretations (cf. registered reports).", "Referees show little agreement on topics-train them to produce better and unprejudiced consensus. There will be perfect agreement if the same ideological or methodological biases are shared.", "Peer review is seen as an objective measure but ironically is very subjective in nature to biases. We need to investigation peer review and publication policies in detail to assess the transmission of scientific knowledge."], "Abstract": "Confirmatory bias is the tendency to emphasize and believe experiences which support one's views and to ignore or discredit those which do not. The effects of this tendency have been repeatedly documented in clinical research. However, its ramifications for the behavior of scientists have yet to be adequately explored. For example, although publication is a critical element in determining the contribution and impact of scientific findings~ little research attention has been devoted to the variables operative in journal review policies. In the present study, 75 journal reviewers were asked to referee manuscripts which described identical experimental procedures but which reported positive, negative, mixed, or no results. In addition to showing poor interrater agreement, reviewers were strongly biased against manuscripts which reported results contrary to their theoretical perspective. The implications of these findings for epistemology and the peer review system are briefly addressed.", "Reference": "Mahoney, M. J. (1977). Publication prejudices: An experimental study of confirmatory bias in the peer review system. Cognitive therapy and research, 1(2), 161-175. https://doi.org/10.1007/BF01173636", "You_may_also_be_interested_in": [{"Relevant_ref": "How scientists can stop fooling themselves (Bishop, 2020b)", "href": "#h.vmo2rkmiz4ms"}, {"Relevant_ref": "Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014)", "href": "#h.6m58pstvoxmx"}, {"Relevant_ref": "Effect of open peer review on quality of reviews and on reviewers\u2019 recommendations: a randomised trial (van Rooyen et al., 1999)", "href": "#h.nqun53nrd48m"}, {"Relevant_ref": "The Peer Reviewers\u2019 Openness Initiative: incentivising open research practices through peer review (Morey et al., 2016)", "href": "#h.sukgh4wj6thx"}]}, "summary_29": {"Title": "Effect of open peer review on quality of reviews and on reviewers\u2019 recommendations: a randomised trial\u00a0(van Rooyen et al., 1999)", "Id": "h.nqun53nrd48m", "Main_Takeaways": ["The British Medical Journal wants to improve peer review. There was no evidence that investigated whether anonymous peer review is better than other forms of peer review.", "Open peer review (i.e. reviewer signing their review) was argued to produce more effort into their reviews, producing better reviews and increasing credibility and accountability.", "The present article aimed to confirm whether the reviews in open review was the same as traditional review.", "Method: when both reviews were received, manuscript was passed to a responsible editor and second editor randomly chosen from 12 other editors to measure review quality.", "Method: The corresponding author of each manuscript sent anonymous copies of two reviews and was told a decision on the manuscript and a review quality instrument was used to measure the quality of the review.", "Method: The quality of the review measure had 7 items and was based on the means of two editor scores and the corresponding author\u2019s score. The time taken to write the review and reviewer\u2019s recommendation concerning publication: minor or major revision or rejection.", "Results: Twelve percent of reviewers were more likely to decline to review if they were identified than if they were anonymous.", "Results: There was no difference in quality of reviews, difference in recommendation of reviewers or time taken to review the papers for anonymous and identified reviewers.", "Results: The editors\u2019 quality scores for reviews was higher than that of the authors. Most authors support open peer review.", "There was no difference in quality of \u00a0and time taken to produce the review.", "Authors rate reviews that recommend publications higher than those who recommend rejection.", "Editors were not affected by the reviewer\u2019s opinion of the merit of a paper when assessing the quality of the review."], "Abstract": "To examine the effect on peer review of asking reviewers to have their identity revealed to the authors of the paper. Randomised trial. Consecutive eligible papers were sent to two reviewers who were randomised to have their identity revealed to the authors or to remain anonymous. Editors and authors were blind to the intervention. The quality of the reviews was independently rated by two editors and the corresponding author using a validated instrument. Additional outcomes were the time taken to complete the review and the recommendation regarding publication. A questionnaire survey was undertaken of the authors of a cohort of manuscripts submitted for publication to find out their views on open peer review. Two editors' assessments were obtained for 113 out of 125 manuscripts, and the corresponding author's assessment was obtained for 105. Reviewers randomised to be asked to be identified were 12% (95% confidence interval 0.2% to 24%) more likely to decline to review than reviewers randomised to remain anonymous (35% v 23%). There was no significant difference in quality (scored on a scale of 1 to 5) between anonymous reviewers (3.06 (SD 0.72)) and identified reviewers (3.09 (0.68)) (P = 0.68, 95% confidence interval for difference - 0.19 to 0.12), and no significant difference in the recommendation regarding publication or time taken to review the paper. The editors' quality score for reviews (3.05 (SD 0.70)) was significantly higher than that of authors (2.90 (0.87)) (P < 0.005, 95%confidence interval for difference - 0.26 to - 0.03). Most authors were in favour of open peer review. Asking reviewers to consent to being identified to the author had no important effect on the quality of the review, the recommendation regarding publication, or the time taken to review, but it significantly increased the likelihood of reviewers declining to review.", "Reference": "Van Rooyen, S., Godlee, F., Evans, S., Black, N., & Smith, R. (1999). Effect of open peer review on quality of reviews and on reviewers' recommendations: a randomised trial. Bmj, 318(7175), 23-27. https://doi.org/10.1136/bmj.318.7175.23", "You_may_also_be_interested_in": [{"Relevant_ref": "Publication Prejudices: An Experimental Study of Confirmatory Bias in the Peer Review System (Mahoney, 1977)", "href": "#h.e0omhrfflv5j"}, {"Relevant_ref": "The Peer Reviewers\u2019 Openness Initiative: incentivising open research practices through peer review (Morey et al., 2016)", "href": "#h.sukgh4wj6thx"}]}, "summary_30": {"Title": "The Peer Reviewers\u2019 Openness Initiative: incentivising open research practices through peer review (Morey et al., 2016)", "Id": "h.sukgh4wj6thx", "Main_Takeaways": ["Openness and transparency is crucial to science. However, technology limits open science.", "Open data, open materials, open code and better replication studies have accelerated scientific progress.", "Scientific articles allow collaboration to learn what is true instead of the findings based on the analysis.", "Openness is an ethical obligation that provides further advantages, is being seen as a policy change and granting agencies.", "Learning openness is not difficult and implementing them will delay publication even if only for a few days.", "Open practices should be considered by reviewers, as it increases scientific quality.", "The relationship between reviewers and authors are important for the process. A missing figure or statistical results requires the author to be contacted for clarification.", "Authors provide a link of the data and materials and if they do not provide, they must justify this reason. \u00a0If there is no real reason (e.g. legal, ethical or impracticality), reviewers should provide a short review for a lack of openness and failure to justify.", "Documents with details on how to interpret any files or code and how to run software should be made available.", "Joining this initiative will ease review load, if these papers do not meet this requirement, they are rejected.", "Open research is not a matter of policy, but a matter of scientific value and quality of product.", "Open practices are not standardised and are driven by practice. Authors that lack training in open practices and scientists need to learn new skills and knowledge.", "Senior researchers can help students curate data and research materials. Once the student leaves, they can allow people to use the materials.", "Open data allows the reviewer the option to check the analysis.", "Initiative is targeted at reviewers, not action editors. All science needs to be open and researchers who value open research practices should join the Initiative to help promote open research."], "Abstract": "Openness is one of the central values of science. Open scientific practices such as sharing data, materials and analysis scripts alongside published articles have many benefits, including easier replication and extension studies, increased availability of data for theory-building and metaanalysis, and increased possibility of review and collaboration even after a paper has been published. Although modern information technology makes sharing easier than ever before, uptake of open practices had been slow. We suggest this might be in part due to a social dilemma arising from misaligned incentives and propose a specific, concrete mechanism\u2014reviewers withholding comprehensive review\u2014to achieve the goal of creating the expectation of open practices as a matter of scientific principle.", "Reference": "Morey, R. D., Chambers, C. D., Etchells, P. J., Harris, C. R., Hoekstra, R., Lakens, D., ... & Vanpaemel, W. (2016). The Peer Reviewers' Openness Initiative: incentivizing open research practices through peer review.\u00a0Royal Society Open Science, 3(1), 150547.\u00a0https://doi.org/10.1098/rsos.150547", "You_may_also_be_interested_in": [{"Relevant_ref": "Publication Prejudices: An Experimental Study of Confirmatory Bias in the Peer Review System (Mahoney, 1977)", "href": "#h.e0omhrfflv5j"}, {"Relevant_ref": "Effect of open peer review on quality of reviews and on reviewers\u2019 recommendations: a randomised trial (van Rooyen et al., 1999)", "href": "#h.nqun53nrd48m"}]}, "summary_31": {"Title": "A 21 Word Solution\u00a0(Simmons et al., 2012)\u25c8", "Id": "h.hxo3saai5nvg", "Main_Takeaways": ["Scientific journals should require authors to disclose data collection and data analysis.", "False positives need to be scrutinised.", "Say what your sample size was in advance, be transparent and disclose information that you did not drop any variables or conditions.", "We cannot trust our colleagues to run and report studies properly, if some people believe it is okay to drop conditions and variables and others do not believe this is good scientific practice.", "Many forms of p-hacking is also encouraged. We should ask if this is a 1 or 2 dependent variable study.", "Disclosure does not reduce p-hacking and does not reduce probability of false positives. We can maintain red tape by substituting some less vital aspects of style requirement from the American Psychological Association guide with those who propose it.", "Papers should include this proposed 21 words to improve its credibility."], "Quote": "\u201cWe report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study.\u201d (p.1)", "Abstract": "", "Reference": "Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri, A 21 Word Solution (October 14, 2012). Available at SSRN: https://ssrn.com/abstract=2160588 or http://dx.doi.org/10.2139/ssrn.2160588", "You_may_also_be_interested_in": [{"Relevant_ref": "False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011)", "href": "#h.sxh34fk82cfh"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "The life of p: \u201cJust significant\u201d results are on the rise (Leggett et al., 2013)", "href": "#h.b910lefd5hzk"}, {"Relevant_ref": "Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019)", "href": "#h.jwujkwh9t0yi"}]}, "summary_32": {"Title": "Rein in the four horsemen of irreproducibility (Bishop, 2019)", "Id": "h.rrhza31fwxpw", "Main_Takeaways": ["Publication bias harms patients. The tendency not to publish negative results misleads readers.", "Low statistical power misleads readers as well, as a small sample size and manipulation is small, there is low probability one will detect an effect even if it exists.", "Resources and time is wasted for these underpowered studies.", "P-hacking distorts findings-choose a finding that looks exciting and write a paper about it. P values when removed are pointless. These problems are exacerbated in older than junior staff.", "Social media is allowing us to criticise the papers. Most journals are adopting registered reports and funders are encouraging strict guidelines, with data and scripts being made open and methods being fully described."], "Quote": "\u201cSome scientists find this revolution in the name of increased transparency and openness distasteful \u2013 they do not see a problem with the current system, and fear that this movement will undermine the public\u2019s trust in science. I would argue that these scientists have lost touch with what the public expects of science. For many non-scientists, learning that transparency is not the norm in science comes as a surprise. To anyone outside of the power hubs of science, it must seem obvious that scientists should be held to a higher standard than used car salespeople.\u201d (p.3)", "Abstract": "Dorothy Bishop describes how threats to reproducibility, recognized but unaddressed for decades, might finally be brought under control.", "Reference": "Bishop, D. (2019). Rein in the four horsemen of irreproducibility. Nature, 568(7753), 435-436. http://doi.org/10.1038/d41586-019-01307-2", "You_may_also_be_interested_in": [{"Relevant_ref": "Trust Your Science? Open Your Data and Code (Stodden, 2011)", "href": "#h.8iegxoy9v5eq"}, {"Relevant_ref": "Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011)", "href": "#h.1mu3kuufj5he"}, {"Relevant_ref": "Is science really facing a reproducibility crisis, and do we need it to? (Fanelli, 2018)", "href": "#h.43r13mkeborf"}, {"Relevant_ref": "Psychologists Are Open to Change, yet Wary of Rules (Fuchs et al., 2012)", "href": "#h.5oevlpnau8wr"}, {"Relevant_ref": "The Nine Circles of Scientific Hell (Neuroskeptic, 2012)", "href": "#h.al58h2hjrsil"}, {"Relevant_ref": "Don\u2019t let transparency damage science (Lewandowsky & Bishop, 2016)", "href": "#h.9irgzcteicjg"}, {"Relevant_ref": "How scientists can stop fooling themselves (Bishop, 2020b)", "href": "#h.vmo2rkmiz4ms"}, {"Relevant_ref": "Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014)", "href": "#h.6m58pstvoxmx"}, {"Relevant_ref": "False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011)", "href": "#h.sxh34fk82cfh"}, {"Relevant_ref": "Registered Reports: Realigning incentives in scientific publishing (Chambers et al., 2015)", "href": "#h.4q6lmxfiq5cj"}, {"Relevant_ref": "Registered Reports: A new publishing initiative at Cortex (Chambers, 2013)", "href": "#h.d0r37aqo7vhl"}, {"Relevant_ref": "Registered Reports: A step change in scientific publishing (Chambers, 2014)", "href": "#h.lopn1u3ro9l1"}, {"Relevant_ref": "Registered reports: a method to increase the credibility of published results (Nosek & Lakens, 2014)", "href": "#h.2azmqwvaq115"}, {"Relevant_ref": "Registered reports (Jamieson et al., 2019)", "href": "#h.sc7lko114tkn"}, {"Relevant_ref": "Quality Uncertainty Erodes Trust in Science (Vazire, 2017)", "href": "#h.6gb96p9e79ii"}, {"Relevant_ref": "Seven Easy Steps to Open Science: An Annotated Reading List (Cr\u00fcwell et al., 2019)", "href": "#h.5slx325ilt8s"}, {"Relevant_ref": "Promoting an open research culture (Nosek et al., 2015)", "href": "#h.6d082dt7x129"}, {"Relevant_ref": "Promoting Transparency in Social Science Research (Miguel et al., 2014)", "href": "#h.kzom3e53thr1"}, {"Relevant_ref": "Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018)", "href": "#h.syrrsgeyw2yr"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "On the persistence of low power in psychological science (Vankov et al., 2014)", "href": "#h.9784fcrnhxbi"}, {"Relevant_ref": "The life of p: \u201cJust significant\u201d results are on the rise (Leggett et al., 2013)", "href": "#h.b910lefd5hzk"}, {"Relevant_ref": "Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019)", "href": "#h.jwujkwh9t0yi"}]}, "summary_33": {"Title": "Seven Easy Steps to Open Science: An Annotated Reading List (Cr\u00fcwell et al., 2019)", "Id": "h.5slx325ilt8s", "Main_Takeaways": ["Students and academics with little knowledge of open science may not easily find and make use of resources.", "Transparency and robustness may not guarantee increased rigour.", "Readers should plan data collection and analysis, be aware of assumptions of statistical models and understanding of statistical tools.", "Credibility of scientific claims depend on replicability.", "Open access removes barriers to access and distributes research.", "The gold route refers to publicly available articles, while green route relates to self-archiving or the works are made publicly available by people who created the (e.g. preprints).", "Open access articles are cited between 36%-600% more than non-open access work. It is given more coverage and discussed more in non-scientific settings.", "Researchers need to consider how they share their data. Is it findable, accessible, inter-operable and reusable (FAIR)?", "All steps of data analysis should be recorded in open source programs (e.g. R or Python) or placed in a reproducible syntax file.", "Pre-registration is an open science practice protecting people from biases, encourages transparency about analytic decision-making, supporting rigorous scientific research, enabling more replicable and reproducible work.", "Open science increases confidence and replicability of scientific results. Direct replication duplicates necessary elements to produce original findings, whereas conceptual replication changes one component of the original procedure- sample or measure."], "Quote": "\u201cWe hope that this paper will provide researchers interested in open science an accessible entry point to the practices most applicable to their needs. For all of the steps presented in this annotated reading list, any time taken by researchers to understand the issues and develop better practices will be rewarded in orders of magnitude. On an individual level, time and effort are ultimately saved, errors are reduced, and one\u2019s own research is improved through a greater adherence to openness and transparency. On a field-wide level, the more researchers invest in adopting these practices, the closer the field will come toward adhering to scientific norms and the values it claims to espouse.\u201d (p.245)", "Abstract": "The open science movement is rapidly changing the scientific landscape. Because exact definitions are often lacking and reforms are constantly evolving, accessible guides to open science are needed. This paper provides an introduction to open science and related reforms in the form of an annotated reading list of seven peer-reviewed articles, following the format of Etz, Gronau, Dablander, Edelsbrunner, and Baribault (2018). Written for researchers and students \u2013 particularly in psychological science \u2013 it highlights and introduces seven topics: understanding open science; open access; open data, materials, and code; reproducible analyses; preregistration and registered reports; replication research; and teaching open science. For each topic, we provide a detailed summary of one particularly informative and actionable article and suggest several further resources. Supporting a broader understanding of open science issues, this overview should enable researchers to engage with, improve, and implement current open, transparent, reproducible, replicable, and cumulative scientific practices", "Reference": "Cr\u00fcwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., ... & Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science. Zeitschrift f\u00fcr Psychologie. https://doi.org/10.1027/2151-2604/a000387", "You_may_also_be_interested_in": [{"Relevant_ref": "Trust Your Science? Open Your Data and Code (Stodden, 2011)", "href": "#h.8iegxoy9v5eq"}, {"Relevant_ref": "Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011)", "href": "#h.1mu3kuufj5he"}, {"Relevant_ref": "Psychologists Are Open to Change, yet Wary of Rules (Fuchs et al., 2012)", "href": "#h.5oevlpnau8wr"}, {"Relevant_ref": "How scientists can stop fooling themselves (Bishop, 2020b)", "href": "#h.vmo2rkmiz4ms"}, {"Relevant_ref": "Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014)", "href": "#h.6m58pstvoxmx"}, {"Relevant_ref": "Registered Reports: Realigning incentives in scientific publishing (Chambers et al., 2015)", "href": "#h.4q6lmxfiq5cj"}, {"Relevant_ref": "Registered Reports: A new publishing initiative at Cortex (Chambers, 2013)", "href": "#h.d0r37aqo7vhl"}, {"Relevant_ref": "Registered Reports: A step change in scientific publishing (Chambers, 2014)", "href": "#h.lopn1u3ro9l1"}, {"Relevant_ref": "Registered reports: a method to increase the credibility of published results (Nosek & Lakens, 2014)", "href": "#h.2azmqwvaq115"}, {"Relevant_ref": "Registered reports (Jamieson et al., 2019)", "href": "#h.sc7lko114tkn"}, {"Relevant_ref": "Quality Uncertainty Erodes Trust in Science (Vazire, 2017)", "href": "#h.6gb96p9e79ii"}, {"Relevant_ref": "Rein in the four horsemen of irreproducibility (Bishop, 2019)", "href": "#h.rrhza31fwxpw"}, {"Relevant_ref": "Promoting an open research culture (Nosek et al., 2015)", "href": "#h.6d082dt7x129"}, {"Relevant_ref": "Promoting Transparency in Social Science Research (Miguel et al., 2014)", "href": "#h.kzom3e53thr1"}, {"Relevant_ref": "Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018)", "href": "#h.syrrsgeyw2yr"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "Signalling the trustworthiness of science (Jamieson et al., 2020)", "href": "#h.yecsgzczx6oz"}, {"Relevant_ref": "Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019)", "href": "#h.jwujkwh9t0yi"}]}, "summary_34": {"Title": "Many hands make tight work (Silberzahn & Uhlmann, 2015)", "Id": "h.yc1aiom51ew9", "Main_Takeaways": ["It is argued that re-running the analysis produces the same outcome. An analysis run by a single team-researchers takes on several roles: inventor creates ideas and hypotheses; analysts scrutinise data to support hypotheses and devil\u2019s advocate use different approaches to show weaknesses in the findings.", "A team of skilled researchers validate findings, inform policymakers and balance discussions.", "Several teams work with the same dataset with the hypotheses and results are held close.", "All researchers discuss results via email exchanges and researchers add notes to individual reports in others\u2019 work. \u00a0A broad range of effect size is disturbing but any single analysis is too seriously mistaken.", "Crowdsourcing is not optimal, demands huge resources for one question. Decisions have to be made concerning hypotheses, data collection and which variables can or cannot be collected.", "Strong storylines are favoured over a messy reality. Scientists are hungry for reliable methods of discovery and to improve their networking."], "Abstract": "Crowdsourcing research can balance discussions, validate findings and better inform policy, say Raphael Silberzahn and Eric L. Uhlmann.", "Reference": "Silberzahn, R., & Uhlmann, E. L. (2015). Crowdsourced research: Many hands make tight work. Nature News, 526(7572), 189.\u00a0https://doi.org/10.1038/526189a", "You_may_also_be_interested_in": [{"Relevant_ref": "Publishing Research With Undergraduate Students via Replication Work: The Collaborative Replications and Education Project (CREP; Wagge et al., 2019)", "href": "#h.1bcavr6huszo"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}]}, "summary_35": {"Title": "A user\u2019s guide to inflated and manipulated impact factor (Ioannidis & Thombs, 2019)", "Id": "h.w1fgusfwh4me", "Main_Takeaways": ["A widely misused metric is the impact factor, reflecting the importance of the publications in a specific journal.", "Promotion and funding depends on impact factor (cf. Goodhart\u2019s Law). Over 14000 say let us remove it as an individual article quality based on the Declaration on Research Assessment.", "Tricks are used to inflate the impact factor of journals. Higher impact factors lead to more articles submitted and published.", "Inappropriate use of impact factor is unlikely to stop, especially with a large number of papers being cited without being counted.", "Self citations or requesting submitting articles to include citations to other recent articles without justification will lead to this inappropriate use of the impact factor.", "Review articles get more citations than research articles. Papers with questionable scientific value will get cited as standard reference.", "Authors should submit papers to target journals based on the journal\u2019s relevance, scientific rigour and quality, not impact factors.", "Authors who submit to journals with high impact inflation become members of bubbles and likely to publish in journals that may be discredited.", "We need to replace impact factor with median citations per item indicators separately for articles, reviews and other article types.", "We need metrics that can exclude self-citation, that are difficult to game and are appropriate."], "Abstract": "This is a view on impact factor by Professor John P.A. Ioannidis and Dr Brett D. Thombs. It contains a discussion of the impact factor being misused, how it is misused by journals and reviewers but provides solutions to overcome the use of this metric. In addition, we should base journals not on the impact factor but the relevance, scientific rigour and quality of the journal.", "Reference": "Ioannidis, J. P., & Thombs, B. D. (2019). A user\u2019s guide to inflated and manipulated impact factors. European journal of clinical investigation, 49(9), e13151. https://doi.org/10.1111/eci.13151", "You_may_also_be_interested_in": [{"Relevant_ref": "Six principles for assessing scientists for hiring, promotion, and tenure (Naudet et al, 2018)", "href": "#h.1j8n5dtwa2fz"}, {"Relevant_ref": "The Matthew effect in science funding (Bol et al., 2018)", "href": "#h.bbkbxhmwh8ms"}]}, "summary_36": {"Title": "Promoting an open research culture (Nosek et al., 2015)", "Id": "h.6d082dt7x129", "Main_Takeaways": ["The reward system focuses on innovation, as opposed to replication and being open, transparent and reproducible.", "There is no means to align individual and communal incentives via universal scientific policies and procedures.", "We should reward researchers for time and effort spent in open practices.", "Citation stands should extend to data, code and research materials. Regular and rigorous citation of these materials and credit them as original intellectual contribution.", "Reproducibility increases confidence in results and allows scholars to learn more about data interpretation.", "The transparency guidelines are used to improve transparency about the research process, while reducing vague or incomplete reporting of methodology.", "Pre-registration of studies facilitates discovery of research, allowing the study to be recorded in a public registry.", "Several levels are used to encourage open science policy: Level 1-no barrier or incentive to open science. This reduces the effort on journal efficiency and workflow.", "Level 2 has stronger expectations but avoids cost to editors and open data is placed in a trusted repository.", "Level 3 is the strongest standard but provides some barriers.", "Quality of publication increases by reducing time spent on communication with authors and reviewers, improving standard of reporting."], "Quote": "\u201cThe journal article is central to the research communication process. Guidelines for authors define what aspects of the research process should be made available to the community to evaluate, critique, reuse, and extend. Scientists recognize the value of transparency, openness, and reproducibility. Improvement of journal policies can help those values become more evident in daily practice and ultimately improve the public trust in science, and science itself.\u201d (p.1425).", "Abstract": "Author guidelines for journals could help to promote transparency, openness, and reproducibility.", "Reference": "Nosek, B. A., Alter, G., Banks, G. C., Borsboom, D., Bowman, S. D., Breckler, S. J., ... & Contestabile, M. (2015). Promoting an open research culture. Science, 348(6242), 1422-1425. http://doi.org/10.1126/science.aab2374", "You_may_also_be_interested_in": [{"Relevant_ref": "Quality Uncertainty Erodes Trust in Science (Vazire, 2017)", "href": "#h.6gb96p9e79ii"}, {"Relevant_ref": "Rein in the four horsemen of irreproducibility (Bishop, 2019)", "href": "#h.rrhza31fwxpw"}, {"Relevant_ref": "Seven Easy Steps to Open Science: An Annotated Reading List (Cr\u00fcwell et al., 2019)", "href": "#h.5slx325ilt8s"}, {"Relevant_ref": "Promoting Transparency in Social Science Research (Miguel et al., 2014)", "href": "#h.kzom3e53thr1"}, {"Relevant_ref": "Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018)", "href": "#h.syrrsgeyw2yr"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}]}, "summary_37": {"Title": "Promoting Transparency in Social Science Research (Miguel et al., 2014)", "Id": "h.kzom3e53thr1", "Main_Takeaways": ["The incentives, institutions and norms that social science makes it difficult to improve research design.", "Social science journals do not instruct adherence to reporting standards, data sharing or study registrations.", "Researchers have incentives to analyse and present data to make them more publishable at the expense of being accurate.", "This article surveys recent progress towards research transparency in the social sciences and provides standards and rules to realign scientific and scholarly incentives with scientific and scholarly values.", "Social scientists should use pre-registration to provide detailed documents, specify statistical models, dependent variables, covariates, interaction terms and multiple testing corrections to reduce biases.", "Open data and materials allows researchers to test alternative approaches on the data, reproduce results, identify misreported or fraudulent results; reuse or adapt materials for replication to improve interventions measures and so on.", "We need to move towards greater research transparency to pre-registration."], "Quote": "\u201cScientific inquiry requires imaginative exploration. Many important findings originated as unexpected discoveries. But findings from such inductive analysis are necessarily more tentative because of the greater flexibility of methods and tests and, hence, the greater opportunity for the outcome to obtain by chance. The purpose of prespecification is not to disparage exploratory analysis but to free it from the tradition of being portrayed as formal hypothesis testing. New practices need to be implemented in a way that does not stifle creativity or create excess burden. Yet we believe that such concerns are outweighed by the benefits that a shift in transparency norms will have for overall scientific progress, the credibility of the social science research enterprise, and the quality of evidence that we as a community provide to policy-makers\u201d (p.31).", "Abstract": "Social scientists should adopt higher transparency standards to improve the quality and credibility of research.", "Reference": "Miguel, E., Camerer, C., Casey, K., Cohen, J., Esterling, K. M., Gerber, A., ... & Laitin, D. (2014). Promoting transparency in social science research.\u00a0Science, 343(6166), 30-31.\u00a0http://doi.org/10.1126/science.1245317", "You_may_also_be_interested_in": [{"Relevant_ref": "Quality Uncertainty Erodes Trust in Science (Vazire, 2017)", "href": "#h.6gb96p9e79ii"}, {"Relevant_ref": "Rein in the four horsemen of irreproducibility (Bishop, 2019)", "href": "#h.rrhza31fwxpw"}, {"Relevant_ref": "Seven Easy Steps to Open Science: An Annotated Reading List (Cr\u00fcwell et al., 2019)", "href": "#h.5slx325ilt8s"}, {"Relevant_ref": "Promoting an open research culture (Nosek et al., 2015)", "href": "#h.6d082dt7x129"}, {"Relevant_ref": "Promoting Transparency in Social Science Research (Miguel et al., 2014)", "href": "#h.kzom3e53thr1"}, {"Relevant_ref": "Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018)", "href": "#h.syrrsgeyw2yr"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}]}, "summary_38": {"Title": "Rebuilding Ivory Tower: A bottom-up experiment in aligning research with societal needs (Hart & Silka, 2020) \u25c8", "Id": "h.tatg87yyjp4p", "Main_Takeaways": ["Scientists are trained to conduct good science, develop interesting research questions, be impartial to data, sceptical about conclusions and open to criticisms from our peers.", "We are taught good science is a reward in itself for improving our world.", "We need strong collaborations with diverse stakeholders in the public and private sectors, non-governmental organisations and civil society in order to identify and solve problems.", "There is greater attention to specific human dimensions: interactions between society and nature, along with university and diverse stakeholders.", "Many universities have a shared focus on research, teaching and service, but it is not enough to unite people for sustained collaborations.", "Academics share a commitment to excellence but not offer guidance for why and where to establish such excellence.", "We try to create an atmosphere of learning from successes and failures. There is no sure-fire formula to match research with societal needs.", "Older faculty are retiring but are being replaced by younger students who are able to move the initiative forward as a result of their skills to be interdisciplinary researchers."], "Quote": "\u201cTwo fundamental commitments [have emerged]: 1) In addition to the traditional focus on the biophysical components underpinning a problem, a much greater emphasis is needed on the human dimensions, including the complex interactions between society and nature; and 2) productive collaborations must be built between the university and diverse stakeholders to develop a sufficient understanding of sustainability problems and viable strategies for solving them.\u201d", "Abstract": "Academic scientists can transcend publish-or-perish incentives to help produce real-world solutions. Here\u2019s how one group did it.", "Reference": "Hart, D. D., & Silka, L. (2020). Rebuilding the ivory tower: bottom-up experiment in aligning research with societal needs.\u00a0Issues Sci Technol, 36(3), 64-70. https://issues.org/aligning-research-with-societal-needs/\u00a0[accessed 14/08/2020]", "You_may_also_be_interested_in": [{"Relevant_ref": "Promoting an open research culture (Nosek et al., 2015)", "href": "#h.6d082dt7x129"}, {"Relevant_ref": "Promoting Transparency in Social Science Research (Miguel et al., 2014)", "href": "#h.kzom3e53thr1"}]}, "summary_39": {"Title": "Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018)", "Id": "h.syrrsgeyw2yr", "Main_Takeaways": ["Senior colleagues and institutions try to hide errors.", "Retraction produces fear in a scientist, as it is associated with shame.", "Errors can be reduced with open science practices.", "Raw data can never be made completely open due to confidentiality but we can modify it to remove identifiable information, so other researchers reproduce what was done.", "Stigma needs to be removed concerning error detection.", "Making an analysis program open does not mean they are error-free. A reproducible result means when the same data is conducted, the same result is produced but this is incorrect.", "Researchers whose error is noticed may respond with denial, anger or silence. \u00a0It damages reputation for integrity, thus should be done via journal but in practice rarely done smoothly.", "Findings may be due to methodological concern, as opposed to errors in calculation or scripts.", "A study without a control group is underpowered, uses unreliable measures or has a major confound.", "Methodological errors due to ignorance not bad faith. These could be honest errors in data, analysis or method compromising conclusions inferred.", "Replication is important, as confidence in robustness of a finding cannot depend on a single study. When there is a failure to replicate, we should uncover why this happened (e.g. contextual factors or research expertise).", "We should not say original researchers are incompetent, frauds etc., but we should not say that critics had malevolent motives and lack expertise. We need to be impartial.", "We should avoid bias and identify publications that are ignored, as positive findings produce more citations than null findings.", "Investigating misconduct is important but challenging. It is a difficult endeavour and requires evidence that takes time to accumulate.", "Academic institutions take an accusation of misconduct against a staff member seriously but takes a long time. We should consider whether people could have vested interests against this academic.", "We should not mock or abuse other scientists who make honest errors, as this would encourage poor research practices and people may be less likely to be open about these errors."], "Quote": "\u201cCriticism is the bedrock of the scientific method. It should not be personal: If one has to point to problems with someone\u2019s data, methods, or conclusions, this should be done without implying that the person is stupid or dishonest. This is important, because the alternative is that many people will avoid engaging in robust debate because of fears of interpersonal conflict\u2014a recipe for scientific stasis. If wrong ideas or results are not challenged, we let down future generations who will try to build on a research base that is not a solid foundation. Worse still, when the research findings have practical applications in clinical or policy areas, we may allow wrongheaded interventions or policies to damage the well-being of individuals or society. As open science becomes increasingly the norm, we will find that everyone is fallible. The reputations of scientists will depend not on whether there are flaws in their research, but on how they respond when those flaws are noted.\u201d (p.6)", "Abstract": "This is a view on the fallibility of science, response to self-errors and errors made by others by Professor Dorothy Bishop. It contains discussion on how open science is the norm but being open and honest about oneself is not. It informs us that we should not mock or be hurtful to others concerning honest mistakes and that misconduct is a serious issue but we need to be supportive of both the researcher who is being accused and the individual who is accusing them.", "Reference": "Bishop, D. V. M. (2018). Fallibility in science: responding to errors in the work of oneself and others. Advances in Methods and Practices in Psychological Science, 1(3), 432-438. https://doi.org/10.1177/2515245918776632", "You_may_also_be_interested_in": [{"Relevant_ref": "Don\u2019t let transparency damage science (Lewandowsky & Bishop, 2016)", "href": "#h.9irgzcteicjg"}, {"Relevant_ref": "Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014)", "href": "#h.6m58pstvoxmx"}, {"Relevant_ref": "Quality Uncertainty Erodes Trust in Science (Vazire, 2017)", "href": "#h.6gb96p9e79ii"}, {"Relevant_ref": "Rein in the four horsemen of irreproducibility (Bishop, 2019)", "href": "#h.rrhza31fwxpw"}, {"Relevant_ref": "Seven Easy Steps to Open Science: An Annotated Reading List (Cr\u00fcwell et al., 2019)", "href": "#h.5slx325ilt8s"}, {"Relevant_ref": "Promoting an open research culture (Nosek et al., 2015)", "href": "#h.6d082dt7x129"}, {"Relevant_ref": "Promoting Transparency in Social Science Research (Miguel et al., 2014)", "href": "#h.kzom3e53thr1"}, {"Relevant_ref": "Signalling the trustworthiness of science should not be a substitute for direct action against research misconduct (Kornfeld & Titus, 2020)", "href": "#h.rgc9h1qzouij"}, {"Relevant_ref": "Reply to Kornfeld and Titus: No distraction from misconduct (Jamieson et al., 2020)", "href": "#h.1iofh0niziw4"}, {"Relevant_ref": "Stop ignoring misconduct (Kornfeld & Titus, 2016)", "href": "#h.iw4mcqqi8ce9"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "Publication Pressure and Scientific Misconduct in Medical Scientists (Tijdink et al., 2014)", "href": "#h.6mzquzro2mzm"}, {"Relevant_ref": "Scientists\u2019 Reputations are Based on Getting it Right, not being Right (Ebersole et al., 2016)", "href": "#h.asb7cw2xwqfj"}]}, "summary_40": {"Title": "Imagine a Research Future Defined by Open Values: Introducing the Open Science MOOC (Tennant, 2019)\u25c8", "Id": "h.l592nojz997z", "Main_Takeaways": ["Research does not work as well as it could. We have to be better scientists-we need to focus on reproducibility crisis,questionable research practices, lack of open access, wasteful research and flawed incentives and reward systems.", "Expectations are changing how to perform and communicate research.", "Modern research demands transparency and collaboration.", "At Open Science MOOC, a peer-to-peer and value-based community works towards better science for society.", "There is a community based on learning, sharing and collaboration that empowers researchers with knowledge and skills to save time and effort, solve research issues and advance global research."], "Abstract": "This is a blog by Jon Tennant, who argues about a research future defined by open values and introducing open science MOOC. It contains information about open science, the benefit of MOOC and that to solve global research we need to be a community based on learning, sharing and collaboration to empower researchers to learn new skills and consolidate further knowledge in order to save time and effort.", "Reference": "Tennant, J. (2019, Feb). Imagine a Research Future Defined by Open Values: Introducing the Open Science MOOC. Generation R. https://genr.eu/wp/imagine-a-research-future-defined-by-open-values-introducing-the-open-science-mooc/\u00a0https://doi.org/10.25815/6hyr-g583", "You_may_also_be_interested_in": [{"Relevant_ref": "Relevant references will be added soon"}]}, "summary_41": {"Title": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "Id": "h.l5025sphi7dt", "Main_Takeaways": ["The paper communicates about the replication crisis and argues that the publication bias exaggerates effect size.", "Small sample sizes are used to conduct multiple studies. Researchers need to decide a priori to collect more data, exclude subjects, conditions, measures or observations and transform a measure or add a covariate to an analysis.", "Meta-analysis estimates an effect effect size that combines results across many experiments to correct for an effect-size exaggeration.", "We need to have improved statistical sophistication for researchers to test hypotheses about populations based on the samples- reward quality and accuracy, as opposed to quantity and flashiness.", "We need to be honest and advocate research-if the idea was inspired by data, we should state it. Report effect size with 95% confidence intervals around them.", "We need to create a detailed research plan providing a priori hypotheses, sample size planning, data exclusion rules, analyses, transformations, covariates etc. We need to be transparent and register a research plan (cf. Pre-registration).", "Pre-registration might be vague, leaving many implicit decisions to be made.. A vague plan helps researchers think through the project in advance and protect them from believing they had a priori hypothesis based on the data.", "Registered report should be considered. Stage 1 is based on the importance of the question to address and rigour of the methodology, if deemed worthy, it is in principle Stage 1 accepted. If work is completed as planned, Stage 2 is accepted, irrespective of the findings.", "However registered reports require a lot of resources to complete and results will possess value irrespective of findings.", "It is important to develop a lab manual that encourages replicability and transparency, it is important to make data, materials and analysis scripts transparent. They should be Findable, Accessible, Interoperable and Reusable (FAIR).", "Researchers place time and effort to make data, scripts and materials to be shared but the difficulty is in making them accessible.", "We also need to address constraints on generality of findings-published findings fail to replicate, failure to replicate could be due to differences in procedures, albeit original work did not indicate such differences modulate effect."], "Quote": "\u201cThe aim of the methodological reform movement is not to restrict psychological research to procedures that meet some fixed criterion of replicability. Replicability is not in itself the goal of science. Rather, the central aim of methodological reform is to make research reports more transparent, so that readers can gain an accurate understanding of how the data were obtained and analyzed and can therefore better gauge how much confidence to place in the findings. A second aim is to discourage practices that contribute to effect-size exaggeration and false discoveries of non-existent phenomena. As per Vazire\u2019s analogy, the call is not for car dealerships to sell nothing but new Ferraris, but rather for dealers to be forthcoming about the weaknesses of what they have on the lot. The grand aim of science is to develop better, more accurate, and more useful understandings of reality. Methodological reform cannot in and of itself deliver on that goal, but it can help.\u201d (p.19).", "Abstract": "Psychological scientists strive to advance understanding of how and why we animals do and think and feel as we do. This is difficult, in part because flukes of chance and measurement error obscure researchers\u2019 perceptions. Many psychologists use inferential statistical tests to peer through the murk of chance and discern relationships between variables. Those tests are powerful tools, but they must be wielded with skill. Moreover, research reports must convey to readers a detailed and accurate understanding of how the data were obtained and analyzed. Research psychologists often fall short in those regards. This paper attempts to motivate and explain ways to enhance the transparency and replicability of psychological science. Specifically, I speak to how publication bias and p hacking contribute to effect-size exaggeration in the published literature, and how effect-size exaggeration contributes, in turn, to replication failures. Then I present seven steps toward addressing these problems: Telling the truth; upgrading statistical knowledge; standardizing aspects of research practices; documenting lab procedures in a lab manual; making materials, data, and analysis scripts transparent; addressing constraints on generality; and collaborating.", "Reference": "Lindsay, D. S. (2020). Seven steps toward transparency and replicability in psychological science. Canadian Psychology/Psychologie canadienne. Advance online publication. https://doi.org/10.1037/cap0000222\u00a0[ungated]", "You_may_also_be_interested_in": [{"Relevant_ref": "The Statistical Crisis in Science (Gelman & Loken, 2014)", "href": "#h.rc4vbzxkf0ax"}, {"Relevant_ref": "Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014)", "href": "#h.6m58pstvoxmx"}, {"Relevant_ref": "False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011)", "href": "#h.sxh34fk82cfh"}, {"Relevant_ref": "A 21 Word Solution (Simmons et al., 2012)\u25c8", "href": "#h.hxo3saai5nvg"}, {"Relevant_ref": "Quality Uncertainty Erodes Trust in Science (Vazire, 2017)", "href": "#h.6gb96p9e79ii"}, {"Relevant_ref": "Rein in the four horsemen of irreproducibility (Bishop, 2019)", "href": "#h.rrhza31fwxpw"}, {"Relevant_ref": "Seven Easy Steps to Open Science: An Annotated Reading List (Cr\u00fcwell et al., 2019)", "href": "#h.5slx325ilt8s"}, {"Relevant_ref": "Many hands make tight work (Silberzahn & Uhlmann, 2015)", "href": "#h.yc1aiom51ew9"}, {"Relevant_ref": "Promoting an open research culture (Nosek et al., 2015)", "href": "#h.6d082dt7x129"}, {"Relevant_ref": "Promoting Transparency in Social Science Research (Miguel et al., 2014)", "href": "#h.kzom3e53thr1"}, {"Relevant_ref": "Psychologists Are Open to Change, yet Wary of Rules (Fuchs et al., 2012)", "href": "#h.5oevlpnau8wr"}, {"Relevant_ref": "Registered Reports: Realigning incentives in scientific publishing (Chambers et al., 2015)", "href": "#h.4q6lmxfiq5cj"}, {"Relevant_ref": "Registered Reports: A new publishing initiative at Cortex (Chambers, 2013)", "href": "#h.d0r37aqo7vhl"}, {"Relevant_ref": "Registered Reports: A step change in scientific publishing (Chambers, 2014)", "href": "#h.lopn1u3ro9l1"}, {"Relevant_ref": "Registered reports: a method to increase the credibility of published results (Nosek & Lakens, 2014)", "href": "#h.2azmqwvaq115"}, {"Relevant_ref": "Registered reports (Jamieson et al., 2019)", "href": "#h.sc7lko114tkn"}, {"Relevant_ref": "Attitudes Toward Open Science and Public Data Sharing: A Survey Among Members of the German Psychological Society (Abele-Brehm et al., 2019)", "href": "#h.10dzxpbtal8"}, {"Relevant_ref": "Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011)", "href": "#h.1mu3kuufj5he"}, {"Relevant_ref": "Constraints on Generality (COG): A Proposed Addition to All Empirical Papers (Simons et al., 2017)", "href": "#h.2c556rf0rgig"}, {"Relevant_ref": "Most people are not WEIRD (Henrich et al., 2010)", "href": "#h.pxy9j3hosvxg"}, {"Relevant_ref": "How scientists can stop fooling themselves (Bishop, 2020b)", "href": "#h.vmo2rkmiz4ms"}, {"Relevant_ref": "CJEP Will Offer Open Science Badges (Pexman, 2017)", "href": "#h.qr7alql3zi3k"}, {"Relevant_ref": "Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency (Kidwell et al., 2016)", "href": "#h.ozpykj3tfhom"}, {"Relevant_ref": "Trust Your Science? Open Your Data and Code (Stodden, 2011)\u25c8", "href": "#h.8iegxoy9v5eq"}, {"Relevant_ref": "Publishing Research With Undergraduate Students via Replication Work: The Collaborative Replications and Education Project (CREP; Wagge et al., 2019)", "href": "#h.1bcavr6huszo"}, {"Relevant_ref": "Is science really facing a reproducibility crisis, and do we need it to? (Fanelli, 2018)", "href": "#h.43r13mkeborf"}, {"Relevant_ref": "Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018)", "href": "#h.syrrsgeyw2yr"}, {"Relevant_ref": "A consensus-based transparency checklist (Aczel et al., 2020)", "href": "#h.mht7gz1mhdkh"}, {"Relevant_ref": "Tell it like it is (Anon, 2020)", "href": "#h.coptdfafnjrh"}, {"Relevant_ref": "Is pre-registration worthwhile? (Szollosi et al., 2020)", "href": "#h.9ogie999427n"}, {"Relevant_ref": "Signalling the trustworthiness of science (Jamieson et al., 2020)", "href": "#h.yecsgzczx6oz"}, {"Relevant_ref": "Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019)", "href": "#h.jwujkwh9t0yi"}]}, "summary_42": {"Title": "Measurement Schmeasurement: Questionable Measurement Practices and How to Avoid Them\u00a0(Flake & Fried, 2019)\u25c8", "Id": "h.rfejemts8w5w", "Main_Takeaways": ["Questionable measurement practices are ignored in the literature and provide researchers the degrees of freedom to obtain the desired results. This poses a serious threat to cumulative psychological science.", "The research process is very flexible and exists. This is done implicitly (cf. Garden of forking paths).", "The definition of a measurement is broad and we need decisions to theorise the nature of a phenomenon to operationalise and analyse.", "Qualitative measurements do not need to be discussed but face questionable measurement practice. The present study measured transparent reporting practices.", "We need to provide information about measurements but it is lacking. This undermines internal and external validity, the statistical conclusion and construct made.", "Transparency does not make science more rigorous but facilitates it to evaluate it more thoroughly and accurately.", "We need to ask what the construct is measuring? Reporting what it is and how it is defined.", "We need to consider theoretical definitions, how it aligns with measures and the existing validity evidence for these measures.", "Once the construct is defined and measures are selected, we must report its origins, exact number of items, the wording, response format, short or long version, language, how it was presented, and hardware and software specification.", "We need to consider how it was transformed, which items form which scores and how was it calculated.", "We need to report decision rules to facilitate others to reproduce and evaluate work such as how it was calculated and why was it calculated.", "Before data collection, potential degrees of freedom are changing response type, changing response style or options, changing item wording or content.", "We need to declare and justify modifications that threaten the validity of inferences.", "When using existing scales, we need to make many decisions. When we create and use a scale-there are threats to its validity. We need to disclose why we created it, why we used it over an existing measure.", "If there is no or little validity, we discuss it as a limitation of the study. Without the transparency, we have to wonder what is significant or replicable. Transparency promotes new lines of inquiry, strengthening the validity of measures and improving the quality of work."], "Quote": "\u201cThe increased awareness and emphasis on QRPs, such as p-hacking, have been an", "Abstract": "Academic scientists can transcend publish-or-perish incentives to help produce real-world solutions. Here\u2019s how one group did it.", "Reference": "Flake, J. K., & Fried, E. I. (2019). Measurement schmeasurement: Questionable measurement practices and how to avoid them. https://psyarxiv.com/hs7wm/", "You_may_also_be_interested_in": [{"Relevant_ref": "Relevant references will be added soon"}]}, "summary_43": {"Title": "A consensus-based transparency checklist (Aczel et al., 2020)", "Id": "h.mht7gz1mhdkh", "Main_Takeaways": ["There is an erosion of trust that affects credibility of specific articles and the discipline as a whole.", "There is a lack of transparency, which is required to evaluate and reproduce findings, but also for research synthesis and meta analysis from the raw data.", "The lack of transparency is not meant to be deceptive or intentional. Human reasoning is prone to biases (e.g. confirmation bias and motivated reasoning).", "Few journals ask about statistical and methodological practices and transparency.", "Journals can support open practices by offering badges, using the transparency and openness promotion guidelines, promote availability of all research items, including data, materials and codes.", "The consensus-based transparency checklist can be submitted with the manuscript to provide critical information about the process to evaluate the robustness of a finding.", "The checklist can be modified by deleting, adding and rewording items with high level of acceptability and consensus with no strong counter argument for single items.", "Researchers can explain the choices at the end of each 36 section. There is a shortened 12-item version to reduce demands on the researchers\u2019 time and facilitate broader adoption that fosters transparency and asks authors to complete a 36-item list."], "Abstract": "We present a consensus-based checklist to improve and document the transparency of research reports in social and behavioural research. An accompanying online application allows users to complete the form and generate a report that they can submit with their manuscript or post to a public repository.", "Reference": "Aczel, B., Szaszi, B., Sarafoglou, A., Kekecs, Z., Kucharsk\u00fd, \u0160., Benjamin, D., ... & Ioannidis, J. P. (2020). A consensus-based transparency checklist. Nature human behaviour,\u00a04(1), 4-6. https://doi.org/10.1038/s41562-019-0772-6", "You_may_also_be_interested_in": [{"Relevant_ref": "How scientists can stop fooling themselves (Bishop, 2020b)", "href": "#h.vmo2rkmiz4ms"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "Tell it like it is (Anon, 2020)", "href": "#h.coptdfafnjrh"}, {"Relevant_ref": "Is pre-registration worthwhile? (Szollosi et al., 2020)", "href": "#h.9ogie999427n"}]}, "summary_44": {"Title": "Tell it like it is (Anon, 2020)", "Id": "h.coptdfafnjrh", "Main_Takeaways": ["A manuscript answers a question(s) based on findings and how they support or contradict hypotheses.", "Current research culture is defined by pressure to present research projects as conclusive narratives leave no room for ambiguity.", "Clean narratives represent a threat to validity and counter reality of what science looks like.", "Report only outcomes to confirm original predictions or excluding research findings that provide messy results.", "These questionable research practices create a distorted picture of research that prevents cumulative knowledge.", "Pre-registration has little value if not heeded or transparently reported.", "It is evident during peer review that a pre-registered analysis is inappropriate or suboptimal. \u00a0Authors have to provide deviations and explain why they did these deviations.", "A pre-registered analysis plan is flawed, authors report results of pre-registered alongside new analyses.", "Authors report multi-study research papers and authors report all work they executed, irrespective of outcomes."], "Quote": "\u201cNo research project is perfect; there are always limitations that also need to be transparently reported. In 2019, we made it a requirement that all our research papers include a limitations section, in which authors explain methodological and other shortcomings and explicitly acknowledge alternative interpretations of their findings\u2026 Science is messy, and the results of research rarely conform fully to plan or expectation. \u2018Clean\u2019 narratives are an artefact of inappropriate pressures and the culture they have generated. We strongly support authors in their efforts to be transparent about what they did and what they found, and we commit to publishing work that is robust, transparent and appropriately presented, even if it does not yield \u2018clean\u2019 narratives\u201d p.1", "Abstract": "Every research paper tells a story, but the pressure to provide \u2018clean\u2019 narratives is harmful for the scientific endeavour.", "Reference": "Anon (2020). Tell it like it is.\u00a0Nat Hum Behav 4, 1. https://doi.org/10.1038/s41562-020-0818-9\u00a0", "You_may_also_be_interested_in": [{"Relevant_ref": "How scientists can stop fooling themselves (Bishop, 2020b)", "href": "#h.vmo2rkmiz4ms"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "A consensus-based transparency checklist (Aczel et al., 2020)", "href": "#h.mht7gz1mhdkh"}, {"Relevant_ref": "Tell it like it is (Anon, 2020)", "href": "#h.coptdfafnjrh"}, {"Relevant_ref": "Is pre-registration worthwhile? (Szollosi et al., 2020)", "href": "#h.9ogie999427n"}]}, "summary_45": {"Title": "Is pre-registration worthwhile? (Szollosi et al., 2020)", "Id": "h.9ogie999427n", "Main_Takeaways": ["Pre-registration solves statistical problems and forces people to think more deeply about theories, methods, and analyses.", "Diagnosticity of statistical tests depend on how well statistical models map onto theories and improve statistical techniques does little to improve theories when mapping is weak.", "Models are useful depending on how accurately the theory is matched to the model. Many statistical models (e.g. general linear model) in psychology are poor estimates of the theory.", "Bad theories are pre-registered, predictions that are barely better than randomly picking an outcome but which bear out in experiments.", "There is no problem with post-hoc scientific inference when the theories are strong.", "Pre-registration does not improve theories but should allow them to think more deeply on how to improve theories through better planning, more precise operationalisation of constructs, and clear motivation for statistical planning.", "We should improve theories when encountering difficulties with pre-registration or when pre-registered predictions are wrong.", "Any improvement depends on a good understanding of how to improve a theory, and pre-registration provides no understanding. Pre-registration encourages thinking, but it is unclear whether the thinking is better or worse.", "Pre-registration could harm the progress in our field.", "Transparency is important but other solutions to solve problems and asking researchers to disclose studies and methods when publishing.", "Pre-registration should be an option to improve research, needing, rewarding or promoting it is not worthwhile.", "Scientific inference is the process to develop better theories.", "Statistical models are simplified mathematical abstractions of scientific problems, simplifications to aid scientific inference but to allow abstraction.", "Poor operationalisation, imprecise measurement, weak connection between theory and statistical method take precedence over problems of statistical inference."], "Abstract": "Proponents of preregistration argue that, among other benefits, it improves the diagnosticity of statistical tests. In the strong version of this argument, preregistration does this by solving statistical problems, such as family-wise error rates. In the weak version, it nudges people to think more deeply about their theories, methods, and analyses. We argue against both: the diagnosticity of statistical tests depend entirely on how well statistical models map onto underlying theories, and so improving statistical techniques does little to improve theories when the mapping is weak. There is also little reason to expect that preregistration will spontaneously help researchers to develop better theories (and, hence, better methods and analyses).", "Reference": "Szollosi, A., Kellen, D., Navarro, D. J., Shiffrin, R., van Rooij, I., Van Zandt, T., & Donkin, C. (2020). Is Preregistration Worthwhile?. Trends in cognitive sciences, 24(2), 94.https://doi.org/10.1016/j.tics.2019.11.009", "You_may_also_be_interested_in": [{"Relevant_ref": "How scientists can stop fooling themselves (Bishop, 2020b)", "href": "#h.vmo2rkmiz4ms"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "A consensus-based transparency checklist (Aczel et al., 2020)", "href": "#h.mht7gz1mhdkh"}, {"Relevant_ref": "Tell it like it is (Anon, 2020)", "href": "#h.coptdfafnjrh"}, {"Relevant_ref": "Arrested theory development: The misguided distinction between exploratory and confirmatory research (Szollosi & Donkin, 2019)", "href": "#h.e69z7oel1bvj"}, {"Relevant_ref": "From pre-registration to publication: a non-technical primer for conducting meta-analysis to synthesize correlation data (Quintana, 2015)", "href": "#h.jtemahk1nbp"}, {"Relevant_ref": "Pre-registration is Hard, And Worthwhile (Nosek et al., 2019)", "href": "#h.ps8ccttak4f3"}, {"Relevant_ref": "Easy preregistration will benefit any research (Mellor & \u00a0Nosek, 2018)", "href": "#h.vjfs2wfa3dc8"}, {"Relevant_ref": "Preregistration of Modeling Exercises May Not Be Useful (MacEachern & Van Zandt, 2019)", "href": "#h.4wfo7wvqmchp"}]}, "summary_46": {"Title": "Arrested theory development: The misguided distinction between exploratory and confirmatory research (Szollosi & Donkin, 2019)\u25c8", "Id": "h.e69z7oel1bvj", "Main_Takeaways": ["It is difficult to provide arguments for why direct replication or pre-registration are important, we do not know whether they aim to achieve coincides with the goals of science.", "We need to think about theories. What makes a good theory? They are a good explanation about phenomena that occur in the world and need exploratory power. They allow us to correct flaws in our theories.", "Good explanation is hard to change consistent with other good theories and cannot be adapted to explain anything.", "A theory limited by existing knowledge without benefit of flexibility can tailor explanation to any possible observation.", "A good theory resists change. A theory can be criticised by argument and rejection of explanations.", "A good theory should observe what has been shown in the past and what we should observe in the future.", "Both pre-registrations and direct replications need researchers to fix what their theories predict in current forms but practice is futile when theory can be changed to explain experimental outcomes.", "If when testing good theories, we expect unchanged predictions and show, the theory remains good, if we do not observe it, it becomes problematic. Flexible theories are inconsequential.", "It is not possible to reconcile expected and observed data, no change should be required to explain data or for which any explanation is possible.", "Theory is rendered problematic and new theories are required."], "Quote": "\u201cRather than quick-fix methodological suggestions, we advocate for an overhaul of how theories are developed and evaluated. In order to move forward, we need to confront the real problems of the field, wherein theories are not held accountable for their flexibility. This is not an easy feat to achieve, but we are optimistic that a deeper understanding of how science progresses can help behavioral scientists develop better theories\u201d (p.10).", "Abstract": "Starting from the view that progress in science consists of the improvement of our theories, in the current paper we ask two questions: what makes a theory good,and how much do the current method-oriented solutions to the replication crisis contribute to the development of good theories? Based on contemporary philosophy of science, we argue that good theories are hard-to-vary: they (1) explain what they are supposed to explain, (2) are consistent with other good theories, and (3) cannot easily be adapted to explain anything.Theories can be improved by identifying problems in them either by argument or by experimental test,and then correcting these problems by changing the theory. Importantly, such changes and the resultant theory should only be assessed based on whether they are hard-to-vary.An assessment ofthe current state of the behavioral sciences reveals that theory development is arrested by the lack of consideration for how easy it is to change theories to account for unexpected observations. Further, most of the current method-oriented solutions are unlikely to contribute much to the development of good theories, because they do not work towards eliminating this problem.Instead, they reward only temporary inflexibility in theories,and promote the assessment of theory change based on whether the theory was changed before (confirmatory) or after (exploratory) an experimental test, but not whether that change yields a hard-to-vary theory.Finally, we argue that these methodological solutions would become irrelevant if we turned our focus to the explicit aim of developing theories that are hard-to-vary.", "Reference": "Szollosi, A., & Donkin, C. (2019). Arrested theory development: The misguided distinction between exploratory and confirmatory research. https://doi.org/10.31234/osf.io/suzej", "You_may_also_be_interested_in": [{"Relevant_ref": "Is pre-registration worthwhile? (Szollosi et al., 2020)", "href": "#h.9ogie999427n"}]}, "summary_47": {"Title": "From pre-registration to publication: a non-technical primer for conducting meta-analysis to synthesize correlation data (Quintana, 2015)", "Id": "h.jtemahk1nbp", "Main_Takeaways": ["Meta analysis is an integration of evidence from numerous studies. Effect size and measures of variance, numerous outcomes can be formed to compute a summary effect size. \u00a0They have not been combined in a single resource targeting psychologists.", "This review will discuss how to conduct meta-analysis via PRISMA guidelines. Pre-registration allows a study rationale to be created that forms good systematic review and helps avoid bias. Meta analyses are prone to hypothesising after the results-HARKING.", "Pre-registration is important for submission, however, few journals need to consider meta-analysis registration. It is more important to pre-register meta analysis than clinical trial pre-registration as meta-analyses drive treatment for practice and health policy.", "Pre-registration avoids unintended meta-analysis duplication. Boolean operators and search limits help literature research. Databases are available with researchers to choose most suitable sources. Numerous scientists use duplicate search terms within two or more databases to cover numerous sources.", "Gray literature is difficult with numerous universities posting dissertations. Fields and research questions exclude blanket recommendation. Meta-analyses detail search strategy for study protocols and methods.", "Model selection centre around assumptions of study homogeneity, how much variation of studies can be due to variations in true effect size. Variation is from random error and true study heterogeneity.", "Forest plots visualise effect sizes and confidence intervals from included studies with summary effect size.", "Publication bias produces stronger effects size and are more likely to be published and included in meta-analysis. A funnel plot is a visual tool to investigate potential publication bias in meta analyses.", "Funnel plots offer a useful visualisation for potential publication bias, it is important to consider asymmetry may represent other types of bias like study quality, location bias and study size.", "Funnel plots suffer from subjective measures of potential publication bias. Two tests used to calculate objective measures of potential bias: trim and fill method and moderating variables."], "Abstract": "Starting from the view that progress in science consists of the improvement of our theories, in the current paper we ask two questions: what makes a theory good,and how much do the current method-oriented solutions to the replication crisis contribute to the development of good theories? Based on contemporary philosophy of science, we argue that good theories are hard-to-vary: they (1) explain what they are supposed to explain, (2) are consistent with other good theories, and (3) cannot easily be adapted to explain anything.Theories can be improved by identifying problems in them either by argument or by experimental test,and then correcting these problems by changing the theory. Importantly, such changes and the resultant theory should only be assessed based on whether they are hard-to-vary.An assessment ofthe current state of the behavioral sciences reveals that theory development is arrested by the lack of consideration for how easy it is to change theories to account for unexpected observations. Further, most of the current method-oriented solutions are unlikely to contribute much to the development of good theories, because they do not work towards eliminating this problem.Instead, they reward only temporary inflexibility in theories,and promote the assessment of theory change based on whether the theory was changed before (confirmatory) or after (exploratory) an experimental test, but not whether that change yields a hard-to-vary theory.Finally, we argue that these methodological solutions would become irrelevant if we turned our focus to the explicit aim of developing theories that are hard-to-vary.", "Reference": "Quintana, D. S. (2015). From pre-registration to publication: a non-technical primer for conducting a meta-analysis to synthesize correlational data. Frontiers in psychology, 6, 1549. https://doi.org/10.3389/fpsyg.2015.01549", "You_may_also_be_interested_in": [{"Relevant_ref": "Is pre-registration worthwhile? (Szollosi et al., 2020)", "href": "#h.9ogie999427n"}, {"Relevant_ref": "Pre-registration is Hard, And Worthwhile (Nosek et al., 2019)", "href": "#h.ps8ccttak4f3"}, {"Relevant_ref": "Easy preregistration will benefit any research (Mellor & \u00a0Nosek, 2018)", "href": "#h.vjfs2wfa3dc8"}, {"Relevant_ref": "Preregistration of Modeling Exercises May Not Be Useful (MacEachern & Van Zandt, 2019)", "href": "#h.4wfo7wvqmchp"}, {"Relevant_ref": "On the reproducibility of meta-analyses: six practical recommendations (Lakens et al., 2014)", "href": "#h.6nk88qnf4l33"}]}, "summary_48": {"Title": "Pre-registration is Hard, And Worthwhile (Nosek et al., 2019)", "Id": "h.ps8ccttak4f3", "Main_Takeaways": ["Pre-registration allows us to make exploratory and confirmatory analyses.", "Pre-registration allows us to make the transparent uncertainty more certain, how many statistical tests were conducted and familywise error rate to be corrected.", "Pre-registration reduces influence of publication bias and pre-registration is a skill that needs experience to be improved.", "Pre-registration promotes intellectual humility and better calibration of scientific claims.", "It allows us to provide information on how methodology is implemented, how hypotheses are tested, the exclusion rules, how variables are combined and what to use concerning the statistical model, covariates and characteristics.", "Pre-registration converts general sense into precise and explicit plans that predict what has not yet occurred and decide what will be done.", "It allows us to stop data collection. What are the steps required to assess questions of interest? What are the outcomes?", "Having a plan is better than no plan, sharing plans to advance is better than not sharing them.", "Planning will improve and benefits will increase for oneself and consumers of research.", "Deviations make it harder to interpret with confidence what occurred to what was planned.", "Transparency is important and all deviations should be reported, this is difficult due to narrative coherence, reviewer expectations and word limits.", "We need to maximise credibility of reporting findings when possible, update pre-registration, deviations before observing data, mention all planned analyses to explain why a planned analysis was not reported.", "Use supplements to share in full not hide inconvenient information and during analysis."], "Abstract": "Preregistration clarifies the distinction between planned and unplanned research by reducing unnoticed flexibility. This improves credibility of findings and calibration of uncertainty. However, making decisions before conducting analyses requires practice. During report writing, respecting both what was planned and what actually happened requires good judgment and humility in making claims.", "Reference": "Nosek, B. A., Beck, E. D., Campbell, L., Flake, J. K., Hardwicke, T. E., Mellor, D. T., ... & Vazire, S. (2019). Preregistration is hard, and worthwhile. Trends in cognitive sciences, 23(10), 815-818.https://doi.org/10.1016/j.tics.2019.07.009\u00a0[ungated]", "You_may_also_be_interested_in": [{"Relevant_ref": "Is pre-registration worthwhile? (Szollosi et al., 2020)", "href": "#h.9ogie999427n"}, {"Relevant_ref": "From pre-registration to publication: a non-technical primer for conducting meta-analysis to synthesize correlation data (Quintana, 2015)", "href": "#h.jtemahk1nbp"}, {"Relevant_ref": "Easy preregistration will benefit any research (Mellor & \u00a0Nosek, 2018)", "href": "#h.vjfs2wfa3dc8"}, {"Relevant_ref": "Preregistration of Modeling Exercises May Not Be Useful (MacEachern & Van Zandt, 2019)", "href": "#h.4wfo7wvqmchp"}]}, "summary_49": {"Title": "Easy preregistration will benefit any research (Mellor & \u00a0Nosek, 2018)", "Id": "h.vjfs2wfa3dc8", "Main_Takeaways": ["It is important to commit to study participants, the public and basic science, we need to be transparent, rigorous and reproducible.", "Pre-registration discriminates between confirmatory and exploratory research.", "We need to address publication bias. Widespread pre-registration addresses key contributions and increases interpretability of most empirical research.", "New practices need to improve and accelerate, not interfere with it, knowledge accumulation.", "Open science framework has more than 10000 registrations and supports multiple registration formats and iterative registrations."], "Abstract": "This view was written by David Mellor and Professor Brian Nosek. They discuss the benefits of pre-registration and how it addresses publication bias. Open-science framework provides support to multiple registration formats..", "Reference": "Mellor, D. T., & Nosek, B. A. (2018). Easy preregistration\u00a0will benefit any research. Nature Human Behaviour, 2(2), 98-98. https://doi.org/10.1038/s41562-018-0294-7", "You_may_also_be_interested_in": [{"Relevant_ref": "Is pre-registration worthwhile? (Szollosi et al., 2020)", "href": "#h.9ogie999427n"}, {"Relevant_ref": "From pre-registration to publication: a non-technical primer for conducting meta-analysis to synthesize correlation data (Quintana, 2015)", "href": "#h.jtemahk1nbp"}, {"Relevant_ref": "Pre-registration is Hard, And Worthwhile (Nosek et al., 2019)", "href": "#h.ps8ccttak4f3"}, {"Relevant_ref": "Preregistration of Modeling Exercises May Not Be Useful (MacEachern & Van Zandt, 2019)", "href": "#h.4wfo7wvqmchp"}]}, "summary_50": {"Title": "Preregistration of Modeling Exercises May Not Be Useful (MacEachern & Van Zandt, 2019)", "Id": "h.4wfo7wvqmchp", "Main_Takeaways": ["The present study focuses on modelling and data analysis and how each round improves analysis that builds richer understanding of data and processes that give rise to data.", "Powerful software and improved graphical capabilities allows us to explore many more features of data.", "The ease with which data is transformed and cleaned, with which a model can be fit may lead to overfitting.", "Model development is intrinsically exploratory and creative.", "The present article disagrees with pre- and post-registration of models. In highly exploratory settings, there is greater difficulty to pre-register a model and analysis.", "Modelling depends on the modeller\u2019s perspective and data collected. Each author performs exploratory analysis and settles on the same transformation for the response variable.", "Two \u00a0authors would not realistically pre-register models or exploratory plans except in general terms.", "When the model is combined with the Bayesian model averaging, the overall model provides a better description of the entire dataset than any single model on its own.", "Reality is too complicated and covariates are sparse enough that it would be a challenge to identify the right model. Models are tools. Different models are used differently to different ends.", "Model construction and development depend on analysing and re-analysing a dataset to determine which of its properties are crucial to understand a phenomenon or make predictions.", "Confirmatory model implies truth to be discovered among models in competition but there is favouritism, as the models invested develops, how we view the world, ease of implementation and so on.", "One model is not true in that some data is captured, and other data is not captured.", "Bayesian methods need to be used, as datasets grow. If pre-registration is required, Bayesian analysts pay attention to influence prior distributions such as Bayes factor.", "Underfitting of the data is as problematic as overfitting. Pre-registration of model development will lead to less and less creative and exploratory analyses that are achieved by needing publication of raw data and code.", "Psychology departments should devote more resources to training in quantitative areas and training include explicit content on under- and over-modelling. Also, we should partner with the statistics department to improve our modelling skills."], "Abstract": "This is a commentary on Lee et al.\u2019s (2019) article encouraging preregistration of model development, fitting, and evaluation. While we are in general agreement with Lee et al.\u2019s characterization of the modeling process, we disagree on whether preregistration\u00a0of this process will move the scientific enterprise forward. We emphasize the subjective and exploratory nature of model development, and point out that \u201cunder-modeling\u201d of data (relying on black-box approaches applied to data without data exploration) is as big a problem as \u201cover-modeling\u201d (fitting noise, resulting in models that generalize poorly). We also note the potential long-run negative impact of preregistration on future generations of cognitive scientists. It is our opinion that preregistration of model development will lead to less, and to less creative, exploratory analysis (i.e., to more under-modeling), and that Lee at al.\u2019s primary goals can be achieved by requiring publication of raw data and code. We conclude our commentary with suggestions on how to move forward.", "Reference": "MacEachern, S. N., & Van Zandt, T. (2019). Preregistration of modeling exercises may not be useful. Computational Brain & Behavior, 2(3-4), 179-182. https://doi.org/10.1007/s42113-019-00038-x", "You_may_also_be_interested_in": [{"Relevant_ref": "Is pre-registration worthwhile? (Szollosi et al., 2020)", "href": "#h.9ogie999427n"}, {"Relevant_ref": "From pre-registration to publication: a non-technical primer for conducting meta-analysis to synthesize correlation data (Quintana, 2015)", "href": "#h.jtemahk1nbp"}, {"Relevant_ref": "Easy preregistration will benefit any research (Mellor & \u00a0Nosek, 2018)", "href": "#h.vjfs2wfa3dc8"}, {"Relevant_ref": "Pre-registration is Hard, And Worthwhile (Nosek et al., 2019)", "href": "#h.ps8ccttak4f3"}]}, "summary_51": {"Title": "Sample size and the fallacies of classical inference (Friston, 2013)", "Id": "h.nwo2bbyzbdbp", "Main_Takeaways": ["Authors and reviewers need to consider sample size and effect size. One should get as much data as possible.", "Trivial effect sizes can be resolved by reporting confidence intervals or interval null hypotheses. Simple point hypotheses are tested with p value.", "Best studies use large numbers of subjects and report them in terms of confidence intervals.", "Large sample sizes increase the power of model comparison.", "Increasing sample size will increase the number of true positives.", "If trivial effect sizes prevail, the positive predictive values measure the proportion of significant results that are not important.", "A trivial effect size does not mean small effect.", "Cross-validation in neuroimaging needs further discussion to validate a model to predict using new observations."], "Abstract": "I would like to thank Michael Ingre, Martin Lindquist and their co-authors for their thoughtful responses to my ironic Comments and Controversies piece. I was of two minds about whether to accept the invitation to reply \u2014 largely because I was convinced by most of their observations. I concluded that I should say this explicitly, taking the opportunity to consolidate points of consensus and highlight outstanding issues.", "Reference": "Friston, K. (2013). Sample size and the fallacies of classical inference. Neuroimage, 81, 503-504.https://doi.org/10.1016/j.neuroimage.2013.02.057", "You_may_also_be_interested_in": [{"Relevant_ref": "Why small low-powered studies are worse than large high-powered studies and how to protect against \u201ctrivial\u201d findings in research: Comment on Friston (2012) (Ingre, 2013)", "href": "#h.iwapj9d4verh"}, {"Relevant_ref": "Ironing out the statistical wrinkles in \u201cten ironic rules\u201d (Lindquist et al., 2013)", "href": "#h.502tqeqmh47i"}, {"Relevant_ref": "Ten ironic rules for non-statistical reviewers (Friston, 2012)", "href": "#h.2ipfvebuwuxk"}]}, "summary_52": {"Title": "Why small low-powered studies are worse than large high-powered studies and how to protect against \u201ctrivial\u201d findings in research: Comment on Friston (2012) (Ingre, 2013)", "Id": "h.iwapj9d4verh", "Main_Takeaways": ["Small underpowered studies provide better evidence due to a lack of small and trivial effect sizes, leading to poor research practices.", "Underpowered studies are less likely to find a true effect in data and failure does not come without consequences.", "Failure to detect true effects means when significant findings are reported-it may be due to type I error.", "Findings from small low-powered studies are weaker than high-powered studies due to the fact that poor statistical power increases false positive.", "Small studies have poor estimates and produce false positives with large effect sizes.", "Researchers should make use of at least one additional statistic value (e.g. t value, effect size or confidence intervals).", "We need to be cautious about effect size seen as trivial, meaningful or unimportant."], "Quote": "\u201cFrom a strictly scientific point of view, you can never have too much precision, and consequently, never too many subjects or too much statistical power (unless a researcher is doing something wrong when reporting and interpreting data). The limiting factors are cost (time, resources and money) and potential harm for the subjects involved in the study. The real question you need to ask is how much cost and harm you can afford to get as good answer as possible.\u201d (p.498)", "Abstract": "It is sometimes argued that small studies provide better evidence for reported effects because they are less likely to report findings with small and trivial effect sizes (Friston, 2012). But larger studies are actually better at protecting against inferences from trivial effect sizes, if researchers just make use of effect sizes and confidence intervals. Poor statistical power also comes at a cost of inflated proportion of false positive findings, less power to \u201cconfirm\u201d true effects and bias in reported (inflated) effect sizes. Small studies (n = 16) lack the precision to reliably distinguish small and medium to large effect sizes (r < .50) from random noise (\u03b1 = .05) that larger studies (n = 100) does with high level of confidence (r = .50, p = .00000012). The present paper presents the arguments needed for researchers to refute the claim that small low-powered studies have a higher degree of scientific evidence than large high-powered studies.", "Reference": "Ingre, M. (2013). Why small low-powered studies are worse than large high-powered studies and how to protect against \u201ctrivial\u201d findings in research: Comment on Friston (2012). Neuroimage, 81, 496-498.https://doi.org/10.1016/j.neuroimage.2013.03.030", "You_may_also_be_interested_in": [{"Relevant_ref": "Sample size and the fallacies of classical inference (Friston, 2013)", "href": "#h.nwo2bbyzbdbp"}, {"Relevant_ref": "Ironing out the statistical wrinkles in \u201cten ironic rules\u201d (Lindquist et al., 2013)", "href": "#h.502tqeqmh47i"}, {"Relevant_ref": "Ten ironic rules for non-statistical reviewers (Friston, 2012)", "href": "#h.2ipfvebuwuxk"}]}, "summary_53": {"Title": "Ironing out the statistical wrinkles in \u201cten ironic rules\u201d (Lindquist et al., 2013)", "Id": "h.502tqeqmh47i", "Main_Takeaways": ["A Collaborative environment encourages good and bad ideas about statistics. Similar to large sample sizes, small sample sizes can detect large effects. However, small effects cannot be detected in small sample sizes, this requires larger sample sizes.", "Large sample sizes are prone to biases masking small effects.", "It is difficult to interpret significant results in small samples and makes it difficult to check certain assumptions or perform sensitivity analyses.", "Increasing sample size, leads to small effects being significant, producing more positive effects.", "More data allows us to detect subtle effects.", "Hypothesis test should not be exploratory, but confirmatory.", "Small sample sizes are better when considering important non-statistical issues such as the lives of animals or side effects.", "Hypothesis testing cannot discriminate between important and \u00a0trivial effects.", "We need estimates, confidence intervals, exploratory plots and other summaries of data with careful scientific thinking.", "We need to consider Lindley\u2019s paradox-Bayesian and frequentist approaches lead to different results for certain types of prior distributions."], "Abstract": "The article \u201cTen ironic rules for non-statistical reviewers\u201d (Friston, 2012) shares some commonly heard frustrations about the peer-review process that all researchers can identify with. Though we found the article amusing, we have some concerns about its description of a number of statistical issues. In this commentary we address these issues, as well as the premise of the article.", "Reference": "Lindquist, M. A., Caffo, B., & Crainiceanu, C. (2013). Ironing out the statistical wrinkles in \u201cten ironic rules\u201d. Neuroimage, 81, 499-502. https://doi.org/10.1016/j.neuroimage.2013.02.056", "You_may_also_be_interested_in": [{"Relevant_ref": "Sample size and the fallacies of classical inference (Friston, 2013)", "href": "#h.nwo2bbyzbdbp"}, {"Relevant_ref": "Why small low-powered studies are worse than large high-powered studies and how to protect against \u201ctrivial\u201d findings in research: Comment on Friston (2012) (Ingre, 2013)", "href": "#h.iwapj9d4verh"}, {"Relevant_ref": "Ten ironic rules for non-statistical reviewers (Friston, 2012)", "href": "#h.2ipfvebuwuxk"}]}, "summary_54": {"Title": "Ten ironic rules for non-statistical reviewers (Friston, 2012)", "Id": "h.2ipfvebuwuxk", "Main_Takeaways": ["Reviewers may not have adequate statistical expertise to provide a critique during peer review to reject a manuscript.", "Handling editors are happy to decline a paper and are placed under pressure to maintain a high rejection rate.", "All journals maximise rejection rates, increase quality of submission and impact factor to explain long-term viability. There are ten rules to follow."], "Abstract": "As an expert reviewer, it is sometimes necessary to ensure a paper is rejected. This can sometimes be achieved by highlighting improper statistical practice. This technical note provides guidance on how to critique the statistical analysis of neuroimaging studies to maximise the chance that the paper will be declined. We will review a series of critiques that can be applied universally to any neuroimaging paper and consider responses to potential rebuttals that reviewers might encounter from authors or editors.", "Reference": "Friston, K. (2012). Ten ironic rules for non-statistical reviewers. Neuroimage, 61(4), 1300-1310. https://doi.org/10.1016/j.neuroimage.2012.04.018", "You_may_also_be_interested_in": [{"Relevant_ref": "Sample size and the fallacies of classical inference (Friston, 2013)", "href": "#h.nwo2bbyzbdbp"}, {"Relevant_ref": "Why small low-powered studies are worse than large high-powered studies and how to protect against \u201ctrivial\u201d findings in research: Comment on Friston (2012) (Ingre, 2013)", "href": "#h.iwapj9d4verh"}, {"Relevant_ref": "Ironing out the statistical wrinkles in \u201cten ironic rules\u201d (Lindquist et al., 2013)", "href": "#h.502tqeqmh47i"}]}, "summary_55": {"Title": "Using OSF to Share Data: A Step-by-Step Guide (Soderberg, 2018)", "Id": "h.1eatzffhrw3j", "Main_Takeaways": ["Materials should findable, accessible, interoperable and reusable forms. Researchers should look for repositories to decide where and how to share their data.", "A repository should contain unique and persistent identifiers, so data can be cited.", "The data is publicly searchable with licenses clarifying how data is reused.", "Rich meta-data descriptions are provided to allow data to be understandable and reusable.", "Open science framework is a free and open-source Web tool to help researchers collaboratively manage, store and share the research process and the files related to their research.", "Step 1: create an account on https://osf.io", "Step 2: Sign-in your account. Enter name and password or login through your institution.", "Step 3: Create a project. Press the green button to create a new project.", "Step 4: Add Collaborators to the project.", "Click on Contributors and press +Add green button. Search for contributors by name and click on the green + button.", "If a collaborator does not come up in search, add them to the project by clicking add as an unregistered contributor link.", "Step 5: upload files- there is no more than 5GB.", "Step 6: Add a description of the project so you and other users \u00a0know what files relate to.", "Step 7: Add a License: reuse is one of the main purposes of the data sharing. Other researchers know how they are allowed to reuse your work.", "Step 8: Add component: place data, analysis script and study materials should be placed in the project. \u00a0Click the Add contributors checkbox before clicking on the green Create button.", "Step 9: Share your project with reviewers. Project is set up that you may want or need to give reviewers access to the contents of your project before you make it public.", "Step 10: Make a project public. To make a project public, press the \u00a0\u201cmake public\u201d button in the top right corner of the project page. Anyone will be able to view and download all files.", "Step 11: Reference open science files in your work. Include the links in the manuscript, lab website or the published article. Ensure readers find the specific links in manuscript, lab website or published article to make the data accessible and useful."], "Abstract": "Sharing data, materials, and analysis scripts with reviewers and readers is valued in psychological science. To facilitate this sharing, files should be stored in a stable location, referenced with unique identifiers, and cited in published work associated with them. This Tutorial provides a step-by-step guide to using OSF to meet the needs for sharing psychological data.", "Reference": "Soderberg, C. K. (2018). Using OSF to share data: A step-by-step guide. Advances in methods and practices in psychological science, 1(1), 115-120. https://doi.org/10.1177/2515245918757689", "You_may_also_be_interested_in": [{"Relevant_ref": "Trust Your Science? Open Your Data and Code (Stodden, 2011)", "href": "#h.3aofr2eo9r9l"}, {"Relevant_ref": "Attitudes Toward Open Science and Public Data Sharing: A Survey Among Members of the German Psychological Society (Abele-Brehm et al., 2019)", "href": "#h.10dzxpbtal8"}, {"Relevant_ref": "Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011)", "href": "#h.1mu3kuufj5he"}]}, "summary_56": {"Title": "On supporting early-career black scholars (Roberson, 2020) \u00a0\u233a", "Id": "h.lhl8q2qj1yrj", "Main_Takeaways": ["Non-black researchers need to take immediate support for early-career Black scholars.", "Black doctoral students hear offensive jokes and try to push against this but there is a racist disciplinary norm.", "You silently agreed and followed this up to support them. These issues push Black scholars out of academia, since these silent signals indicate the space was not created for them.", "We must challenge white supremacy in terms of science, it will be costly to our careers but it is worse for \u00a0early-career black scholars, who face an onslaught of racist micro- and macro-aggressions on a daily basis.", "We should be proactive in our outreach. We should invite early-career Black scholars, if they have expertise to improve our research project. Our careers and science will benefit from this help.", "Do not encourage us to apply, provide us material support, share funded grants, work with us on developing aims page and write a persuasive letter of support. \u00a0Invite Black scholars to series, colloquia or conference programs.", "Inviting them will increase their credibility as experts and expand the audience\u2019s familiarity with their scholarship. Manels are now being prohibited but we need to eliminate all-white speaker panels.", "Educate yourself on rising Black scholars in your field, learn from early-career Black researchers, investigate journals that publish their scholarships, be familiar with the Black community\u2019s professional societies, affinity groups and diversify your following list on Twitter.", "Incorporate their work into your syllabi. This is necessary to eliminate structural racism. However, it requires individuals with the most amount of power. These steps will promote Black people to thrive among trainees and early-career scholars.", "This will remove barriers to promote a more inclusive environment!"], "Abstract": "Professor Mya Roberson provides a detailed commentary about the struggles that Black people encounter in academia and starting steps to eliminate structural racism.", "Reference": "Roberson, M. L. (2020). On supporting early-career Black scholars. Nature Human Behaviour, 1-1. https://doi.org/10.1038/s41562-020-0926-6", "You_may_also_be_interested_in": [{"Relevant_ref": "Open Science Isn\u2019t Always Open to All Scientists (Bahlai et al., 2019)", "href": "#h.b6say8wj1wto"}]}, "summary_57": {"Title": "", "Id": "h.cabgdujacj28", "Main_Takeaways": ["We adhere to 5% for false positives but we pay little attention to false negatives.", "Scientists are humans and respond to incentives where personal success is related to quality and quantity of publications produced.", "A single transformative study might produce great prestige if accepted in a highly regarded journal.", "This prestige depends on whether a high-risk high-reward strategy is used. If the desired results are not produced, the journal may not accept it for publication.", "Authors may conduct a larger number of smaller studies, likely to produce publishable findings instead of using limited resources in a smaller number of larger studies.", "If experiments are repeated with the same sample size, on average, 50% of the time, these studies will be replicated.", "There is a need for structural change that is necessary to enforce rigorous requirements and editors of guidelines.", "Journals are introducing registered reports or registered replication reports."], "Abstract": "A comment by Dr Ivan Vankov, Professors Jeffrey Bowers and Marcus Munafo on the persistence of low power in psychological sciences. They discuss issues concerning false negatives, the importance of highly-regarded journals and that power is an issue to be discussed. They state that we need structural changes in journals in order to avoid the replicability crisis.", "Reference": "Vankov, I., Bowers, J., & Munaf\u00f2, M. R. (2014). Article commentary: On the persistence of low power in psychological science. Quarterly journal of experimental psychology, 67(5), 1037-1040. https://doi.org/10.1080/17470218.2014.885986", "You_may_also_be_interested_in": [{"Relevant_ref": "Is science really facing a reproducibility crisis, and do we need it to? (Fanelli, 2018)", "href": "#h.43r13mkeborf"}, {"Relevant_ref": "Registered Reports: Realigning incentives in scientific publishing (Chambers et al., 2015)", "href": "#h.4q6lmxfiq5cj"}, {"Relevant_ref": "Registered Reports: A new publishing initiative at Cortex (Chambers, 2013)", "href": "#h.d0r37aqo7vhl"}, {"Relevant_ref": "Registered Reports: A step change in scientific publishing (Chambers, 2014)", "href": "#h.lopn1u3ro9l1"}, {"Relevant_ref": "Registered reports: a method to increase the credibility of published results (Nosek & Lakens, 2014)", "href": "#h.2azmqwvaq115"}, {"Relevant_ref": "Registered reports (Jamieson et al., 2019)", "href": "#h.sc7lko114tkn"}, {"Relevant_ref": "Rein in the four horsemen of irreproducibility (Bishop, 2019)", "href": "#h.rrhza31fwxpw"}]}, "summary_58": {"Title": "Publication Decisions and their possible effects on inferences drawn from tests of significance or vice versa (Sterling, 1959)", "Id": "h.mdfswd9pzpwa", "Main_Takeaways": ["There is a risk to reject the null hypothesis.", "Test of significance evaluates observed difference and the probability of results.", "Depending on the confidence of methodology and data collection, readers can reject or accept the null hypothesis.", "Acceptance and rejection of null hypothesis is taken at p < .05.", "When a fixed level is used as a criterion, it may result in embarrassing and surprising results.", "What are the inferences drawn from the statistical tests, if the reader is not aware of the dependent variable? Can the reader justify the same level of significance? The author will reject the null hypothesis.", "What risks for type I or II error happen by rejecting null hypothesis? Risk from the author cannot be accepted at face value once printed."], "Abstract": "There is some evidence that in fields where statistical tests of significance are commonly used, research which yields non-significant results is not published. Such research being unknown to other investigators may be repeated independently until eventually by chance a significant result occurs - an \"error of the first kind\" - and is published. Significant results published in these fields are seldom verified by independent replication. The possibility thus arises that the literature of such a field consists in substantial part of false conclusions resulting from errors of the first kind in statistical tests of significance.", "Reference": "Sterling, T. D. (1959). Publication decisions and their possible effects on inferences drawn from tests of significance\u2014or vice versa. Journal of the American statistical association, 54(285), 30-34.\u00a0https://doi.org/10.1080/01621459.1959.10501497", "You_may_also_be_interested_in": [{"Relevant_ref": "Relevant references will be added soon"}]}, "summary_59": {"Title": "Replicability as a Publication Criterion (Lubin, 1957)", "Id": "h.ejuzqrvlmcd0", "Main_Takeaways": ["How can we reduce publication lag?", "Perform replications to show the results were repeated.", "Replicability and generalisability are not new criteria and these need to be performed to judge the rigour of an article.", "Articles with replication designs are not adequate to the editor and made the lowest publication priority.", "If results are replicated, it is not important to discuss other small variables.", "If these results are not replicated, it could result from several factors (e.g. time of day, temperature, kind of food eaten etc.)"], "Abstract": "A comment by Dr Ardie Lubin on replicability being a criterion of publication. Replications are seen as fundamental but may not be seen as adequate to the editor, thus are seen as the lowest publication priority. However, replications are important to remove any trivial variables that may explain the findings.", "Reference": "Lubin, A. (1957). Replicability as a publication criterion. American Psychologist, 12(8), 519-520. https://doi.org/10.1037/h0039746", "You_may_also_be_interested_in": [{"Relevant_ref": "Relevant references will be added soon"}]}, "summary_60": {"Title": "The life of p: \u201cJust significant\u201d results are on the rise (Leggett et al., 2013)", "Id": "h.b910lefd5hzk", "Main_Takeaways": ["We use computers to execute analyses to produce precise p values. Modern software enables simple and instantaneous calculations, allowing researchers to monitor data while collecting it.", "This ease to analyse data produces issues of optional stopping and selective exclusion of outliers.", "All practices manipulate p-values and drive them towards significance.", "The current study measured whether there is a spike in p values at .05 and whether there is an over-representation of p values below .05 for 2005 than 1965.", "Method: \u00a0P values were collected for all articles published between 1965 and 2005.", "Method: P values that were incorrectly reported, \u00a0categorised as p < .05 and .01 or values reported only 2 decimal places were recalculated to an accuracy of 6 decimal places.", "Method: Insufficient information available to determine the exact p values, thus data was excluded from analyses. P values around significance cut-off point of .05 values and between .01 and .10 were the main focus.", "Results: \u00a0Frequency of p <.05 was greater than expected compared to p frequencies in other ranges.", "Results: \u00a0Over-representation found for values published in both 1965 and 2005, much greater for the latter.", "Results: \u00a0P values close but over .05 were more likely to be rounded down or incorrectly reported in 2005 than in 1965.", "Magnitude of spike at .05 is larger in recent articles than in 1965. Majority was inaccurately rounding p values.", "Changes in how statistical analyses are executed due to shifting research climates may explain this spike.", "Values outside this cut-off point should not be seen as significant. Trends should not be seen as trends but non-significant.", "Advances in analytical procedure make it easier to engage in suboptimal research practices.", "We need to use confidence intervals and effect sizes, mandatory methods disclosure and registered reporting.", "Reduce focus on p values and encourage use of optimal research practices. P values along is prone to human fallibility."], "Abstract": "Null hypothesis significance testing uses the seemingly arbitrary probability of .05 as a means of objectively determining whether a tested effect is reliable. Within recent psychological articles, research has found an overrepresentation of p values around this cut-off. The present study examined whether this overrepresentation is a product of recent pressure to publish or whether it has existed throughout psychological research. Articles published in 1965 and 2005 from two prominent psychology journals were examined. Like previous research, the frequency of p values at and just below .05 was greater than expected compared to p frequencies in other ranges. While this overrepresentation was found for values published in both 1965 and 2005, it was much greater in 2005. Additionally, p values close to but over .05 were more likely to be rounded down to, or incorrectly reported as, significant in 2005 than in 1965. Modern statistical software and an increased pressure to publish may explain this pattern. The problem may be alleviated by reduced reliance on p values and increased reporting of confidence intervals and effect sizes.", "Reference": "Leggett, N. C., Thomas, N. A., Loetscher, T., & Nicholls, M. E. (2013). The life of p:\" just significant\" results are on the rise. Quarterly journal of experimental psychology (2006), 66(12), 2303. https://doi.org/10.1080/17470218.2013.863371", "You_may_also_be_interested_in": [{"Relevant_ref": "How scientists can stop fooling themselves (Bishop, 2020b)", "href": "#h.vmo2rkmiz4ms"}, {"Relevant_ref": "The Statistical Crisis in Science (Gelman & Loken, 2014)", "href": "#h.rc4vbzxkf0ax"}, {"Relevant_ref": "Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014)", "href": "#h.6m58pstvoxmx"}, {"Relevant_ref": "A 21 Word Solution (Simmons et al., 2012)", "href": "#h.hxo3saai5nvg"}, {"Relevant_ref": "Rein in the four horsemen of irreproducibility (Bishop, 2019)", "href": "#h.rrhza31fwxpw"}, {"Relevant_ref": "False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011)", "href": "#h.sxh34fk82cfh"}, {"Relevant_ref": "Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019)", "href": "#h.jwujkwh9t0yi"}]}, "summary_61": {"Title": "Experimental power comes from powerful theories \u2013 the real problem in null hypothesis testing (Ashton, 2013)", "Id": "h.ey0j6cy8z74r", "Main_Takeaways": ["Although null hypothesis testing is a powerful tool for decision making.", "Power analysis of an effect size must be carried out before an experiment to test the null hypothesis against an alternative-based hypothesis based on effect size.", "The alternative hypothesis is left open with the effect size being estimated from data.", "A vague open hypothesis in neuroscience is open to being amenable, we need more replications.", "Alternative hypothesis tests depend on theory and the problems being investigated.", "Observations are retained if below significance threshold moderated by spurious statistical significance if open-ended search for more nuanced levels of effect lead to false positives."], "Abstract": "A commentary by John C. Ashton who discusses the paper written by Professor Kate Button on small sample sizes. Ashton argues that power analyses and effects sizes should be used to estimate the alternative hypothesis.", "Reference": "Ashton, J. C. (2013). Experimental power comes from powerful theories\u2014the real problem in null hypothesis testing. Nature Reviews Neuroscience, 14(8), 585-585. https://doi.org/10.1038/nrn3475-c2", "You_may_also_be_interested_in": [{"Relevant_ref": "Negative results are disappearing from most disciplines and countries (Fanelli, 2011)", "href": "#h.7flonowsfw4n"}, {"Relevant_ref": "Negativity towards negative results: a discussion of the disconnect between scientific worth and scientific culture (Matosin et al., 2014)", "href": "#h.dziny5s31t"}]}, "summary_62": {"Title": "Negative results are disappearing from most disciplines and countries (Fanelli, 2011)", "Id": "h.7flonowsfw4n", "Main_Takeaways": ["There is a worry that scientific knowledge might have to endure the loss of negative results.", "Method: 4600 papers in all disciplines between 1990 and 2007 were used, including variables such as frequency of papers to test a hypothesis and a report to support it.", "Method: Country of location was included, information on year of publication and country was coded. Whether the evidence was positive or negative.", "Results: Frequency of positive findings increased between 1990 and 2007 by 22%. This increase was larger in social and some biomedical disciplines.", "Results: There were fewer positive results published by in American than Asian countries. More positive results in American than European countries.", "Negative results decreased in frequency across disciplines due to publication bias.", "The authors seem to suggest that science is now closer to truth today than 20 years ago.", "There is an editorial bias that favours the United States that enables them to publish as many or more negative results than any other country, not fewer. The United States has a stronger bias against negative findings than Europe.", "Self-correcting principles do not work efficiently where theoretical predictions are less accurate, methodologies are less codified and true replications are rare."], "Abstract": "Concerns that the growing competition for funding and citations might distort science are frequently discussed, but have not been verified directly. Of the hypothesized problems, perhaps the most worrying is a worsening of positive-outcome bias. A system that disfavours negative results not only distorts the scientific literature directly, but might also discourage high-risk projects and pressure scientists to fabricate and falsify their data. This study analysed over 4,600 papers published in all disciplines between 1990 and 2007, measuring the frequency of papers that, having declared to have \u2018\u2018tested\u2019\u2019 a hypothesis, reported a positive support for it. The overall frequency of positive supports has grown by over 22% between 1990 and 2007, with significant differences between disciplines and countries. The increase was stronger in the social and some biomedical disciplines. The United States had published, over the years, significantly fewer positive results than Asian countries (and particularly Japan) but more than European countries (and in particular the United Kingdom). Methodological artefacts cannot explain away these patterns, which support the hypotheses that research is becoming less pioneering and/or that the objectivity with which results are produced and published is decreasing.", "Reference": "Fanelli, D. (2012). Negative results are disappearing from most disciplines and countries. Scientometrics, 90(3), 891-904. https://doi.org/10.1007/s11192-011-0494-7", "You_may_also_be_interested_in": [{"Relevant_ref": "Experimental power comes from powerful theories \u2013 the real problem in null hypothesis testing (Ashton, 2013)", "href": "#h.ey0j6cy8z74r"}, {"Relevant_ref": "Negativity towards negative results: a discussion of the disconnect between scientific worth and scientific culture (Matosin et al., 2014)", "href": "#h.dziny5s31t"}]}, "summary_63": {"Title": "Negativity towards negative results: a discussion of the disconnect between scientific worth and scientific culture (Matosin et al., 2014)", "Id": "h.dziny5s31t", "Main_Takeaways": ["There is pressure on scientists to choose investigative avenues in high-impact knowledge.", "Scientists pursue research that is high in impact, not that is hypothesis-driven.", "Negative results are not given the same value as positive results.", "Scientific principles are under reconsideration and occasions- new evidence refutes old hypotheses.", "Negative findings are seen as an inconvenient truth, ignoring equivocal findings is only human.", "Science is a collaborative discipline and we should report negative findings, so we do not waste time and resources repeating our findings.", "When time is money, research output is judged based on impact and citations, why waste time?", "We face resistance at scientific conferences, when disseminating evidence as we were criticised. Why is a negative finding viewed as a bad thing? A negative is seen as philosophical than practical.", "If negative questions are rephrased as positive questions, does that mean a negative finding is a positive finding?", "Negative findings are seen as taboo and worthy of publication and clinical relevance translated to other related research fields.", "Negative results are not worthy of attention, thus placed in a file drawer and seen as less important.", "We should determine the importance not by the process of \u201cpublish or perish\u201d but hypothesis-driven science to fill holes in our knowledge."], "Quote": "\u201cAt the core, it is our duty as scientists to both: (1) publish all data, no matter what the outcome, because a negative finding is still an important finding; and (2) have a hypothesis to explain the finding. If the experiment has been performed to plan, the data has not been manipulated or pulled out of context and there is compiled evidence of a negative result, then it is our duty to provide an explanation as to why we are seeing what we are seeing. Only by truly rethinking the current scientific culture, which clearly favours positive findings, will negative results be esteemed for their entire value. Only then can we work towards an improved scientific paradigm.\u201d (p.173)", "Abstract": "\u201cWhat gets us into trouble is not what we don\u2019t know, it\u2019s what we know for sure that just ain\u2019t so.\u201d \u2013 Mark Twain. Science is often romanticised as a flawless system of knowledge building, where scientists work together to systematically find answers. In reality, this is not always the case. Dissemination of results are straightforward when the findings are positive, but what happens when you obtain results that support the null hypothesis, or do not fit with the current scientific thinking? In this Editorial, we discuss the issues surrounding publication bias and the difficulty in communicating negative results. Negative findings are a valuable component of the scientific literature because they force us to critically evaluate and validate our current thinking, and fundamentally move us towards unabridged science.", "Reference": "Matosin, N., Frank, E., Engel, M., Lum, J. S., & Newell, K. A. (2014). Negativity towards negative results: a discussion of the disconnect between scientific worth and scientific culture. Disease Models & Mechanisms, 7(2), 171. https://doi.org/10.1242/dmm.015123", "You_may_also_be_interested_in": [{"Relevant_ref": "Experimental power comes from powerful theories \u2013 the real problem in null hypothesis testing (Ashton, 2013)", "href": "#h.ey0j6cy8z74r"}, {"Relevant_ref": "Negative results are disappearing from most disciplines and countries (Fanelli, 2011)", "href": "#h.7flonowsfw4n"}]}, "summary_64": {"Title": "A farewell to Bonferroni: the problems of low statistical power and publication bias (Nakagawa, 2004)", "Id": "h.gir6qelk1er7", "Main_Takeaways": ["The statistical power of a null hypothesis is the probability to reject the null hypothesis when the alternative is correct.", "Using several effect sizes: Cohen\u2019s d and Pearson\u2019s r. One can assess the mean difference using Cohen\u2019s d or Pearson\u2019s r to assess the strength of the relationship.", "There is a greater opportunity to make a false negative.", "Bonferroni correction tries to reduce false positives when multiple tests or comparisons are performed.", "Sequential Bonferroni procedure leads to a reduction in power.", "Standard Bonferroni correction- a statistical power of each t-test drops to as low as 33%", "Sequential Bonferroni is not as severe as Standard Bonferroni correction.", "Reviewers may demand Bonferroni to remove irrelevant variables and reduce the number of false positives but it can still lead to publication bias.", "We should discourage Bonferroni or practice of reviewers demanding Bonferroni. We are focusing on p values, not biological or statistical values. You need effect sizes and p values for the importance of these effects.", "Also, report confidence intervals for effect sizes."], "Abstract": "Professor Shinichi Nakagawa provides a commentary on low statistical power and the need to discourage Bonferroni corrections. In addition, we should rely on effect sizes and their confidence intervals to determine the value of science findings.", "Reference": "Nakagawa, S. (2004). A farewell to Bonferroni: the problems of low statistical power and publication bias. Behavioral ecology, 15(6), 1044-1045. https://doi.org/10.1093/beheco/arh107", "You_may_also_be_interested_in": [{"Relevant_ref": "Relevant references will be added soon"}]}, "summary_65": {"Title": "The File-drawer problem revisited: A general weighted method for calculating fail-safe numbers in meta analysis (Rosenberg, 2005)", "Id": "h.78nm3viryw3a", "Main_Takeaways": ["There is a file-drawer problem in which studies that observe no significant effects are not published.", "One measure to assess the number of non-significant findings is a fail-safe number. These unpublished studies need to be added to a meta-analysis to reduce significant results to non-significance.", "This approach is the best avenue to approach publication bias but identify whether more complex processes are necessary. We should consider unweighted effect sizes.", "Studies with larger sample size or small variance are given higher weight than small sample sizes or large variance.", "Sample size is equivalent to studies with null effect and mean weight necessary to reduce significance level.", "N is the size of a single study of no effect. The relative size means a single study would need to be weighted.", "Degrees of freedom need to be considered.If N is interpreted as multiple studies of mean weight.", "N must be solved for iteratively and variations in t are small with moderate degree of freedom, only a few iterations are needed for convergence.", "Fail-safe calculation is not a method to identify publication bias or explain the publication that exists; it is a procedure to estimate publication bias if it exists and may be safely ignored."], "Abstract": "Quantitative literature reviews such as meta-analysis are becoming common in evolutionary biology but may be strongly affected by publication biases. Using fail-safe numbers is a quick way to estimate whether publication bias is likely to be a problem for a specific study. However, previously suggested fail-safe calculations are unweighted and are not based on the framework in which most meta-analyses are performed. A general, weighted fail-safe calculation, grounded in the meta-analysis framework, applicable to both fixed- and random-effects models, is proposed. Recent meta-analyses published in Evolution are used for illustration.", "Reference": "Rosenberg, M. S. (2005). The file\u2010drawer problem revisited: a general weighted method for calculating fail\u2010safe numbers in meta\u2010analysis. Evolution, 59(2), 464-468.https://doi.org/10.1111/j.0014-3820.2005.tb01004.x", "You_may_also_be_interested_in": [{"Relevant_ref": "The \u201cFile Drawer Problem\u201d and Tolerance for Null Results (Rosenthal, 1979)", "href": "#h.b30ist4dfi61"}]}, "summary_66": {"Title": "The \u201cFile Drawer Problem\u201d and Tolerance for Null Results (Rosenthal, 1979)", "Id": "h.b30ist4dfi61", "Main_Takeaways": ["The file drawer problem is that journals have 5% of articles with false positives, while file drawers have 95% non-significant results.", "We need to calculate the number of studies with null findings before the overall false positives are made.", "Another conservative alternative is that when the exact p levels are not present is to set Z = .00 for any non-significant and Z = 1.645 for p < .05.", "Small number of studies that are not significant, even when combined can be distorting and misleading with a few studies making a significant effect, non-significant.", "Currently, there are no firm guidelines that can constitute an unlikely number of unretrieved or unpublished studies.It could be 100 or 500, while for others even 10 or 20 seem unlikely."], "Quote": "\u201c[...] more and more reviewers of research literature are estimating average effect sizes and combined ps of the studies they summarize. It would be very helpful to readers if for each combined p they presented, reviewers also gave the tolerance for future null results associated with their overall significance level.\u201d (p.640)", "Abstract": "For any given research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the \"file drawer problem\" is that journals are filled with the 5% of the studies that show Type I errors, while the file drawers are filled with the 95% of the studies that show nonsignificant results. Quantitative procedures for computing the tolerance for filed and future null results are reported and illustrated, and the implications are discussed.", "Reference": "Rosenthal, R. (1979). The file drawer problem and tolerance for null results. Psychological bulletin, 86(3), 638\u2013641. https://doi.org/10.1037/0033-2909.86.3.638. [ungated]", "You_may_also_be_interested_in": [{"Relevant_ref": "The File-drawer problem revisited: A general weighted method for calculating fail-safe numbers in meta analysis (Rosenberg, 2005)", "href": "#h.78nm3viryw3a"}]}, "summary_67": {"Title": "At random: Sense and Nonsense (McNemar, 1960)", "Id": "h.57aolhm1f18e", "Main_Takeaways": ["Psychology is seen as a quintessential behavioural science.", "1% of the American Psychological Association are statisticians and 3% of abstracts deal with statistics.", "Survival of psychology can either be due to little discoveries that are scientifically and statistically significant and to the number of little men who are significant to insignificant others.", "There are too many test builders that think to repel statisticians to neglect statistical and psychometric theory of test construction. They produce difficult and voluminous writings and may not be suitable to promote the use of the statistical methods.", "Chi square test was involved in psychology until the 1930s but it was misused, leading to mistaken significance level.", "ANOVAs became more popular in the late 1930s. During this time, more complex designs were included and seen as a better approach to psychology.", "Significance tests are necessary but not sufficient conditions for the development of a science.Co-variance method is a real blessing to social, child, clinical and educational psychology.", "We have to use an interval scale for means, standard deviation and correlation coefficients such as Pearson\u2019s r with log-transformed scores.", "Spearman\u2019s rho is between two sets of rank.", "A simple design with a simple statistical treatment all leads to a simple conclusion. What could be simpler? Why not a simple approach?", "Mathematical statisticians can no longer keep up with each other, so what is the hope for psychology?", "Statistics teaching will be incomprehensible to most psychology students, they need a sound understanding to have an intelligent and critical reading of the literature.", "Journal editors may reject manuscripts with negative findings."], "Abstract": "A commentary by Dr Quinn McNemar who discusses sense and nonsense data, the difficulties of statistical teaching and the advancements of research design and statistics.", "Reference": "McNemar, Q. (1960). At random: Sense and nonsense. American Psychologist, 15(5), 295\u2013300. https://doi.org/10.1037/h0049193", "You_may_also_be_interested_in": [{"Relevant_ref": "Relevant references will be added soon"}]}, "summary_68": {"Title": "Replication Report: Interpretation of levels of significance by psychological researchers (Beauchamp & May, 1964)", "Id": "h.sk3d8m7l5mbq", "Main_Takeaways": ["Importance: one of the first papers dealing with replication in psychology.", "Study: Psychology lecturers were more cautious than psychology graduate students when making confidence judgments about research findings for the p value.", "Methods: Subjects had to state the degree of belief in research findings as a result of associated p levels. Subjects rated each of the 12 p\u00a0levels for each sample size on a six point scale from 5 to 0.", "Methods:\u00a0They knew results based on 100 produced more precise effects than results based on 10 for significance testing.", "Results: There was a difference in confidence related to sample size, which varied across sample size.", "Discussion: \u00a0There was no cliff effect following p < \u00a0.05, .01, or any other p value."], "Abstract": "A commentary by Drs Kenneth Beauchamp and Richard May who investigated confidence judgments about research findings for p value.", "Reference": "Beauchamp, K. L., & May, R. B. (1964). Replication Report: Interpretation of Levels of Significance by Psychological Researchers. Psychological Reports, 14(1), 272-272. https://doi.org/10.2466/pr0.1964.14.1.272\u00a0[ungated]", "You_may_also_be_interested_in": [{"Relevant_ref": "Further evidence for the Cliff Effect in the Interpretation of Levels of Significance (Rosenthal & Gaito, 1964)"}]}, "summary_69": {"Title": "Further evidence for the Cliff Effect in the Interpretation of Levels of Significance (Rosenthal & Gaito, 1964)", "Id": "h.jefmzz864f9t", "Main_Takeaways": ["Importance: one of the first papers dealing with replication in psychology.", "There was a non-montonicity decrease of confidence as p values increased.", "11 graduate student subjects showed greater degree of confidence in .05, not .03 level.", "There is something special about .05 characteristics.", "Probability of .05 possesses a sharp decrease in confidence."], "Abstract": "A commentary by Drs Robert Rosenthal and John Gaito who investigated confidence judgments about research findings and confidence in the levels of significance.", "Reference": "Rosenthal, R., & Gaito, J. (1964). Further Evidence for the Cliff Effect in the Interpretation of Levels of Significance. Psychological Reports, 15(2), 570-570. https://doi.org/10.2466/pr0.1964.15.2.570", "You_may_also_be_interested_in": [{"Relevant_ref": "Replication Report: Interpretation of levels of significance by psychological researchers (Beauchamp & May, 1964)"}]}, "summary_70": {"Title": "Ten Simple Rules for Effective Statistical Practice (Kass et al., 2016)", "Id": "h.rb2pj2fi4jhg", "Main_Takeaways": ["Inexperienced users of statistics take data and scientific issues for granted and base on data structure instead of the scientific goal.", "We need statistical experts with scientific collaborators to answer questions and assess what kinds of studies might be useful.", "Signals always come with noise-grappling with variability is crucial in statistics.", "Statistical analyses measure the signal provided by data, which provides interesting variability in the presence of noise or irrelevant variability.", "We should plan ahead before data collection such as what should my sample size be? Someone with statistical experience should consider many aspects of data collection and ask what is the ideal outcome of the experiment and how would you interpret it?", "We need to worry about data quality: garbage in, garbage out. We need hands on experience and data cleaning to reveal important concerns about data quality about what is measured and the losses that are cut early.", "Statistical analyses are more than a set of computations, they are there to assist, not define analyses. Include references to pieces of software and spell out clearly whether the analytical technique is linked to the question being answered.", "Keep it simple: start with simple approaches then add complexity. Consider assumptions, especially independence, which is incorrect and requires careful examination.", "Nearly all biological measurements show variation and create uncertainty in results of every calculation based on data.", "Extra variability means extra uncertainty in conclusions should be reported.", "Check your assumptions.", "Some assume linear relationships: linearity works well as a first approximation when the amount of noise makes it difficult to discriminate linear and non-linear relationships.", "Multiple observations are statistically independent.", "Missing data and systematic biases in measurement and several factors violate assumptions of statistical models.", "When possible, replicate!", "Include several visualisations and slices of the data. Ignoring this reality is dishonest when inferences look at data, they no longer have their usual interpretation.", "Make the analysis reproducible-different standards are used to make it easier to achieve is reproducible- a complete description to reproduce tables, figures and inferences.", "Statistics is a language to aid the process. To achieve full fluency, one needs training and practice. However, it is important to treat statistics as a science, not a recipe."], "Abstract": "A commentary by Dr Robert Kass providing 10 rules about effective statistical practices and how to improve statistical practices.", "Reference": "Kass, R. E., Caffo, B. S., Davidian, M., Meng, X. L., Yu, B., & Reid, N. (2016). Ten Simple Rules for Effective Statistical Practice. Plos Computational Biology, 12(6), e1004961-e1004961. https://doi.org/10.1371/journal.pcbi.1004961"}, "summary_71": {"Title": "Raise standards for preclinical cancer research (Begley & Ellis, 2012)", "Id": "h.rogo8o84i3hs", "Main_Takeaways": ["Clinical trials in oncology have the highest failure rates.", "Barriers to clinical development may be lower and a large number of drugs with suboptimal preclinical validation enter oncology trials. Low success rate is not sustainable or acceptable.", "Contributor to failure in oncology trials in quality of published pre-clinical data.", "Drug development depends on literature with new targets and biology-clinical endpoints focus on survival instead of intermediate endpoints e.g. cholesterol levels for statins.", "Results of preclinical studies must be robust to withstand rigours and challenges of clinical trials.", "Claims in a preclinical study needs to be taken at face value.", "Issue of irreproducible data has been discussed and received greater attention at costs of drug development.", "We need to contact original authors for mixed findings, exchange reagents and repeat experiments under authors\u2019 direction.", "In studies for which findings could be reproduced, authors pay close attention to controls, reagents, investigator bias and describing complete dataset.", "These results do not mean the entire system is flawed.", "How can robustness of published preclinical cancer research be increased? We need commitment and change of prevalent cultures.", "We need to consider negative preclinical data and report all findings, irrespective of the outcome. The funding agencies, reviewers and journal editors agree negative data is as informative as positive data.", "There are transparent opportunities for trainees, technicians and colleagues to discuss and report troubling or unethical behaviours without fearing adverse consequences.", "There needs to be a greater dialogue between physicians, scientists, patient advocates and patients.", "Scientists need to learn about clinical reality, whereas physicians need better knowledge of challenges and limitations of preclinical studies. Both groups benefit from improved understanding of patients\u2019 concerns.", "Institutions and committees should give more credit for teaching and mentoring, relying solely on publications for promotion or grant funding can be misleading and does not recognise the valuable contribution of greater mentors, educators and administrators.", "The Academic system and peer review process encourages erroneous, selective or irreproducible data."], "Abstract": "Glenn Begley and Lee M. Ellis propose how methods, publications and incentives must change if patients are to benefit.", "Reference": "Begley, C. G., & Ellis, L. M. (2012). Raise standards for preclinical cancer research. Nature, 483(7391), 531-533. https://doi.org/10.1038/483531a"}, "summary_72": {"Title": "The cumulative effect of reporting and citation biases on the apparent efficacy of treatment: the case of depression (deVries et al., 2018)", "Id": "h.6nejty9i8thl", "Main_Takeaways": ["We analysed the cumulative influence of bias on efficacy and discussed remedies using evidence base for two effective treatments for depression: antidepressants and psychotherapy.", "Trials report non-significant results produce accurate effect size estimates but results interpretation is positively biased, which influence apparent efficacy. If an article has been spun, treatments are more beneficial.", "Citation bias is an obstacle to ensure negative findings are discoverable.", "105 antidepressant trials-53 (50%) trials were considered positive by FDA and 52 (50%) were considered negative or questionable.", "All but one of positive trials (98%) were published, only 25 (48%) of negative trials were published.", "10 negative trials become positive in published literature, omitting unfavourable outcomes or switching status of primary and secondary outcomes.", "5 were published with a spin in the abstract.", "Among negative trials, mild spin received 36 citations and negative abstract received 25 citations, synergistic effect-spin and citation biases-negatively presented studies receive few citations.", "Effects of different biases accumulate to hide non-significant results from view.", "Remaining abstracts were positive or mixed.", "Negative trials with positive or mixed abstracts were cited more often than those with negative abstracts.", "Increasing pressure may explain why recently completed negative antidepressants are more frequently published than older negative trials. Registration seems insufficient to ensure complete and accurate reporting of a trial.", "Close examination of registries by independent researchers may be important for registration is an effective deterrent to study publication and outcome reporting bias.", "Adoption of registered reports reduces citation bias, thus reducing a tendency for positive studies to be published in higher impact journals.", "Peer reviewers could contribute to abstracts accurately report trial results and important negative studies are cited.", "Prevalence of spin and citation bias show the importance of assessing a study\u2019s actual results and conducting independent literature searches and producing a number of positive studies.", "Publication bias is well known, which has removed most negative results and left few published negative results. \u00a0These biases are unlikely to be unique to antidepressant trials."], "Abstract": "Dr deVries and colleagues discuss the importance of a spin on clinical trials, citation biases for positive trials and the benefits of registered reports and pre-registration.", "Reference": "De Vries, Y. A., Roest, A. M., de Jonge, P., Cuijpers, P., Munaf\u00f2, M. R., & Bastiaansen, J. A. (2018). The cumulative effect of reporting and citation biases on the apparent efficacy of treatments: the case of depression. Psychological medicine, 48(15), 2453-2455. https://doi.org/10.1017/S0033291718001873", "You_may_also_be_interested_in": [{"Relevant_ref": "Likelihood of Null Effects of Large NHLBI Clinical Trials Has Increased over Time (Kaplan & Irvin, 2015)", "href": "#h.qthkdgbclv0e"}]}, "summary_73": {"Title": "Likelihood of Null Effects of Large NHLBI Clinical Trials Has Increased over Time (Kaplan & Irvin, 2015)", "Id": "h.qthkdgbclv0e", "Main_Takeaways": ["The article investigates whether null results have increased over time in the National Heart, Lung, and Blood Institute.", "Method: All large randomised controlled trials between 1970-2012 were identified.", "Method: Two independent searches to improve probability to accurately capture all related trials- one by study author and second by grant databases from 1970-2012.", "Results: 57% of papers were published prior to 2000 that showed benefit of intervention on primary outcome in comparison to only 2 among 25 (8%) trials published after 2000.", "Results: There was no change in the proportion of trials compared treatment to placebo versus active comparator.", "Results: Industry co-sponsorship was not linked to benefit but pre-registration linked to null findings.", "The probability of finding a treatment decreased as studies became more precise.", "Positive trials were more likely to be published. Following the year 2000, file drawer problems became more prominent leading to over-reported positive findings.", "There is a need to have stricter reporting standards for biases and greater rigour to suppress positive outcomes.", "Many early trials were not sufficiently powered to show changes in mortality. Effects on total mortality were null were registered or not prior to publication.", "Before the 2000s, p-level and directionality of hypotheses were changed post-hoc."], "Abstract": "We explore whether the number of null results in large National Heart Lung, and Blood Institute (NHLBI) funded trials has increased over time. We identified all large NHLBI supported RCTs between 1970 and 2012 evaluating drugs or dietary supplements for the treatment or prevention of cardiovascular disease. Trials were included if direct costs >$500,000/year, participants were adult humans, and the primary outcome was cardiovascular risk, disease or death. The 55 trials meeting these criteria were coded for whether they were published prior to or after the year 2000, whether they registered in clinicaltrials.gov prior to publication, used active or placebo comparator, and whether or not the trial had industry co-sponsorship. We tabulated whether the study reported a positive, negative, or null result on the primary outcome variable and for total mortality.17 of 30 studies (57%) published prior to 2000 showed a significant benefit of intervention on the primary outcome in comparison to only 2 among the 25 (8%) trials published after 2000 (\u03c72=12.2,df= 1, p=0.0005). There has been no change in the proportion of trials that compared treatment to placebo versus active comparator. Industry co-sponsorship was unrelated to the probability of reporting a significant benefit. Pre-registration in clinical trials.gov was strongly associated with the trend toward null findings.The number of NHLBI trials reporting positive results declined after the year 2000. Prospective declaration of outcomes in RCTs, and the adoption of transparent reporting standards, as required by clinicaltrials.gov, may have contributed to the trend toward null findings.", "Reference": "Kaplan, R. M., & Irvin, V. L. (2015). Likelihood of null effects of large NHLBI clinical trials has increased over time. PloS one, 10(8), e0132382. https://doi.org/10.1371/journal.pone.0132382", "You_may_also_be_interested_in": [{"Relevant_ref": "The cumulative effect of reporting and citation biases on the apparent efficacy of treatment: the case of depression (deVries et al., 2018)", "href": "#h.6nejty9i8thl"}]}, "summary_74": {"Title": "Publication Pressure and Scientific Misconduct in Medical Scientists (Tijdink et al., 2014)", "Id": "h.6mzquzro2mzm", "Main_Takeaways": ["Some scientists see fraudulent colleagues but increasing evidence that scientific misconduct compromises credibility of science.", "Different definitions and classifications for scientific misconduct: fabrication, falsification, plagiarism are seen as fraud.", "Publication pressure is a risk factor for scientific misconduct but has not been studied.", "The present study addresses the relationship between publication pressure, and self-reported fraud and questionable research practices.", "Method: All researchers received a survey and a publication pressure questionnaire that assessed scientific misconduct.", "Method: 315 Respondents provided demographic information on gender, age, type of specialty; years working as a scientist; appointment status; main professional activity and Hirsch index.", "Results: 15% of respondents admitted they had fabricated, falsified and plagiarised or manipulated data.", "Results: Fraud was more common among younger scientists working in a university hospital.", "Results:\u00a072% rated publication pressure as too high. Publication pressure was related to scientific misconduct severity score.", "Discussion: Publication pressure is a psychological stress. \u00a0The pressure generated by this stress affects the amount of errors made in scientific research. Data is more suited for identification of potential determinants of self-reported misconduct than they are for absolute prevalence of misconduct."], "Abstract": "There is increasing evidence that scientific misconduct is more common than previously thought. Strong emphasis on scientific productivity may increase the sense of publication pressure. We administered a nationwide survey to Flemish biomedical scientists on whether they had engaged in scientific misconduct and whether they had experienced publication pressure. A total of 315 scientists participated in the survey; 15% of the respondents admitted they had fabricated, falsified, plagiarized, or manipulated data in the past 3 years. Fraud was more common among younger scientists working in a university hospital. Furthermore, 72% rated publication pressure as \u201ctoo high.\u201d Publication pressure was strongly and significantly associated with a composite scientific misconduct severity score.", "Reference": "Tijdink, J. K., Verbeke, R., & Smulders, Y. M. (2014). Publication pressure and scientific misconduct in medical scientists. Journal of Empirical Research on Human Research Ethics, 9(5), 64-71. https://doi.org/10.1177/1556264614552421\u00a0[ungated]", "You_may_also_be_interested_in": [{"Relevant_ref": "Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018)", "href": "#h.syrrsgeyw2yr"}, {"Relevant_ref": "Signalling the trustworthiness of science should not be a substitute for direct action against research misconduct (Kornfeld & Titus, 2020)", "href": "#h.rgc9h1qzouij"}, {"Relevant_ref": "Reply to Kornfeld and Titus: No distraction from misconduct (Jamieson et al., 2020)", "href": "#h.1iofh0niziw4"}, {"Relevant_ref": "Stop ignoring misconduct (Kornfeld & Titus, 2016)", "href": "#h.iw4mcqqi8ce9"}, {"Relevant_ref": "Scientists\u2019 Reputations are Based on Getting it Right, not being Right (Ebersole et al., 2016)", "href": "#h.asb7cw2xwqfj"}]}, "summary_75": {"Title": "Using science and psychology to improve the dissemination and evaluation of scientific work (Buttliere, 2014)", "Id": "h.5cn2u86okgrv", "Main_Takeaways": ["Publishing system is an obstacle that must be overcome. We have to publish in high impact journals to be competitive and people look for the edge of competition.", "To be ahead of others can drive science and human progress but it is ineffective when we have questionable research practices, lack of open data and the file drawer problem.", "Many competitors divide manpower and no tool has all the features that scientists want. Researchers invest hours to set up their profile, learn the interface and build up their network.", "Individuals post a paper, dataset general comment, new protocol and shows up in the newsfeed of a system.", "Individuals interact with the post and the system notifies the original poster and displays the content from the same source.", "Researchers have a question they cannot find in the discussion of a paper or subfield, the system could indicate a list of experts to answer the question.", "We need individuals to engage with prosocial activities to increase impact and reduce questionable research practice and make it more difficult to engage.", "Reviews are done pre-publication and privately provide feedback or reviews are public and serve as a discussion of a certain number of comments.", "Science help is to get people to adopt the system."], "Abstract": "Here I outline some of what science can tell us about the problems in psychological publishing and how to best address those problems. First, the motivation behind questionable research practices is examined (the desire to get ahead or, at least, not fall behind). Next, behavior modification strategies are discussed, pointing out that reward works better than punishment. Humans are utility seekers and the implementation of current change initiatives is hindered by high initial buy-in costs and insufficient expected utility. Open science tools interested in improving science should team up, to increase utility while lowering the cost and risk associated with engagement. The best way to realign individual and group motives will probably be to create one, centralized, easy to use, platform, with a profile, a feed of targeted science stories based upon previous system interaction, a sophisticated (public) discussion section, and impact metrics which use the associated data. These measures encourage high quality review and other prosocial activities while inhibiting self-serving behavior. Some advantages of centrally digitizing communications are outlined, including ways the data could be used to improve the peer review process. Most generally, it seems that decisions about change design and implementation should be theory and data driven.", "Reference": "Buttliere, B. T. (2014). Using science and psychology to improve the dissemination and evaluation of scientific work. Frontiers in computational neuroscience, 8,\u00a082. https://doi.org/10.3389/fncom.2014.00082"}, "summary_76": {"Title": "Bias against research on gender bias (Cislak et al., 2018) \u00a0\u233a", "Id": "h.t7jyy2cy17mq", "Main_Takeaways": ["Some scientists see fraudulent colleagues but increasing evidence that scientific misconduct compromises credibility of science.", "Different definitions and classifications for scientific misconduct: fabrication, falsification, plagiarism are seen as fraud.", "Publication pressure is a risk factor for scientific misconduct but has not been studied.", "The present study addresses the relationship between publication pressure, and self-reported fraud and questionable research practices.", "Method: All researchers received a survey and a publication pressure questionnaire that assessed scientific misconduct.", "Method: 315 Respondents provided demographic information on gender, age, type of specialty; years working as a scientist; appointment status; main professional activity and Hirsch index.", "Results: 15% of respondents admitted they had fabricated, falsified and plagiarised or manipulated data.", "Results: Fraud was more common among younger scientists working in a university hospital.", "Results:\u00a072% rated publication pressure as too high. Publication pressure was related to scientific misconduct severity score.", "Discussion: Publication pressure is a psychological stress. \u00a0The pressure generated by this stress affects the amount of errors made in scientific research. Data is more suited for identification of potential determinants of self-reported misconduct than they are for absolute prevalence of misconduct."], "Abstract": "The bias against women in academia is a documented phenomenon that has had detrimental consequences, not only for women, but also for the quality of science. First, gender bias in academia affects female scientists, resulting in their underrepresentation in academic institutions, particularly in higher ranks. The second type of gender bias in science relates to some findings applying only to male participants, which produces biased knowledge. Here, we identify a third potentially powerful source of gender bias in academia: the bias against research on gender bias. In a bibliometric investigation covering a broad range of social sciences, we analyzed published articles on gender bias and race bias and established that articles on gender bias are funded less often and published in journals with a lower Impact Factor than articles on comparable instances of social discrimination. This result suggests the possibility of an underappreciation of the phenomenon of gender bias and related research within the academic community. Addressing this meta-bias is crucial for the further examination of gender inequality, which severely affects many women across the world.", "Reference": "Cislak, A., Formanowicz, M., & Saguy, T. (2018). Bias against research on gender bias. Scientometrics, 115(1), 189-200. https://doi.org/10.1007/s11192-018-2667-0", "You_may_also_be_interested_in": [{"Relevant_ref": "Surviving (thriving) in academia: feminist support networks and women ECRs (Macoun & Miller, 2014)", "href": "#h.by7jhssej2ru"}, {"Relevant_ref": "Global gender disparities in science (Lariviere et al., 2013)", "href": "#h.36lhjcfi5x2s"}, {"Relevant_ref": "The Pandemic and Gender Inequality in Academia (Kim & Patterson, Jr, 2020)\u25c8", "href": "#h.psety12z4b7u"}, {"Relevant_ref": "Gender in the editorial boards of scientific journals: A study on the current state of the art (Ghasemi et al., 2020) \u25c8", "href": "#h.oon3nmopqtb4"}, {"Relevant_ref": "Something\u2019s Got to Give (Flaherty, 2020) \u25c8", "href": "#h.hhzwj9smvj5g"}]}, "summary_77": {"Title": "When it is fine to fail/ Irreproducibility is not a sign of failure, but an inspiration for fresh ideas (Anon, 2020)", "Id": "h.rhbi94as9xwc", "Main_Takeaways": ["Everyone discusses reproducibility and growing recognition that results must be independently before accepted.", "The root of uncertainty is not fully understood and due to undiscovered errors in how value is measured or need for new physics.", "Experimental results question long-held theories or point to the existence of another theory altogether.", "Biomedicine and social science do not reduce the number of theories so cleanly but \u00a0include many more sources of variability.", "Authors contend that trust and confidence in the research process is helped. Researchers must talk and share the experiences of reproducibility.", "We should be careful about assuming something is inherently wrong when researchers cannot reproduce a result when adhering to the best agreed standards."], "Quote": "\u201cIrreproducibility should not automatically be seen as a sign of failure. It can also be an indication that it\u2019s time to rethink our assumptions.\u201d (p.192)", "Abstract": "The history of metrology holds valuable lessons for initiatives to reproduce results.", "Reference": "Anon (2020). It is fine to fail/Irreproducibility is not a sign of failure, but an inspiration for fresh ideas. Nature, 578, 191-192. https://doi.org/10.1038/d41586-020-00380-2", "You_may_also_be_interested_in": [{"Relevant_ref": "Relevant references will be added soon"}]}, "summary_78": {"Title": "Signalling the trustworthiness of science (Jamieson et al., 2020)", "Id": "h.yecsgzczx6oz", "Main_Takeaways": ["The research community has started to thwart human biases and increase trustworthiness of scholarly work.", "Scientists have difficulties assessing the confidence in the work and press.", "Policy makers and The public at large must base the trust decision on inappropriate grounds such as irrational biases, non-scientific beliefs and misdirection by conflicted stakeholders and malicious actors.", "Researchers can improve the understanding of how norms of science are honoured by communicating the value of practices more explicitly and transparent to clarify these misconceptions of science.", "Science advances as researchers build on each other\u2019s work, with new technological revolutions, new areas of research and previous interpretations are opened and corrected, respectively.", "Central to this progress of science is a culture of critique, replication and independent validation of results, and self correction.", "Science discourages group-think, countermands, human biases and rewards a dispassionate stance to the subject and institutionalised organised scepticism but fosters competition for scientists to replicate and challenge each other\u2019s work.", "Archiving data and analysis plans in publicly available repositories make it possible to both validate and build on results of others.", "Most journal editors publish statements of retractions to identify issues that lead to de-certification and when possible who was responsible for the paper\u2019s limitations, allowing the blame to be more narrow (cf. CRediT, as it allows us to look and identify the contributor who caused this issue, without blaming all the authors).", "Conflict of interest allows relationships to be disclosed- we need to state is there any competing interest, relevant interest or relationships.", "We have checklists and badges as a means that authors, journals and publishing platforms use to signal trustworthiness of findings.", "Many journals use plagiarism, image manipulation, independent statistics checks and verification that authors comply with community endorsed reporting and archiving standards."], "Quote": "\u201cScience enjoys a relatively high level of public trust. To sustain this valued commodity, in our increasingly polarized age, scientists and the custodians of science would do well to signal to other researchers and to the public and policy makers the ways in which they are safeguarding science\u2019s norms and improving the practices that protect its integrity as a way of knowing...beyond this peer-to-peer communication, the research community and its institutions also can signal to the public and policy makers that the scientific community itself actively protects the trustworthiness of its work.\u201d (p.19235)", "Abstract": "Trust in science increases when scientists and the outlets certifying their work honor science\u2019s norms. Scientists often fail to signal to other scientists and, perhaps more importantly, the public that these norms are being upheld. They could do so as they generate, certify, and react to each other\u2019s findings: for example, by promoting the use and value of evidence, transparent reporting, self-correction, replication, a culture of critique, and controls for bias. A number of approaches for authors and journals would lead to more effective signals of trustworthiness at the article level. These include article badging, checklists, a more extensive withdrawal ontology, identity verification, better forward linking, and greater transparency.", "Reference": "Jamieson, K. H., McNutt, M., Kiermer, V., & Sever, R. (2019). Signaling the trustworthiness of science. Proceedings of the National Academy of Sciences, 116(39), 19231-19236.https://doi.org/10.1073/pnas.1913039116", "You_may_also_be_interested_in": [{"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "Psychologists Are Open to Change, yet Wary of Rules (Fuchs et al., 2012)", "href": "#h.5oevlpnau8wr"}, {"Relevant_ref": "CJEP Will Offer Open Science Badges (Pexman, 2017)", "href": "#h.qr7alql3zi3k"}, {"Relevant_ref": "Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014)", "href": "#h.6m58pstvoxmx"}, {"Relevant_ref": "Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency (Kidwell et al., 2016)", "href": "#h.ozpykj3tfhom"}, {"Relevant_ref": "Quality Uncertainty Erodes Trust in Science (Vazire, 2017)", "href": "#h.6gb96p9e79ii"}, {"Relevant_ref": "Signalling the trustworthiness of science should not be a substitute for direct action against research misconduct (Kornfeld & Titus, 2020)", "href": "#h.rgc9h1qzouij"}, {"Relevant_ref": "Reply to Kornfeld and Titus: No distraction from misconduct (Jamieson et al., 2020)", "href": "#h.1iofh0niziw4"}, {"Relevant_ref": "Seven Easy Steps to Open Science: An Annotated Reading List (Cr\u00fcwell et al., 2019)", "href": "#h.5slx325ilt8s"}]}, "summary_79": {"Title": "On the reproducibility of meta-analyses: six practical recommendations (Lakens et al., 2014)", "Id": "h.6nk88qnf4l33", "Main_Takeaways": ["Meta analysis can produce varied results: significant or nonsignificant. The quality of meta-analysis and calculations need to be reproducible.", "Quality control is important. Reproducible meta-analyses are easier to re-analyse published meta-analyses using different inclusion criteria.", "These analyses need to be updated and prevent outdated scientific conclusions from affecting public policy or real-life application.", "A lack of openness about the data and choice for inclusion criteria is important to resolve the publication of meta-analyses.", "Meta-analyses use sample sizes per condition, means, standard deviation, confidence intervals, test statistics, the type of design and correlations between dependent observations in repeated-measure design. \u00a0These need to be considered.", "Ask the original authors to share meta-analytic data not reported in the original article. Statistics that are not included in meta analysis nor are they recommended for reporting standards by American Psychological Associations.", "Researchers who reproduce published meta-analyses found errors above 50% for effect size. They have to choose the effect size for simple effect or interactions and be consistent across studies, but they are not consistent.", "There is an incorrect inclusion of effect size. Inclusion criteria is either subjective or objectively wrong. There is a lack of description for inclusion criteria, search strategy or details how the data was extracted from the individual papers.", "We need a reporting guideline as a checklist such as Use Preferred Reporting Items for Systematic Reviews and Meta-analyses, which involves \u00a0a two-page checklist and flow of diagrams.", "Also, conflicts of interest are overlooked in meta-analyses but may influence how they are performed (e.g. Inclusion of only one\u2019s own studies).", "Checklist needs researchers to report possible sources of bias and invites authors to explain how conflicts of interest are dealt with or how they might limit conclusions drawn from meta-analyses.", "Reproducibility of literature search is important for quality control and updates of meta-analysis. Current protocols fail to guarantee reproducible literature searches.", "Pre-register a research protocol before doing actual meta-analysis. Pre-registration prevents confirmation bias and differentiate theory-driven \u00a0and data driven choices in meta-analysis.", "Large sample sizes provide high power when a strict alpha level and increases informational value of statistical inferences. Reproducible meta-analysis allows researchers to analyse a subset of studies included in meta-analysis to investigate influence of different inclusion criteria.", "Use an a priori power analysis based on a specific subset of studies that is most similar to planned research.", "Pre-registered inclusion criteria and reviewed by peers are subjective and debatable, not only researchers with different ideas but also as new ideas emerge.", "If results are not sensitive to imputation strategy, readers can be assured the results are not contingent on subjective decision.", "Sharing data for meta-analysis allows readers to evaluate appropriateness of inclusion criteria, creation of sub-categories or other subjective analytic choices and how to apply own criteria to results."], "Quote": "APA Style Reference", "Abstract": "Meta-analyses play an important role in cumulative science by combining information across multiple studies and attempting to provide effect size estimates corrected for publication bias. Research on the reproducibility of meta-analyses reveals that errors are common, and the percentage of effect size calculations that cannot be reproduced is much higher than is desirable. Furthermore, the flexibility in inclusion criteria when performing a meta-analysis, combined with the many conflicting conclusions drawn by meta-analyses of the same set of studies performed by different researchers, has led some people to doubt whether meta-analyses can provide objective conclusions. The present article highlights the need to improve the reproducibility of meta-analyses to facilitate the identification of errors, allow researchers to examine the impact of subjective choices such as inclusion criteria, and update the meta-analysis after several years. Reproducibility can be improved by applying standardized reporting guidelines and sharing all meta-analytic data underlying the meta-analysis, including Quote\u00a0from articles to specify how effect sizes were calculated. Pre-registration of the research protocol (which can be peer-reviewed using novel \u2018registered report\u2019 formats) can be used to distinguish a-priori analysis plans from data-driven choices, and reduce the amount of criticism after the results are known. The recommendations put forward in this article aim to improve the reproducibility of meta-analyses. In addition, they have the benefit of \u201cfuture-proofing\u201d meta-analyses by allowing the shared data to be re-analyzed as new theoretical viewpoints emerge or as novel statistical techniques are developed. Adoption of these practices will lead to increased credibility of meta-analytic conclusions, and facilitate cumulative scientific knowledge.", "Reference": "Lakens, D., Hilgard, J., & Staaks, J. (2016). On the reproducibility of meta-analyses: Six practical recommendations. BMC psychology, 4(1), 24. \u00a0https://doi.org/10.1186/s40359-016-0126-3", "You_may_also_be_interested_in": [{"Relevant_ref": "From pre-registration to publication: a non-technical primer for conducting meta-analysis to synthesize correlation data (Quintana, 2015)", "href": "#h.jtemahk1nbp"}]}, "summary_80": {"Title": "Specification Curve: Descriptive and Inferential Statistics on All Reasonable Specifications (Simonsohn et al., 2015) \u25c8", "Id": "h.qfqlfbtri4bb", "Main_Takeaways": ["To convert a scientific hypothesis into a testable prediction, researchers make several decisions for data analysis.", "Specification-Curve Analysis reduces the problem. Specifications are consistent with the underlying theory, predicted to be statistically valid and not redundant with other specifications in set.", "There is a large, possibly infinite, set of specifications to be run.", "Reasonable specifications include only non-redundant alternatives. Without this, researchers selectively report a few specifications in their papers. It expands on what gets reported but not beyond.", "Specification curve analysis reduces arbitrary analytical decisions, whereas preserving influence of non-arbitrary analytical decisions.", "Competent researchers disagree about whether a specification is an appropriate test of hypothesis of interest and/or valid for data at hand.", "Specification-curve analysis facilitates debates. Panel B considers two researchers who have an ex-ante agreement regarding a set of valid specifications.", "With specification-curve analysis both researchers report very similar sets of analyses.", "Panel C has two researchers who are in disagreement. Most specifications considered valid by Researcher 1 deemed invalid by Researcher 2, and vice versa. This may occur if researchers 1 and 2 base their analyses on different theories.", "Specification curve disentangles whether different conclusions originate in differences regarding sets of analyses or merely in which specific few analyses the researchers report.", "Researchers modify a model to explain initial model selection guided by fit e.g. quadratic vs. cubic polynomial.", "Another research assesses abest fitting model among class of models fit better than expected-selecting post-hoc as best.", "The third approach reports standard deviation of point estimates across alternative specifications.", "Specification Curve analysis provides a step-by-step guide to produce set or reasonable specifications, aids in identification of source of variation in results across specifications via descriptive specification curve and provides formal joint significance test for family of alternative specifications.", "If different valid analyses lead to different conclusions, traditional pre-analysis plans lead researchers to blindly pre-commit to one vs the other conclusion by pre-committing to one vs another valid analysis.", "Specification Curve analysis enables learning what conclusion hinges on.", "Specification Curve analysis, if you can come up with a large number of defensible ways to analyse data, run all of them and evaluate them across all analyses."], "Abstract": "Empirical results often hinge on data analytic decisions that are simultaneously defensible, arbitrary, and motivated. To mitigate this problem we introduce Specification-Curve Analysis, which consists of three steps: (i) identifying the set of theoretically justified, statistically valid, and non-redundant analytic specifications, (ii) displaying alternative results graphically, allowing the identification of decisions producing different results, and (iii) conducting statistical tests to determine whether as a whole results are inconsistent with the null hypothesis. We illustrate its use by applying it to three published findings. One proves robust, one weak, one not robust at all.", "Reference": "Simonsohn, U., Simmons, J. P., & Nelson, L. D. (2020). Specification curve analysis. Nature Human Behaviour,\u00a01-7.https://doi.org/10.1038/s41562-020-0912-z\u00a0[ungated]", "You_may_also_be_interested_in": [{"Relevant_ref": "False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011)", "href": "#h.sxh34fk82cfh"}]}, "summary_81": {"Title": "Prestige drives epistemic inequality in the diffusion of scientific ideas (Morgan et al., 2018) \u00a0\u233a", "Id": "h.rzwvgpt4pcda", "Main_Takeaways": ["There is no clear evidence that epistemic inequality is driven by non-meritocratic social mechanisms.", "It remains unknown how an idea spreads in the scientific community.", "If the origin does shape its scientific discourse, what is the relationship between the intrinsic fitness of the idea and its structural advantage by prestige of origin?", "The present study takes a different approach to define how faculty hiring drive epistemic inequality and is able to determine which researchers are situated in which institutions and the origin of the idea.", "Method: \u00a05032 tenured or tenure-track faculty data were collected. Data was collected from faculty hiring networks, nodes reflect university and the connections if a PhD was acquired at that university and if they held a tenure-track position.", "Networks with a self-loop contained individuals who received their PhD at the same institution and held a faculty position.", "Small departments have high placement power, while large departments have power. Elite institutions have a structural advantage.", "Faculty hiring may not contribute to the spread of every research idea. Hiring contributes to others. Faculty hiring is a possible mechanism for diffusion of ideas in academia.", "The spread of information from a varying level of prestige for universities were investigated.", "Results: Research from prestigious institutions spreads more quickly and completely than work of similar quality originating from less prestigious institutions.", "Higher quality research from less prestigious universities have similar success as lower-quality research in more prestigious universities.", "Idea dissemination in academia is not meritocratic, when the assessment of an idea\u2019s quality is objective.", "Researchers at prestigious institutions benefit from structural advantage allows ideas to be more easily spread throughout the network of institutions and impact discourse of science.", "Lower quality ideas are overshadowed by comparable ideas from more prestigious institutions, high quality ideas circulate widely, irrespective of origin."], "Abstract": "The spread of ideas in the scientific community is often viewed as a competition, in which good ideas spread further because of greater intrinsic fitness, and publication venue and citation counts correlate with importance and impact. However, relatively little is known about how structural factors influence the spread of ideas, and specifically how where an idea originates might influence how it spreads. Here, we investigate the role of faculty hiring networks, which embody the set of researcher transitions from doctoral to faculty institutions, in shaping the spread of ideas in computer science, and the importance of where in the network an idea originates. We consider comprehensive data on the hiring events of 5032 faculty at all 205 Phd.-granting\u00a0departments of computer science in the U.S. and Canada, and on the timing and titles of 200,476 associated publications. Analyzing five popular research topics, we show empirically that faculty hiring can and does facilitate the spread of ideas in science. Having established such a mechanism, we then analyze its potential consequences using epidemic models to simulate the generic spread of research ideas and quantify the impact of where an idea originates on its long-term diffusion across the network. We find that research from prestigious institutions spreads more quickly and completely than work of similar quality originating from less prestigious institutions. Our analyses establish the theoretical trade-offs between university prestige and the quality of ideas necessary for efficient circulation. Our results establish faculty hiring as an underlying mechanism that drives the persistent epistemic advantage observed for elite institutions, and provide a theoretical lower bound for the impact of structural inequality in shaping the spread of ideas in science.", "Reference": "Morgan, A. C., Economou, D. J., Way, S. F., & Clauset, A. (2018). Prestige drives epistemic inequality in the diffusion of scientific ideas. EPJ Data Science, 7(1), 40. https://doi.org/10.1140/epjds/s13688-018-0166-4", "You_may_also_be_interested_in": [{"Relevant_ref": "Early co-authorship with top scientist predicts success in academic careers (Li et al., 2019)", "href": "#h.rvz88tdscswt"}, {"Relevant_ref": "Open Science Isn\u2019t Always Open to All Scientists (Bahlai et al., 2019)", "href": "#h.b6say8wj1wto"}, {"Relevant_ref": "Scientists\u2019 Reputations are Based on Getting it Right, not being Right (Ebersole et al., 2016)", "href": "#h.asb7cw2xwqfj"}, {"Relevant_ref": "The Matthew effect in science funding (Bol et al., 2018)", "href": "#h.bbkbxhmwh8ms"}]}, "summary_82": {"Title": "Open Science Isn\u2019t Always Open to All Scientists (Bahlai et al., 2019) \u233a", "Id": "h.b6say8wj1wto", "Main_Takeaways": ["Open science focuses on accountability and transparency, invites anyone to observe, contribute and create.", "Open science focuses on conviction that research performs in dialogue with society. Science is a mainstreamed but increasing sense of competition rewards scientists who discover ideas and publish findings.", "Open science is a response to transformative technological change and web connectivity.", "Open science makes science accessible to everyone but there are systemic barriers (e.g. financial and social) that make open science more accessible to some not others such as career stage, employment stability, financial circumstance, country of origin and cultural context.", "It must be universally accessible so all people have access to dialogue of science and accessible in context means usable by all, specific interest on communities not served by scientific productions.", "Open science practices are not equally accessible to all scientists (e.g. paywalls) make research inaccessible but do not lament paywalls to prevent scientists from sharing their work.", "Certain institutions have fewer resources to overcome these barriers, not by choice but lack thereof.", "Open access is paid out of our personal funds, instead of grant or institution funding sources.", "Tenure is problematic, as applications with too many papers in open access are being dismissed and gold open-access publishing creates social pressures.", "These barriers prevent scientists from pursuing further and should not be used to deny further participation, including receiving grant funding or job applications.", "Preprints and signed peer reviews are universal goods without reinforcing inequalities. However, we should not ignore important disparities, as people of colour and women benefit from double-masked review processes to reduce active and unconscious bias.", "Open peer review assumes that everyone accepts criticism in a friendly manner and there is no form of bias. Actions in the name of progress push our field backwards.", "There are power imbalances that contribute to convincing research groups to use open sciences and are not recommended to engage in practices until stable employment or in a senior position.", "Making data available is seen as high risk as someone can publish analyses with your data before you can.", "We should not be using absolutism but relativism. We want to be perfectly open. This movement will cure many problems that exist in science but reinforce existing biases and inequalities found in academia."], "Quote": "\u201cAlthough open-science advocates claim that this movement will cure many problems that exist within science, in practice it can reinforce the existing biases and inequalities commonly found in academia in the ways we lay out in this article. Open science by itself cannot fix all the problems that its proponents claim it will solve, because the problems of bias and inequality are inherent in our broader culture. Many proponents, especially those within academia, act as if scientists are somehow removed from the cultures that have shaped us all and that we are somehow \u201cabove\u201d things such as implicit bias, ego, and pettiness\u2014as if our training in removing bias from our research somehow trains us to ignore it within ourselves\u201d \u00a0(p.15)", "Abstract": "Current efforts to make research more accessible and transparent can reinforce inequality within STEM professions.", "Reference": "Bahlai, C., Bartlett, L. J., Burgio, K. R., Fournier, A., Keiser, C. N., Poisot, T., & Whitney, K. S. (2019). Open science isn\u2019t always open to all scientists. American Scientist, 107(2), 78-82. https://doi.org/10.1511/2019.107.2.78", "You_may_also_be_interested_in": [{"Relevant_ref": "Early co-authorship with top scientist predicts success in academic careers (Li et al., 2019)", "href": "#h.rvz88tdscswt"}, {"Relevant_ref": "Prestige drives epistemic inequality in the diffusion of scientific ideas (Morgan et al., 2018)", "href": "#h.rzwvgpt4pcda"}, {"Relevant_ref": "On supporting early-career black scholars (Roberson, 2020)", "href": "#h.lhl8q2qj1yrj"}, {"Relevant_ref": "The Matthew effect in science funding (Bol et al., 2018)", "href": "#h.bbkbxhmwh8ms"}]}, "summary_83": {"Title": "Surviving (thriving) in academia: feminist support networks and women ECRs (Macoun & Miller, 2014) \u233a", "Id": "h.by7jhssej2ru", "Main_Takeaways": ["This paper argues that peer support networks to sustain feminist research allowing emerging female scholars to engage in academia.", "Women who participated in the reading group are actively intellectually engaged in theorising their own experiences.", "The group perform functions linked to reading groups, create an informal space concerned with furthering disciplinary knowledge and developing academic skills.", "Feminist reading group created a community of belong for many women who participated, providing them with personal support, knowledge, and cultural and social capital.", "Participants share resources and information about institutional processes and gain confidence to navigate complex and hostile spaces of the University.", "School\u2019s official spaces are seen as gendered and not reflective of our research interests or intellectual backgrounds.", "Participants state Feminist reading group allowed them to continue their studies in time of difficulty.", "Feminist reading groups provide opportunity to broaden exposure to other fields and improve critical thinking skills.", "Women in the feminist reading group learn essential academic skills from experience with writing and publishing to developing presentation and analytical skills without fear of seeming to be an inadequate researcher.", "Academic work can be isolating, early career researchers feel unsettled, anxious and experience self-doubt.", "Feminist reading group re-dress this opacity and operate as information sharing network for participants to learn about how things work at the University and in the department.", "Women graduates receive less mentoring, less involvement in professional and social networking than their male peers.", "Feminist reading group participation also stimulated other academic activities, with members encouraging each other to attend conferences and present paper.", "Most participants were white, straight, cis-gendered and middle class. Group was whiter than our department as a whole.", "Feminist reading reading group provide participants opportunity to understand individual experiences of exclusion, exploitation, self-doubt, discrimination as shared and fundamentally political in character.", "Our backgrounds and experiences are not homogeneous, most participants in the reading group are racially and socio-economically privileged."], "Abstract": "In this paper, we reflect upon our experiences and those of our peers as doctoral students and early career researchers in an Australian Political Science department. We seek to explain and understand the diverse ways that participating in an unofficial Feminist Reading Group in our department affected our experiences. We contend that informal peer support networks like reading groups do more than is conventionally assumed, and may provide important avenues for sustaining feminist research in times of austerity, as well as supporting and enabling women and emerging feminist scholars in academia. Participating in the group created a community of belonging and resistance, providing women with personal validation, information and material support, as well as intellectual and political resources to understand and resist our position within the often hostile spaces of the University. While these experiences are specific to our context, time and location, they signal that peer networks may offer critical political resources for responding to the ways that women\u2019s bodies and concerns are marginalised in increasingly competitive and corporatised university environments.", "Reference": "Macoun, A., & Miller, D. (2014). Surviving (thriving) in academia: Feminist support networks and women ECRs. Journal of Gender Studies, 23(3), 287-301. https://doi.org/10.1080/09589236.2014.909718", "You_may_also_be_interested_in": [{"Relevant_ref": "Global gender disparities in science (Lariviere et al., 2013)", "href": "#h.36lhjcfi5x2s"}, {"Relevant_ref": "The Pandemic and Gender Inequality in Academia (Kim & Patterson, Jr, 2020)", "href": "#h.psety12z4b7u"}, {"Relevant_ref": "Gender in the editorial boards of scientific journals: A study on the current state of the art (Ghasemi et al., 2020)", "href": "#h.oon3nmopqtb4"}, {"Relevant_ref": "Bias against research on gender bias (Cislak et al., 2018)", "href": "#h.t7jyy2cy17mq"}, {"Relevant_ref": "Something\u2019s Got to Give (Flaherty, 2020)", "href": "#h.hhzwj9smvj5g"}]}, "summary_84": {"Title": "Global gender disparities in science (Lariviere et al., 2013) \u233a", "Id": "h.36lhjcfi5x2s", "Main_Takeaways": ["Gender inequality is rife in science.", "There are gender inequalities in hiring, earnings, funding, satisfactions and patenting persist.", "Men publish more papers than women. There is no consensus for gender differences as a result of bias, childbearing or other variables.", "The present state of quantitative knowledge of gender disparities in science was shaped by anecdotal reports and studies.", "Studies consider a rise in collaborative research and other changes in scholarly practices.", "The present study analysed 5483841 research papers and review articles with 27329915 authorships.", "They assigned gender using data from the US Social Security databases.", "Citation disadvantage is highlighted by the fact that women\u2019s publication portfolios are more domestic than male colleagues and profit less from extra citations that international collaborations accrue.", "Men dominate scientific production in nearly every country to what extent varies by region.", "Women explain fewer than 30% fractionalised authorships-men represent was more than 70%. Women are underrepresented when it comes to first authorships.", "For every article with a female first author, nearly two (1.93) articles first-authored by men.", "Greater gender equality South American and Eastern European countries such as communist and formerly communist states.", "Only 9 countries had female dominance in terms of proportion of authorships, and five of these (Macedonia, Sri Lanka, Latvia, Ukraine, Bosnia and Herzegovina) had more than 1000 articles in our analysis.", "Countries with more than 1000 papers and high degrees of male dominance: Saudi Arabia, Iran, Japan, Jordan, United Arab Emirates, Cameroon, Qatar and Uzbekistan.", "US states with more than 1000 articles with a gender assigned and high male dominance include New Mexico, Mississippi and Wyoming. US states and Canadian provinces close to achieving gender equality include Vermont, Rhode Island, Maine, Manitoba, Nova Scotia and Quebec.", "Some of these states and provinces are among the lowest ranking in terms of scientific output. Female collaborations are more domestically oriented than collaborations of males from the same country.", "The present study analysed prominent author positions-sole, first- and last-authorship. When a woman was in any of these roles, paper attracted fewer citations than in cases wherein a man was in one of these roles.", "Academic pipeline from junior to senior faculty leaks female scientists, senior ranks of science bear imprint of previous generations\u2019 barriers to progression of women.", "Women\u2019s research is weaker and less than men\u2019s research.", "Barriers to women in science remain widespread worldwide, despite more than a decade of policies aimed at levelling the playing field. For a country to be scientifically competitive, it needs to maximise its human intellectual capital.", "Collaboration is one of the main drivers of research output and scientific impact, programmes fostering international collaboration for female researchers might help to level the playing field.", "No country can afford to neglect the intellectual contributions of half its population."], "Abstract": "Cassidy R. Sugimoto and colleagues present a bibliometric analysis confirming that gender imbalances persist in research output worldwide.", "Reference": "Larivi\u00e8re, V., Ni, C., Gingras, Y., Cronin, B., & Sugimoto, C. R. (2013). Bibliometrics: Global gender disparities in science. Nature News, 504(7479), 211. https://doi.org/10.1038/504211a", "You_may_also_be_interested_in": [{"Relevant_ref": "Surviving (thriving) in academia: feminist support networks and women ECRs (Macoun & Miller, 2014)", "href": "#h.by7jhssej2ru"}, {"Relevant_ref": "The Pandemic and Gender Inequality in Academia (Kim & Patterson, Jr, 2020)\u25c8", "href": "#h.psety12z4b7u"}, {"Relevant_ref": "Gender in the editorial boards of scientific journals: A study on the current state of the art (Ghasemi et al., 2020)", "href": "#h.oon3nmopqtb4"}, {"Relevant_ref": "Bias against research on gender bias (Cislak et al., 2018)", "href": "#h.t7jyy2cy17mq"}, {"Relevant_ref": "Something\u2019s Got to Give (Flaherty, 2020)", "href": "#h.hhzwj9smvj5g"}]}, "summary_85": {"Title": "The Pandemic and Gender Inequality in Academia (Kim & Patterson, Jr, 2020)\u25c8\u00a0\u233a", "Id": "h.psety12z4b7u", "Main_Takeaways": ["The COVID-19 pandemic worsened existing gender inequalities across society.", "The present study investigated the influence of the current pandemic requires addressing an academic publication pipeline best measured in months, if not years.", "If the pandemic disproportionately influences the productivity of female faculty, the effects on research productivity may not fully materialise for years and evaluation and promotion of female scholars could adversely be affected by gender-related inequalities woven into the system years before.", "The present study determined the proportion of work- and family-related tweets sent by male and female academics using subject-specific keywords.", "The pandemic caused the gender-related differences in professional tweeting to increase by 239%. \u00a0The lockdown increased the gap between male and female faculty member\u2019s propensity to tweet about family and care-giving.", "Women bear all care-giving activities- both men and women experienced an increase in family-related tweets-patterns we uncover reveal that female careers are more severely taxed by these commitments.", "Method: Our sample was narrowed to tenure-track or tenured faculty based in the United States, producing approximately 3000 handles.", "Method: We first identified all tweets related to career-promoting and family-related activities, and began with terms (e.g. publication, new paper, child care and home school).", "Each tweet was coded as work- and family-related or not. A more extensive set of keywords classified the entire corpus.", "Most papers and articles are shared on Twitter via URL, tweet was classified as work-related, if shared, URL address indicates file type, publication venue or data repository services.", "Results: Faculty members of both genders were affected by the pandemic, the gap in work-related tweets between male and female academics roughly tripled following the work-from-home.", "Variation in effects between junior and senior faculty indicates this relationship is not driven by an intrinsic gender difference. This effect is produced by gendered differences in adapting a work/life balance to the pandemic.", "Female academics who reach full professor have overcome existing barriers to gender equality in academia.", "Parenting obligations overshadow all other factors in limiting research productivity, indicating the influence of parenting on productivity.", "Increased efforts to address these deep-rooted inequalities, the cracks in the pipeline continue to loom large.", "Gender imbalances are less pronounced among the ranks of junior faculty, efforts to explain biases in early career trajectories would have the greatest long-term influence on the pipeline of female academics."], "Quote": "\u201cWith gender imbalances less pronounced among the ranks of junior faculty, efforts to account for biases in early career trajectories would have the greatest long-term impact on the pipeline of female academics. Moreover, as female role-models can positively influence young women\u2019s propensities to enter male-dominated fields (Bonneau and Kanthak, 2018; Breda et al., 2020), administrators\u2019 success or failure here could have downstream impacts on female representation in the academy for the next generation.\u201d (p.15)", "Abstract": "Does the pandemic exacerbate gender inequality in academia? The temporal lag in publication pipeline complicates the effort to determine the extent to which women\u2019s productivity is disproportionately affected by the COVID-19 crisis. We provide real-time evidence by analyzing 1.8 million tweets from approximately 3,000 political scientists, leveraging their use of social media for career advancement. Using automated text analysis and difference-in-differences estimation, we find that while faculty members of both genders were affected by the pandemic, the gap in work-related tweets between male and female academics roughly tripled following work-from-home. We further argue that these effects are likely driven by the increased familial obligations placed on women, as demonstrated by the increase in family-related tweets and the more pronounced effects among junior academics. Our causal evidence on work-family trade-off provides an opportunity for proactive efforts to address gender disparities that may otherwise take years to manifest.", "Reference": "Kim, E., & Patterson, S. (2020). The Pandemic and Gender Inequality in Academia. Available at SSRN 3666587. http://dx.doi.org/10.2139/ssrn.3666587", "You_may_also_be_interested_in": [{"Relevant_ref": "Surviving (thriving) in academia: feminist support networks and women ECRs (Macoun & Miller, 2014)", "href": "#h.by7jhssej2ru"}, {"Relevant_ref": "Global gender disparities in science (Lariviere et al., 2013)", "href": "#h.36lhjcfi5x2s"}, {"Relevant_ref": "Gender in the editorial boards of scientific journals: A study on the current state of the art (Ghasemi et al., 2020) \u25c8", "href": "#h.oon3nmopqtb4"}, {"Relevant_ref": "Bias against research on gender bias (Cislak et al., 2018)", "href": "#h.t7jyy2cy17mq"}, {"Relevant_ref": "Something\u2019s Got to Give (Flaherty, 2020) \u25c8", "href": "#h.hhzwj9smvj5g"}]}, "summary_86": {"Title": "Gender in the editorial boards of scientific journals: A study on the current state of the art (Ghasemi et al., 2020) \u25c8\u00a0\u233a", "Id": "h.oon3nmopqtb4", "Main_Takeaways": ["There is a large number of studies on gender in academia, gender in membership of editorial boards of scientific journals garner attention of research and little literature. \u00a0They make the policies and determine what is accepted for publication and what is not.", "Admission or rejection of articles influences academic careers of authors: full professors or PhD students. Gender in editorial boards attracted attention from several researchers, albeit studies focus on journals of a specific field of knowledge.", "Works dealing with women and academia are addressed, those works focusing on editorial boards are reviewed. Male professors, male authors in journals and male dominance is higher than female counterparts.", "Women\u2019s receipt of professional awards and prizes and funding increased in the past two decades-men continue to win a higher proportion of awards and funding for scholarly research than expected based on the nomination pool.", "Stereotypes about women\u2019s abilities, harsh self-assessment of scientific ability by women than by men; academic and professional climates dissatisfying to women and unconscious bias contribute to achieving fewer awards and funds.", "Female board representations have improved over time, is consistent across countries, and gendered subdisciplines attract higher female board representations. \u00a0Inequities persist at the highest level: women are under-represented as editors and on boards of higher ranked journals. \u00a0Three factors for women under-representation in editorial board: discipline, journal's prestige and editor\u2019s gender.", "The last 15 years hinders women\u2019s ability to attain scholarly recognition and advancement and carries risk to the narrow nature and scope of research in the field. \u00a0They all show a worrying trend of under-representation of women and agree on negative consequences for advancement of science."], "Abstract": "Gender issues have been studied in a broad range of fields and in many areas of society, including social relations, politics, labour, and also academia. However, gender in the membership of editorial boards of scientific journals is a topic that only recently has started to attract the attention of researchers, and there is little literature on this subject as of today. The objective of this work is to present a study of the current state of editorial boards with regard to gender. The methodology is based on a literature review of gender issues in academia, and more specifically in the incipient field of gender in editorial boards. The main findings of this work, according to the reviewed bibliography, are that women are underrepresented in academic institutions, that this underrepresentation is increasingly marked in higher rank positions in academia and in editorial boards, and that this carries the risk of narrowing the nature and scope of the research in some fields of knowledge.", "Reference": "Ghasemi, N. M., Perramon Tornil, X., & Sim\u00f3 Guzm\u00e1n, P. (2019, March). Gender in the editorial boards of scientific journals: a study on the current state of the art. In Congr\u00e9s Dones Ci\u00e8ncia i Tecnologia 2019: Terrassa, 6 i 7 de mar\u00e7 de 2019. http://hdl.handle.net/2117/134267", "You_may_also_be_interested_in": [{"Relevant_ref": "Surviving (thriving) in academia: feminist support networks and women ECRs (Macoun & Miller, 2014)", "href": "#h.by7jhssej2ru"}, {"Relevant_ref": "Global gender disparities in science (Lariviere et al., 2013)", "href": "#h.36lhjcfi5x2s"}, {"Relevant_ref": "The Pandemic and Gender Inequality in Academia (Kim & Patterson, Jr, 2020)\u25c8", "href": "#h.psety12z4b7u"}, {"Relevant_ref": "Bias against research on gender bias (Cislak et al., 2018)", "href": "#h.t7jyy2cy17mq"}, {"Relevant_ref": "Something\u2019s Got to Give (Flaherty, 2020) \u25c8", "href": "#h.hhzwj9smvj5g"}]}, "summary_87": {"Title": "Something\u2019s Got to Give (Flaherty, 2020) \u25c8\u00a0\u233a", "Id": "h.hhzwj9smvj5g", "Main_Takeaways": ["Female academics\u2019 research productivity dropped at the COVID-19 outbreak-experts attribute to role being more caregiving before the pandemic and some blame it on emotional labour.", "Journal submission rates for women improved in some cases.", "Female authors were down for submissions and they were shifted to middle authors.", "Female first-author submissions to medRxiv dropped from 36% in December to 20% in April.", "Other researchers have found COVID-19 related papers in medicine and economics have fewer female authors than expected.", "Senior and author submissions by women decreased 6% over the same period, while male senior author submissions rose 5%.", "Men win the COVID-19 game, whereas women lose. Male authors outnumber female authors by more than three to one.", "Most manuscripts are single authored.", "Submissions were up since COVID-19, but the share of submissions submitted by women was down.", "Authors with heavier duties move to the front of the list when necessary. The pandemic publishing conversation is getting longer on data and about to get longer, it is short on solutions.", "We need to allow part-time work and different work shifts should be available to those who need them and agencies should extend grant end dates and allow for increased funding carryover from year to year.", "Kibbe recommends pausing tenure clock during the pandemic who say it is not enough and can hurt women and other minoritized faculty members in that it delays career progression and decreases lifetime earnings. We are now normalising the idea of a scholar-parent identity to some degree."], "Abstract": "Women's journal submission rates fell as their caring responsibilities jumped due to COVID-19. Without meaningful interventions, the trend is likely to continue.", "Reference": "Flaherty, C. (2020, August, 20). Something's Got to Give. Inside Higher Ed. Retrieved from https://www.insidehighered.com/", "You_may_also_be_interested_in": [{"Relevant_ref": "Surviving (thriving) in academia: feminist support networks and women ECRs (Macoun & Miller, 2014)", "href": "#h.by7jhssej2ru"}, {"Relevant_ref": "Global gender disparities in science (Lariviere et al., 2013)", "href": "#h.36lhjcfi5x2s"}, {"Relevant_ref": "The Pandemic and Gender Inequality in Academia (Kim & Patterson, Jr, 2020)\u25c8", "href": "#h.psety12z4b7u"}, {"Relevant_ref": "Bias against research on gender bias (Cislak et al., 2018)", "href": "#h.t7jyy2cy17mq"}, {"Relevant_ref": "Gender in the editorial boards of scientific journals: A study on the current state of the art (Ghasemi et al., 2020) \u25c8", "href": "#h.oon3nmopqtb4"}]}, "summary_88": {"Title": "Publication metrics and success on the academic job market (Van Dijk et al., 2014)", "Id": "h.ugiw8p46vedd", "Main_Takeaways": ["Number of applicants outnumber available academic faculty positions.", "There has been no quantitative analysis of who becomes a principal investigator. Success in academia is predictable and depends on the number of publications, impact factors and number of papers that receive more citations than average for the journal in which they were published.", "Method:We qualified more than 200 different metrics of publication output for authors who became Principal Investors and those who did not.", "Method:\u00a0Whether or not a scientist becomes a scientist depends on publication record considering first few years of publication and effect of each publication feature independent of other confounding variables.", "Results:\u00a0Authors with more first author publications, more papers in high impact journals are more likely to become principal investigators.", "Results: They have higher h-index, which predicts future scientific success.", "Results: Actual number of citations is less predictive of becoming a Principal Investigator than journal impact factor.", "Results: Many scientists who become Principal Investigators never published in high impact factor journals.", "Results: Authors with more first or second author publications are more likely to become Principal Investigators and more middle author publications are of no help unless helped in high impact journals.", "Results: Authors who are middle author on papers with many co-authors are less likely to become Principal Investigators. \u00a0First authors on papers with many co-authors are given less credit for these publications.", "Results: Authors who take longer than seven years to become a Principal Investigator have more citations per paper than authors who become Principal Investigators more quickly.", "Results: Scientists who publish important papers in low impact factors can become Principal Investigators but take longer.", "Results: Men are over-represented as Principal investigators after correcting for all other publication and non-publication derived features, being male is positively predictive of becoming a Principal Investigator.", "Quality of publication is given more weight than its actual quality. The number of citations a publication receives is correlated with the impact factor of the journal.", "In a linear model, we find that cites/impact factor is the fourth most predictive feature after impact factor, number of publications and gender. Hiring committees consider exceptional papers published in lower impact factor journals.", "These authors have a two-fold increase in their first-author publication rate relative to authors who do not become Principal Investigator, indicating that more first-author publications per year can compensate for lack of high impact factor publications.", "The Set of Principal investigator is enriched for scientists who attend higher ranked universities, linked to many other features. It predicts becoming Principal Investigator independent of other publication features.", "Scientists from higher ranked institutions become Principal Investigators before those from lower ranked institutions.", "H-index predicts principal investigator. \u00a0Better universities attract better people and produce more Principal investigators.", "University rank correlates with soft skills or names of highly ranked universities look good on applicants\u2019 CVs.", "There is a bias supporting from highly-ranked universities in men but cannot discriminate bias in hiring from a self-selective one in which men from high ranked universities prefer to become Principal Investigators."], "Abstract": "Research conducted by Drs David van Dijk, Ohad Manor and Lucas Carey on the variables that predict the likelihood of becoming a Principal investigator.", "Reference": "Van Dijk, D., Manor, O., & Carey, L. B. (2014). Publication metrics and success on the academic job market. Current Biology, 24(11), R516-R517. https://doi.org/10.1016/j.cub.2014.04.039", "You_may_also_be_interested_in": [{"Relevant_ref": "Six principles for assessing scientists for hiring, promotion, and tenure (Naudet et al, 2018)", "href": "#h.1j8n5dtwa2fz"}, {"Relevant_ref": "Faculty promotion must assess reproducibility (Flier, 2017) \u233a", "href": "#h.c3k3blbo0exl"}, {"Relevant_ref": "Rewarding Research Transparency (Gernsbacher, 2018)", "href": "#h.jg9mj2pk5e0m"}, {"Relevant_ref": "The Matthew effect in science funding (Bol et al., 2018)", "href": "#h.bbkbxhmwh8ms"}]}, "summary_89": {"Title": "Scientists\u2019 Reputations are Based on Getting it Right, not being Right (Ebersole et al., 2016)", "Id": "h.asb7cw2xwqfj", "Main_Takeaways": ["What happens if my finding does not replicate?", "Replications can succeed or fail due to consequences for findings being studied and methodology used to study it.", "Many would argue scientists should be evaluated only for things they control.", "Researchers control questions, hypotheses, design, implementation, analysis and report but do not control results.", "Results are determined by reality, but scientists produce ideas and insights allow discovery of results.", "Exciting, innovative results are better than boring, incremental results and reproducible results are better than uncertain and irreproducible results.", "Both innovative and certain is ideal. Which would you choose? Respondents evaluate reproducible and boring more favourable than exciting but uncertain or not reproducible results.", "Should we publish it and move on to chase next exciting finding or should we work to achieve greater certainty via replication and other strategies?", "Should we respond when others replicate findings increase certainty independently?", "Reputation measured by perceived ability and ethics- did not mirror assessments of truth of original results.", "Reputations more closely tied to process whether they respond to others\u2019 replications or pursue their own.", "Reputations increase if self-replication failure was reported or if other replication failure was pursued with follow-up research.", "Replications of results with large, representative samples will be useful to obtain precise, generalisable estimates.", "A similar difference between researchers and the general population was observed in the second survey.", "More researchers rated scientists as boring but certain was better and more ethical than one who produces exciting but uncertain findings. Former is keptand more celebrated by wide margins.", "Impact of quality of evidence in original and replication studies, pre-existing reputation and how reputational stakes and tradeoffs among scientific values differ across disciplines."], "Abstract": "Replication is vital for increasing precision and accuracy of scientific claims. However, when replications \u201csucceed\u201d or \u201cfail,\u201d they could have reputational consequences for the claim\u2019s originators. Surveys of United States adults (N = 4,786), undergraduates (N = 428), and researchers (N = 313) showed that reputational assessments of scientists were based more on how they pursue knowledge and respond to replication evidence, not whether the initial results were true. When comparing one scientist that produced boring but certain results with another that produced exciting but uncertain results, opinion favored the former despite researchers\u2019 belief in more rewards for the latter. Considering idealized views of scientific practices offers an opportunity to address incentives to reward both innovation and verification.", "Reference": "Ebersole, C. R., Axt, J. R., & Nosek, B. A. (2016). Scientists\u2019 reputations are based on getting it right, not being right. PLoS biology, 14(5), e1002460.", "You_may_also_be_interested_in": [{"Relevant_ref": "Publication Pressure and Scientific Misconduct in Medical Scientists (Tijdink et al., 2014)", "href": "#h.6mzquzro2mzm"}, {"Relevant_ref": "Prestige drives epistemic inequality in the diffusion of scientific ideas (Morgan et al., 2018)", "href": "#h.rzwvgpt4pcda"}, {"Relevant_ref": "Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018)", "href": "#h.syrrsgeyw2yr"}]}, "summary_90": {"Title": "Rewarding Research Transparency (Gernsbacher, 2018)", "Id": "h.jg9mj2pk5e0m", "Main_Takeaways": ["Reproducing results is an active ingredient of any science.", "Pre-register studies\u2019 goals and analysis plans make materials and data open to everyone and make reports available to everyone.", "Taking steps to research transparency takes time and steps are not rewarded.", "Reward research transparency when hiring, evaluating researchers for academic promotion and tenure and select researchers for society and national awards.", "We reward during one of the most incentivised phases of academic life: hiring.", "Departments argue that hiring announcements value transparent research practices and value job candidates who ascribe to transparent research practices.", "Reward research transparency for people who serve search committees to evaluate job candidates in commitment to research transparency.", "They can illustrate commitment to research transparency-describing commitment in their cover letters, create research transparency section in research statements, annotating vita to indicate which of their studies are based on pre-registration, open materials, open data and open-access research reports.", "Job candidates illustrate commitment to research transparency by asking letter writers to address research transparency activities in a letter of recommendation.", "If a department and candidates articulate commitment, the department evaluates them according to this commitment, and academic hiring incentivise research transparency.", "Material deserves not only attribution, proper citations, and acknowledgement during evaluation.", "Rewarding steps taken for greater research transparency should be based on value-based metric.", "Departments state their commitment to research transparency in job advertisements and promotion criteria, and candidates illustrate commitment in job applications and promotion dossiers.", "Changing scientific culture needs top-down leadership, together with bottom-up enthusiasm, institutional commitment, supporting departmental agreement, publication and funding gatekeepers in sync with publication and funding gate knockers and actions, together with words."], "Abstract": "Cognitive scientists are increasingly enthusiastic about research transparency. However, their enthusiasm could be tempered if the research reward system fails to acknowledge and compensate these efforts. This article suggests ways to reward greater research transparency during academic job searches, academic promotion and tenure evaluations, and society and national award selections.", "Reference": "Gernsbacher, M. A. (2018). Rewarding research transparency.\u00a0Trends in cognitive sciences, 22(11), 953-956. https://doi.org/10.1016/j.tics.2018.07.002", "You_may_also_be_interested_in": [{"Relevant_ref": "Six principles for assessing scientists for hiring, promotion, and tenure (Naudet et al, 2018)", "href": "#h.1j8n5dtwa2fz"}, {"Relevant_ref": "Publication metrics and success on the academic job market (Van Dijk et al., 2014)", "href": "#h.ugiw8p46vedd"}]}, "summary_91": {"Title": "Fast Lane to Slow Science (Frith, 2020)", "Id": "h.b5mhf64mvfpk", "Main_Takeaways": ["Fast Science is bad for scientists and bad for science.", "Slow science may help us to make faster progress, how can we slow down? People hardly have time to read the original studies. There is little chance to cultivate broader interests: impairing mental health and well-being of a researcher.", "We lost talented people resulting in decreased diversity.", "Fast science cuts corners and contributes to the reproducibility crisis.", "We could set up a working group-a small conference where practical ideas could be discussed.", "We must look differently at timescales and consider bigger aims of science. Researchers need to be reminded that we contribute to human effort that transcends an individual\u2019s lifetime.", "We work for the sake of truth and for the benefit of society, as they have reason to believe science continuously improves our models of the world.", "A farsighted vision is important to create and test big theories, irrespective of obstacles.", "How funders view lengths of grant proposals and intervals for evaluations.", "Early career researchers believe they need to amass publications and grants. Established researchers assume that grants need to be maintained for their teams and facilities.", "Researchers need to be encouraged and rewarded for long-term projects that depend on collaborations and may not have short-term pay-off.", "We must teach students about the history of science, its noble goals, how it moves forward through failure and success through collaboration and competition.", "Researchers should actively model thinking pauses.", "We need to inform researchers about regret and make them aware that in time they may feel similarly. Quality as opposed to quantity should be grounds for giving grants, for hiring people and promotion and awards.", "Quality feels too subjective and tainted by bias stems from being part or wishing to be part of high-status networks.", "How then do we assess quality, authors can be good judges of their work?", "Best papers have something new and fascinating to say in a well-argued theoretical framework and be concise and use simple languages.", "Collaborations are visible and replace the lone genius stereotype.", "New solutions to big problems can be found more readily when researchers of diverse skills and different viewpoints interact. This is not difficult, first we need to achieve common ground and language.", "Need for vigilance to measure reliability and discriminate fact from fake. Engaging with those who bring different perspectives and make us aware of flaws in our theories and experiments. Why not develop a system that allows listing in the manner of film credits?", "We need to restrict the number of grants anyone hold at any one time and limit the number of papers published per year.", "Funders, institutions and publishers regulate an initially voluntary triage to a pre-arranged number.", "New models of science communication overcome some problems of traditional journal articles and provide answers to tricky problems of credits.", "Doing less is better but we need to develop tools to measure quality. It would be exciting to set a goal and have content between those who continue in the fast lane and those who decide to switch lanes."], "Abstract": "Fast Science is bad for scientists and bad for science. Slow Science may actually help us to make faster progress, but how can we slow down? Here, I offer preliminary suggestions for how we can transition to a healthier and more sustainable research culture.", "Reference": "Frith, U. (2020). Fast lane to slow science. Trends in cognitive sciences, 24(1), 1-2. https://doi.org/10.1016/j.tics.2019.10.007", "You_may_also_be_interested_in": [{"Relevant_ref": "Let\u2019s Publish Fewer Papers (Nelson et al., 2012)", "href": "#h.4ldkk89lm24z"}]}, "summary_92": {"Title": "Lessons for psychology laboratories from industrial laboratories (Gomez et al., 2017)", "Id": "h.t8f8varqnpso", "Main_Takeaways": ["The proposal does not discuss outright fraud and reaches well-intentioned researchers to produce best possible scientific work.", "How can we increase the quality of the data in psychology and cognitive neuroscience laboratories?", "Behavioural and social scientists are not less ethical than scientists from other disciplines but noiseiness of data obtained from human behaviour contributes to these fields\u2019 problems.", "It is a research ethics imperative to reduce sources of noise in our data by implementing data quality systems.", "Academic laboratories do not have external controls and scientists rarely get trained in quality systems.", "Industrial laboratories have a very different culture as quality systems are widely used.", "High quality standards are an imperative for industrial activities, as there are external forces that cannot be ignored.", "Junior graduate students mess up times before they adopt their own quality habits and development of formal, explicit and enforceable quality policies would be beneficial for everyone involved, benefits would quickly outweigh costs of developing and enforcing these systems.", "Reduce waste of resources on failed studies, facilitate adoption of open science practices and improve signal to noise ratio in the data.", "Quality system needs of a group do survey-based studies might be different than needs of a group collecting neurophysiological data.", "Quality Assessment should be the responsibility of senior members of the team, as this process is strategic, pre-planned and has a long-term time frame.", "More stringent quality system would be to have an external group perform a quality verification audit.", "A laboratory could be audited by a buddy laboratory from either the same or different institution.", "Research could have verification badges the same way that some of open science initiatives provide forms of certification for different levels of openness."], "Abstract": "In the past decade there has been a lot of attention to the quality of the evidence in experimental psychology and in other social and medical sciences. Some have described the current climate as a \u2018crisis of confidence\u2019. We focus on a specific question: how can we increase the quality of the data in psychology and cognitive neuroscience laboratories. Again, the challenges of the field are related to many different issues, but we believe that increasing the quality of the data collection process and the quality of the data per se will be a significant step in the right direction. We suggest that the adoption of quality control systems which parallel the methods used in industrial laboratories might be a way to improve the quality of data. We recommend that administrators incentivize the use of quality systems in academic laboratories.", "Reference": "Gomez, P., Anderson, A. R., & Baciero, A. (2017). Lessons for psychology laboratories from industrial laboratories. Research Ethics, 13(3-4), 155-160. https://doi.org/10.1177/1747016117693827", "You_may_also_be_interested_in": [{"Relevant_ref": "Minimising Mistakes in Psychological Science (Rouder et al., 2018)", "href": "#h.p6f1r6qkslar"}]}, "summary_93": {"Title": "Let\u2019s Publish Fewer Papers (Nelson et al., 2012)", "Id": "h.4ldkk89lm24z", "Main_Takeaways": ["Authors file away less successful papers leading to publication bias but we need to focus on cluttered office effects.", "In an office full of papers, it is hard to tell good ones from bad ones. Not all researchers would receive equal consideration.", "The less established researcher is unlikely to be noticed and praised. Researchers seeking top jobs would better to comment on a paper by a famous and higher status researcher.", "Advancement depends on values of papers rescued from the file drawer.", "When every paper is available, it becomes difficult to find good papers and harder to find not done by a famous person, school or in a popular research area.", "For every good idea we have, we need to consider many bad ones.", "It is a good idea to drop bad ideas before they mature to bad papers. Bad papers are easy to write but difficult to publish.", "We make it easier to publish papers, we do not introduce good papers, we introduce more bad papers.", "Some published papers are false-positives. An occasional paper is bad but lots of false positives are catastrophic. False positives are hard to identify and correct and produce severe costs on the scientific community.", "Costs are felt more by field than an individual researcher.", "Researchers are heavily rewarded for having new and exciting ideas and only vaguely rewarded for being accurate.", "Researchers are trained to defeat the review process and conquer the publisher.", "Researchers are rewarded for quantity of papers and less for the truth value of our shared knowledge.", "Researcher who chooses among all effects they publish to focus on one they can obtain reliably.", "Rather than wondering how to evaluate two candidates who differ in quality and quantity, candidates would be matched on the latter, allowing them to focus on the former.", "Pursue their own work with improved clarity and focus, as there is only one paper to write this year."], "Abstract": "This commentary is written by Professor Jon Simons about the importance of publishing fewer papers.", "Reference": "Nelson, L. D., Simmons, J. P., & Simonsohn, U. (2012). Let's publish fewer papers. Psychological Inquiry, 23(3), 291-293.https://doi.org/10.1080/1047840X.2012.705245", "You_may_also_be_interested_in": [{"Relevant_ref": "Fast Lane to Slow Science (Frith, 2020)", "href": "#h.b5mhf64mvfpk"}]}, "summary_94": {"Title": "Informal Teaching Advice (Bloom, 2020)\u25c8", "Id": "h.kdc4l4p216u9", "Main_Takeaways": ["You need to be enthusiastic and act like there\u2019s no place in the world you rather be.", "Enjoy the material more, make your audience perk up, like you more and learn more.", "Be confident-and act as if you have done this a 100 times before and its always gone smashingly.", "3. Max it up- do not do the same thing over and over again, throw in some variety-movies, demos, etc. Variety cures boredom.", "4. Bring in other people, Guest lectures, interviews-easy to do with Zoom.", "5. Be modest in goals for each class. Most common mistake of beginning teachers is cramming too much material in any single session.", "6. Be yourself. Everyone has strength and teaches in a way that aligns with one\u2019s strength.", "7. Teaching prep can leech away all the time. Don\u2019t let it. Opportunity costs and repeat as needed.", "8. Provide a well-timed \u201cGreat question. I don\u2019t know but I\u2019ll find out for next class\u201d is charming and makes everyone feel good.", "9. Use specific students as examples in arbitrary ways.", "10. When I was in the second grade, I asked a stupid question and the Teacher said it was stupid. Always say how interesting at a minimum level, no matter how off topic.", "11. Use concrete examples whenever possible, often from your own life. They do not necessarily have to be true.", "12. Many good teachers self-medicate before class, especially if they suffer from anxiety."], "Abstract": "This commentary is written by Professor Paul Bloom about how to make your teaching more engaging with your students.", "Reference": "Bloom, P. (2020). Informal Teaching advice. https://www.dropbox.com/s/glm1agnxtz5tbww/informal-teaching-advice.pdf?dl=0", "You_may_also_be_interested_in": [{"Relevant_ref": "Relevant references will be added soon"}]}, "summary_95": {"Title": "The Matthew effect in science funding (Bol et al., 2018)", "Id": "h.bbkbxhmwh8ms", "Main_Takeaways": ["Why is academic success so unequally distributed across scientists?", "If only one of two talented young scholars is given an award, an award winning scholar will have a more successful career and receive benefits.", "Luck also plays a role.", "Matthew effect undermines meritocracy by allowing an initially fortunate scientist to self-perpetuate, whereas an equally talented but less fortunate counterpart remains underappreciated.", "The present study uses causal inference problems using a regression-discontinuity approach and study Matthew effect in science funding.", "Next to identify a participation mechanism driving Matthew effect-early stage failure inhibits participation in further competition through discouragement and lack of resources.", "Matthew effect may be dominant in accumulation of individual research funding. \u00a0Past successes aid quality assessment under uncertainty and allocate research funds.", "Method: A critical test of Matthew effect in early career academic funding is being given a single granting program as a primary funding source for young Dutch scientists.", "Method:\u00a02000 concentrated in hands of a minority of 3660 applicants with proposals seen with superior promise.", "Method:\u00a0Effects of recent PhDs winning an early career grant and compare funding of nonwinners with evaluation scores below thresholds to winners with scores above it.", "Method:\u00a0Data on applicants in midcareer competition allow us to assess whether improved rates of success among winners of earlier grants result from increased participation in these grants.", "Results: An early career award are 2.5 more likely to win a midcareer award than those who did not.", "Results:\u00a0Midcareer award chances do not differ from early career competition. This does not reflect superior proposal quality or scientific ability.", "Results:\u00a0Improved success is due to recognition for winning early career grants.", "Results:\u00a0Early career grant allows winners to write a higher quality proposal for midcareer competition.", "Results:\u00a0Later-stage funding differences between winner and non-winners.", "Results:\u00a0The funding gap in the present study explains 40% of differences in earning between best and worst applicant.", "Results: Being awarded an early career grant raises long-term prospects of a full professorship by 47%.", "Dutch academic funding system allows demand side dynamics being widespread in grant competition, indicating a positive feedback effect is likely to extend to other contexts.", "Prior academic success merit review criterion.", "Investigators are unlimited in the number of grants pursued and information about past grants is available for consideration by reviewers, panelists and program directors.", "Earlier grants try luck in later competition in greater numbers than non-winners, indicating funding agencies could consider outreach to reduce this gap.", "Agencies could provide detailed information on how close evaluation scores were to funding thresholds which may prevent near-winners with good past proposals from concluding future odds are too low for investing time and effort in a new application.", "Positive feedback may be crucial through which money is concentrated in the hands of a few extremely successful scholars but origins of emergent distinction in scientists\u2019 careers may be of an arbitrary nature."], "Abstract": "A classic thesis is that scientific achievement exhibits a \u201cMatthew effect\u201d: Scientists who have previously been successful are more likely to succeed again, producing increasing distinction. We investigate to what extent the Matthew effect drives the allocation of research funds. To this end, we assembled a dataset containing all review scores and funding decisions of grant proposals submitted by recent PhDs in a V2 billion granting program. Analyses of review scores reveal that early funding success introduces a growing rift, with winners just above the funding threshold accumulating more than twice as much research funding (\u20ac180,000) during the following eight years as nonwinners just below it. We find no evidence that winners\u2019 improved funding chances in subsequent competitions are due to achievements enabled by the preceding grant, which suggests that early funding itself is an asset for acquiring later funding. Surprisingly, however, the emergent funding gap is partly created by applicants, who, after failing to win one grant, apply for another grant less often.", "Reference": "Bol, T., de Vaan, M., & van de Rijt, A. (2018). The Matthew effect in science funding. Proceedings of the National Academy of Sciences, 115(19), 4887-4890. https://doi.org/10.1073/pnas.1719557115", "You_may_also_be_interested_in": [{"Relevant_ref": "Early co-authorship with top scientist predicts success in academic careers (Li et al., 2019)", "href": "#h.rvz88tdscswt"}, {"Relevant_ref": "Prestige drives epistemic inequality in the diffusion of scientific ideas (Morgan et al., 2018)", "href": "#h.rzwvgpt4pcda"}, {"Relevant_ref": "Open Science Isn\u2019t Always Open to All Scientists (Bahlai et al., 2019)", "href": "#h.b6say8wj1wto"}, {"Relevant_ref": "A user\u2019s guide to inflated and manipulated impact factor (Ioannidis & Thombs, 2019)", "href": "#h.w1fgusfwh4me"}, {"Relevant_ref": "Publication metrics and success on the academic job market (Van Dijk et al., 2014)", "href": "#h.ugiw8p46vedd"}]}, "summary_96": {"Title": "Minimising Mistakes in Psychological Science (Rouder et al., 2018)", "Id": "h.p6f1r6qkslar", "Main_Takeaways": ["The easiest mistake to detect is a malformed statement of statistical test-combine test statistics and degrees of freedom do not match to p-value.", "If simple errors occur randomly, they may go against researchers\u2019 preferred directions.", "A starting point is to consider practices in high risk fields where mistakes can have debilitating consequences.", "Principle 1: Sensitivity to Operations: Those of us in psychology focus on what of this business? What are our experiments? What is the data? What are the theories? What do the data allow us to infer about theories?", "How do we ensure experiments are randomised? How do we document who ran what where? How do we ensure the integrity of knowledge produced? Preoccupation With Failure is that they not only scrutinise operations and points of failure.", "Resilience refers to maturity about failures-minimised and occur from time to time.", "Organisation has processes to learn from failures so they will not be repeated.", "Reluctance to simplify means to diagnose causes of failures.", "Deference to expertise address hierarchies in organisations.", "Administrators may be higher in organisational structures. Defer to people who execute these operations on a daily basis such as people who record and log all mistakes.", "People should use a relational database, prewritten little scripts to insert metadata into the database.", "Different labs adopt different solutions in search of radical automation. They choose to do on their own will find much help on the web for learning database management, shell scripting, and Software Carpentry.", "Retain multiple versions of work products.", "Lab should have a common approach to versioning. Appending dates or version numbers and people make mistakes. File-name approach defeats versioning on most cloud storage focusing on automatic versioning.", "Researchers who use Excel cannot recreate a graph, data analysis should be coded and use menu-driven systems.", "There are choices that need to be made while navigating menus. Accurately report results of analyses.", "Include a healthy trail indicating what code produced analysis, what version of code, what version of data, when analysis was conducted and by whom. Rmarkdown-write APA-compliant manuscripts using papaja and apa_print() and eliminates transcription errors in typesetting tables."], "Abstract": "Developing and implementing best practices in organizing a lab is challenging, especially in the face of new cultural norms such as the open-science movement. Part of this challenge in today\u2019s landscape is using new technologies such as cloud storage and computer automation. Here we discuss a few practices designed to increase the reliability of scientific labs by focusing on what technologies and elements minimize common, ordinary mistakes. We borrow principles from the Theory of High-Reliability Organizations which has been used to characterize operational practices in high-risk environments such as aviation and healthcare. From these principles, we focus on five elements: 1. implementing a lab culture focused on learning from mistakes; 2. using computer automation in data and meta-data collection wherever possible; 3. standardizing organization strategies; 4. using coded rather than menu-driven analyses; 5. developing expanded documents that record how analyses were performed.", "Reference": "Rouder, J. N., Haaf, J. M., & Snyder, H. K. (2019). Minimizing mistakes in psychological science. Advances in Methods and Practices in Psychological Science, 2(1), 3-11. https://doi.org/10.1177/2515245918801915", "You_may_also_be_interested_in": [{"Relevant_ref": "Lessons for psychology laboratories from industrial laboratories (Gomez et al., 2017)", "href": "#h.t8f8varqnpso"}]}, "summary_97": {"Title": "Open Science at Liberal Arts Colleges (Lane et al., 2020)\u25c8", "Id": "h.t05ib5vtdmo5", "Main_Takeaways": ["Small liberal arts colleges focus on excellent undergraduate education and adopt open science practices slower than research institutes.", "Times have changed with a commitment to understand and support one another to make important adjustments.", "Faculty who engage in open science practices may be hesitant to engage undergraduates in discussions regarding replicability crisis.", "Faculty lack time to add to their course or create a course addresses open science movement and practices.", "Faculty may fear students will lose trust in the field, or worry that they are not knowledgeable or qualified enough about open science to teach it.", "Small liberal arts colleges have broad interests and make their decision not to further their education to value critical thinking and well-rounded education.", "Open science should be included in all classes that focus on replicability and reproducibility in terms of specific findings and open science more generally.", "Data collection and analysis is important for statistics and advanced methods.", "Through pre-registration, students detail research questions, hypotheses, methods and data analytic procedures prior to onset of data collection.", "Not all psychology students will do research but should study as part of liberal arts experience or general education.", "Open science allows transferable skills to be taught (e.g. framing questions, thinking critically, working collaboratively, grappling with data and communicating clearly).", "Scientists prior to the open science movement favoured exciting findings. We need an explicit and transparent account of how discoveries were made. We need to focus on process rather than outcome. Pre-registration takes time, it not only represents good science but provides concrete mechanisms for teaching best practices.", "Pre-registration can be used as a pedagogical tool for undergraduate students and serves as a checkpoint during the research process letting the faculty supervisor evaluate if students understand their projects to commence data collection.", "Many faculty use small sample sizes and urge researchers to make decisions about underpowered studies and p-hacking.", "We should use larger sample size and design better studies. A priori power analyses provide evidence-based guidance for sample size.", "We should also reduce the influence of researcher degrees of freedom to produce false positives.", "Conducting larger studies might seem daunting. Most liberal arts colleges have limited participant pools and researchers quickly deplete available participants and fund to increase sample size.", "Open science levels the playing field for well-conceived studies and slower place and hands-on role of faculty at small liberal arts colleges lent themselves to home well-designed studies.", "Researchers should not compete in terms of quantity and focus on higher quality studies.", "Prior to open science practices, data was examined and decided whether to continue or terminate data collection, inflating false positives.", "Use sequential analysis to conduct analyses at predetermined intervals throughout the data collection process, to balance use of available participants with false positive.", "Data sharing and material sharing increases credibility and trustworthiness of a research project.", "Open science framework enables materials to be uploaded and time-stamped but embargoed until a later point in time.", "Material and data sharing is useful to increase efficiencies and effectiveness in working with other researchers.", "Current studies learn best practices in open science and will be able to learn from what came before, saving research mentor time and energy expended to track down questionnaires, datasets and analyses.", "Open science framework provides unconstrained free storage.", "Senior members of a department support open science by educating colleagues about the value of open science, contextualising quantity and timing of work in internal letters of support for junior colleagues.", "Write external letters for a colleague promotion and senior colleagues make sure to list reviewers who understand and guide the committee away from reviewers who are hostile towards open science.", "Senior scholars support open science by positively commenting on the value of open science practices and highlight aspects of the candidate's work when evaluating colleagues at other institutions.", "Inclusion is a primary focus on the Society of Improving Psychological Science, it is to make sure non-PhD granting institutions have a strong voice in its governance.", "Most projects stem from work at Society of Improving Psychological Science are reviewed with regard to diversity, including type and size of institution."], "Abstract": "Adopting and sustaining open science practices is accompanied by particular opportunities and challenges for faculty at small liberal arts colleges (SLACs). Their predominantly undergraduate student body, small size, limited resources, substantial teaching responsibilities, and focus on intensive faculty-student interactions make it difficult to normalize open science at SLACs. However, given the unique synergy between teaching and research at SLACs, many of these practices are well-suited for work with undergraduate psychology students. In addition, the opportunities for collaboration afforded by the open science community may be especially attractive for those doing research at SLACs. In this paper, we offer suggestions for how open science can further grow and flourish among faculty who work closely with undergraduates, both in classrooms and in labs. We also discuss how to encourage professional development and transform institutional culture around open science practices. Most importantly, this paper serves as an invitation to SLAC psychology faculty to participate in the open science community.", "Reference": "Lane, K. A., Le, B., Woodzicka, J. A., Detweiler-Bedell, J., & Detweiler-Bedell, B. (2020, August 23). Open Science at Liberal Arts Colleges. https://doi.org/10.31234/osf.io/437c8", "You_may_also_be_interested_in": [{"Relevant_ref": "Relevant references will be added soon"}]}, "summary_98": {"Title": "How to prove that your therapy is effective, even when it is not: a guideline (Cuijpers & Cristea, 2016)", "Id": "h.tx1eb4t5pef8", "Main_Takeaways": ["Treatment guidelines use randomised trials to advise professionals use specific interventions and not others, policy makers and health insurance companies use evidence to indicate whether or not a specific intervention should be adopted and implemented.", "How can you make sure the randomised trial results in positive outcomes that a therapy is effect?", "In fact, if you attained one important method to optimise the chance, results of trial are favourable. The placebo effect may lead to an expectation that a therapy works.", "Advertise your trial in the medial as innovative and unique and the best among the available interventions.", "Learn when you want to optimise effects found for therapy is randomised trials have weak spots-risk of bias.", "Consider randomisation of participants-randomisation contributes to the trial-if participants are not randomised in groups, effects could be due to baseline differences between groups, not the intervention.", "Two factors of randomisation: random numbers should be generated-use coin toss for instance or allocation concealment-researchers conduct trial or assistant to assign participants to respond well to intervention to intervention group instead of to control group.", "Use non-blinded raters of clinical assessment of outcomes to influence outcomes of trial.", "Participants get therapy scores better because they received therapy. Inform raters of assigned conditions or don\u2019t say anything and hope an interaction between rater and participant becomes clear whether the latter was in intervention condition or not.", "Also, there is the issue of attrition for individuals who do not respond to intervention or experience side effects. It does not help them, harm them, so why continue?", "Ignore attrition in analyses of outcomes and look exclusively at completers, participants who continued analyse them.", "Therapy had better outcomes than included individuals who dropped out.", "The correct alternative is to include all participants who are randomised in the final analyses. Another risk of bias is outcome measures.", "Include multiple outcomes and analyse results so you can look at which outcome has the best result.", "Present outcomes in report and not mention other measures.", "Published articles can fail to mention trial registration number, not prompting readers to dig up available protocol and check for selective outcome reporting.", "Do not compare the new therapy for the same problem.", "You can say during presentations- the therapy works better than the existing one and user reports are positive but you do not examine that in your trial.", "To show your therapy is better than existing interventions, design a trial with a very large group of participants, you cannot expect your therapy is better than existing therapy.", "Use the right publication strategy even when you applied all techniques, your trials still show no effects of therapy.", "Do not publish these findings and wait until a new trial is done and observe positive findings.", "If this is unethical towards participants and funder, just tell yourself, others do it too.", "If you manage to convince other clinicians that this is a good and innovative therapy, some of them will do their own trials.", "Many findings were not found to be true when other researchers tried to replicate findings.", "Wait until a trial is conducted and published that observes positive outcomes. You can claim the therapy is effective and evidence-based.", "The goal is to show the therapy works, not what it does and convince others it is effective."], "Abstract": "Suppose you are the developer of a new therapy for a mental health problem or you have several years of experience working with such a therapy, and you would like to prove that it is effective. Randomised trials have become the gold standard to prove that interventions are effective, and they are used by treatment guidelines and policy makers to decide whether or not to adopt, implement or fund a therapy. You would want to do such a randomised trial to get your therapy disseminated, but in reality your clinical experience already showed you that the therapy works. How could you do a trial in order to optimise the chance of finding a positive effect? Methods that can help include a strong allegiance towards the therapy, anything that increases expectations and hope in participants, making use of the weak spots of randomised trials (risk of bias), small sample sizes and waiting list control groups (but not comparisons with existing interventions). And if all that fails one can always not publish the outcomes and wait for positive trials. Several methods are available to help you show that your therapy is effective, even when it is not.", "Reference": "Cuijpers, P., & Cristea, I. A. (2016). How to prove that your therapy is effective, even when it is not: a guideline. Epidemiology and Psychiatric Sciences, 25(5), 428-435. https://doi.org/10.1017/S2045796015000864", "You_may_also_be_interested_in": [{"Relevant_ref": "Relevant references will be added soon"}]}, "summary_99": {"Title": "Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019)", "Id": "h.jwujkwh9t0yi", "Main_Takeaways": ["It is easy to overlook the findings that may depend on chosen analytic strategy is imbued theory, assumptions and choice points. What if scientific results are highly dependent on subjective decisions at the analysis stage?", "This article addressed current lack of knowledge about how much diversity in analytic choice there can be when different researchers analyse the same data and whether diversity leads to different conclusions.", "The present study report of analytical decisions on research findings obtained by 29 teams analysed the same dataset to answer the same research question. Would you treat each red-card decision as an independent observation?", "How would you address the possibility that some referees give more red cards than others? Would you control for seniority?", "Would you consider whether a referee\u2019s familiarity with a player influences the referee\u2019s probability with a player affecting the referee\u2019s likelihood of assigning a red card?", "Would you look at whether players are more likely to receive red cards relative to players in other leagues and whether the proportion of players with dark skin differ across leagues and player positions? Many analytic decisions are needed.", "The project provides a data set used for this project obtained, documented and prepared for dissemination to participate analysts (Stage 1). Analysts were recruited to participate in the project (stage 2).", "The first round of data analysis (Stage 3) was followed by round-robin peer evaluations of each analysis (Stage 4). Second round of data analysis (Stage 5) followed by an initial discussion of findings and debate led to further analyses (Stage 6a).", "Tried to decide on a common conclusion while writing, editing and reviewing the manuscript (Stage 6b). Further questions emerged an internal peer review was started. In this review, each team\u2019s approach was measured by other analysts who were experts in the technique (Stage 7).", "Method: Scientists had to build a dataset, recruitment and initial survey of data analysts-77 researchers expressed initial interest in participating and were given access to an open science framework project to obtain the data.", "Method: 33 submitted a report in the first round (Stage 3) and 29 teams submitted a final report. Stage 3: first round of data analysis- after registering and answering subjective-beliefs surveys for the first time, research teams were given access to the data.", "Method: Each team decided on its own analytic approach primary research question and analysed the data independently of the other teams.", "Method: Teams submitted to the coordinators structured summaries of their analytic approach, including information about data transformations, exclusions, covariates, statistical techniques used, software used and results.", "Method: Stage 4: round-robin peer evaluations of overall analysis quality: teams were predicted to work independently of each other. They were encouraged to discuss and debate their respective approaches to the dataset.", "Method: Structured summaries collated into a single questionnaire and distributed to all teams for peer review. Each team had the opportunity to learn from others\u2019 analytic approaches and from qualitative and quantitative feedback provided by peer reviewers but did not have access to other teams\u2019 estimated effect sizes.", "Method: Stage 5: second round of data analysis: teams had opportunity to change analytic strategies and draw new conclusions. Teams not forced to present single effect sizes without robustness checks.", "Method: Stages 6: open discussion, debate, further analyses and draft report on project.", "Method: Stage 7: more granular peer assessments of analysis quality-identify potential flaws explain variability in results.", "Results: Analytic approaches differed widely across team-effect size range from 0.89 to 2.93.", "Results: Twenty teams had positive effects and 9 teams did not show relationships. 29 different analyses used 21 unique combinations of covariates.", "Results: Neither analysts prior beliefs about effect of interest nor level of expertise explained difference in outcome of analyses.", "Results: Peer ratings of quality of analyses did not explain variability.", "Results: Differences in results of analyses of complex data may be difficult to avoid, even by experts with honest intentions.", "Difference in measures, samples and random errors produce differences in findings not explained by differences in expertise.", "Analysts with high and lower levels of expertise showed high levels of variability in effect sizes.", "High favourable evaluations from peers showed same variability in final effect sizes as analytic approaches less favourably rated.", "Few editors accept a manuscript if researchers admitted if they chose to reach p <.05 criterion.", "Pre-registration solves the problems of forking paths and p-hacking removing the flexibility of data-contingent analyses and reducing opportunity to present post-hoc tests as a priori. Pre-registration does not prevent differences in effect size estimates across teams in this study.", "When there is less variability in analytic approach, there is more faith.", "In such extreme cases of little to no convergence in findings, crowdsourcing processes indicate the scientific community should have no faith that the hypothesis is true, even if one or two teams find significant support with a defensible analysis."], "Abstract": "Twenty-nine teams involving 61 analysts used the same data set to address the same research question: whether soccer referees are more likely to give red cards to dark-skin-toned players than to light-skin-toned players. Analytic approaches varied widely across the teams, and the estimated effect sizes ranged from 0.89 to 2.93 (Mdn = 1.31) in odds-ratio units. Twenty teams (69%) found a statistically significant positive effect, and 9 teams (31%) did not observe a significant relationship. Overall, the 29 different analyses used 21 unique combinations of covariates. Neither analysts\u2019 prior beliefs about the effect of interest nor their level of expertise readily explained the variation in the outcomes of the analyses. Peer ratings of the quality of the analyses also did not account for the variability. These findings suggest that significant variation in the results of analyses of complex data may be difficult to avoid, even by experts with honest intentions. Crowdsourcing data analysis, a strategy in which numerous research teams are recruited to simultaneously investigate the same research question, makes transparent how defensible, yet subjective, analytic choices influence research results.", "Reference": "Silberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F., Awtrey, E., ... & Carlsson, R. (2018). Many analysts, one data set: Making transparent how variations in analytic choices affect results. Advances in Methods and Practices in Psychological Science, 1(3), 337-356.https://doi.org/10.1177/2515245917747646", "You_may_also_be_interested_in": [{"Relevant_ref": "How scientists can stop fooling themselves (Bishop, 2020b)", "href": "#h.vmo2rkmiz4ms"}, {"Relevant_ref": "The Statistical Crisis in Science (Gelman & Loken, 2014)", "href": "#h.rc4vbzxkf0ax"}, {"Relevant_ref": "Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014)", "href": "#h.6m58pstvoxmx"}, {"Relevant_ref": "A 21 Word Solution (Simmons et al., 2012)", "href": "#h.hxo3saai5nvg"}, {"Relevant_ref": "Rein in the four horsemen of irreproducibility (Bishop, 2019)", "href": "#h.rrhza31fwxpw"}, {"Relevant_ref": "Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020)", "href": "#h.l5025sphi7dt"}, {"Relevant_ref": "The life of p: \u201cJust significant\u201d results are on the rise (Leggett et al., 2013)", "href": "#h.b910lefd5hzk"}, {"Relevant_ref": "Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014)", "href": "#h.6m58pstvoxmx"}, {"Relevant_ref": "False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011)", "href": "#h.sxh34fk82cfh"}, {"Relevant_ref": "Seven Easy Steps to Open Science: An Annotated Reading List (Cr\u00fcwell et al., 2019)", "href": "#h.5slx325ilt8s"}]}, "summary_100": {"Title": "ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008) \u25c8\u00a0\u233a", "Id": "h.fxf9ychze3l3", "Main_Takeaways": ["Poverty rates have been increasing together with debate on socio-economic status. Parental income is an indicator of socio-economic status reflecting potential for social and economic resources.", "Parental education is a component of socio-economic status.", "Learning in a meaningful context so at-risk students can immediately apply when they have learned and connect it to their own lives and individual experiences.", "Many dropouts are not only from low SES backgrounds but also mismatched learning styles.", "SES affects children\u2019s academic achievement. It is beneficial to determine the type of home environment, how educators will best support them at school.", "Learning environment must be structured to achieve the highest level of internal motivation from all students.", "School success is greatly determined by family socio-economic status. American society may be failing to provide educational opportunities for every student and citizen irrespective of socio-economic background.", "Many poor students come to school without social and economic benefits available to most middle and high SES students. Sufficient resources for optimal academic achievement irrespective of socio-economic status.", "The educational system produces an intergenerational cycle of school failures and short change an entire future American society as a result of family socio-economic status.", "Method: 31 surveys were handed out and 13 were returned. \u00a0Some of the answers include health/nutrition; level of IQ; motivation or lack of motivation of teacher; amount of parental support; class size; quality of instruction/teaching resources; support available in home; school; student disabilities; language; education in culture; style of learning exposure to style; gender; peer influence; natural ability; attendance; family loss of tragic event; pregnancy full term; expectations and teacher/student relationship were also considered.", "Method: Every teacher felt that the environment contributed most when considering academic achievement.", "Method: Additional variables for socio-economic status were included: attitude; self-confidence; need to please; desire to do better; love of learning; acceptance; economics in the home; stability of family; siblings; age of parent(s); age of student maturity; family involvement; importance placed on learning; cognitive level; family history; neighbourhood; modelling of good work; ethics; pride; choices made; resources available; parental achievement; attending pre-k; home literacy; received early intervention; good nutrition; health; high IQ; oral language development; self-care skills; family life; class dynamics; personality and mood on any given day tells a specific teacher what they can or cannot do on a given day.", "Results: The higher the socio-economic status, the higher the academic achievement.", "The current literature is not available as specific students in low socio-economic status homes have high academic achievement.", "Income, education and occupation are responsible for low academic achievement in many low SES families.", "Socio-economic status causes less time with children and a result of lower education level of a parent, students from families of higher economic status tend to have parents who read to and with them, parents more apt to talk to them about the world and offer them more cultural experiences, manty of students struggle with reading come from low SES and parents struggle with reading.", "If a family does not have a good educational background or materials to use to work with their child, the child may suffer as a result of their environment.", "If education is not valued in the home, students will not value education, more expectation for higher education in higher classes."], "Abstract": "In this literature review, family environments of low socioeconomic status (SES) students were examined and a comparison made in learning styles between low and high achievers Socioeconomic factors such as family income, education, and occupation play major role in the academic achievement of all students. There is a positive correlation between SES and academic achievement. The conclusions of this review have implications for all educators as well as the entire future of American society.", "Reference": "Quagliata, T. (2008). Is there a positive correlation between socioeconomic status and academic achievement?.\u00a0Paper: Education masters (p. 78). https://fisherpub.sjfc.edu/cgi/viewcontent.cgi?article=1077&context=education_ETD_masters", "You_may_also_be_interested_in": [{"Relevant_ref": "Education and Socio-economic status (APA, 2017b)", "href": "#h.d8en3o2spkj7"}, {"Relevant_ref": "Ethnic and Racial minorities and socio-economic status (APA, 2017)", "href": "#h.pwgw2ot3vok"}, {"Relevant_ref": "Women and Socio-economic status (APA, 2010)", "href": "#h.5cve5vi76ant"}, {"Relevant_ref": "Disability and Socio-economic status (APA, 2010)", "href": "#h.fwgotxock7y7"}, {"Relevant_ref": "Lesbian, Gay, Bisexual and Transgender Persons & Socioeconomic Status (APA, 2010)", "href": "#h.75aqilh5muke"}]}, "summary_101": {"Title": "Education and Socio-economic status (APA, 2017b) \u25c8\u233a", "Id": "h.d8en3o2spkj7", "Main_Takeaways": ["Children from low socio-economic status take longer to develop academic skills than children from higher socio-economic status groups. They have poor cognitive development. Inadequate education and increased attrition influences academic achievement.", "Improving school systems and early intervention programs reduce some risk factors.", "Increased research on correlation between socio-economic status and education is important. Children of low socio-economic status are less likely to have experiences for development of reading acquisition.", "Reading competence is linked to a home literacy environment. Poor households have less access to learning materials and experiences.", "Students from low SES-background are less likely to have access to informational resources about college. \u00a0Young adults with low socio-economic status have higher risk of student loans.", "Classroom environments contribute more to SES differences in learning rates than family characteristics.", "Students who were randomly assigned to higher quality classrooms in grades earned more, more likely to attend college, saved more for retirement and lived in a better neighbourhood.", "Children in low-income schools are less likely to have well-qualified teachers.", "A focus on improving teaching and learning, creation of an information-rich environment, building of a learning community, continuous professional development, involvement of parents, and increased funding and resources.", "Schools with students in poverty have fewer library resources to be drawn on than middle-income children. Children from low SES enter high school with literacy skills 5 years behind high-income students.", "High school dropout rate was highest in low-income compared to high-income families.", "Low income students are likely to succeed in STEM disciplines than less privileged backgrounds.", "Individuals in top family quartiles are 8 times more likely to obtain a bachelor\u2019s degree by age 24 relative to lowest family income quartile.", "Lower SES and learning difficulties or other negative psychological outcomes influence academic achievement.", "Toxic stress in early childhood leads to lasting impact on learning, behaviour and health.", "Children from lower SES households are twice as likely as those from high SES households to show learning related behaviour problems.", "Perception of family economic stress and personal financial constraints influenced emotional distress/depression in students and academic outcomes.", "Racial, ethnic and socio-economic barriers hinder individuals\u2019 vocational development.", "Career barriers higher for poor backgrounds, people of colour, women, those who are disabled, and LGBTIQ-identified individuals.", "Individuals from a lower social class had less career-related self-efficacy when it came to vocational aspirations.", "Higher social class backgrounds were more successful in developing career aspirations and better prepared for the world of work.", "Higher social class backgrounds are more successful to develop career aspirations and better prepared for the world of work due to access to resources such as career offices and familial experience with higher education."], "Abstract": "This fact sheet explains the impact socioeconomic status has on educational outcomes.", "Reference": "APA (2017, July). Education and Socioeconomic Status [Blog post]. Retrieved from https://www.apa.org/pi/ses/resources/publications/education", "You_may_also_be_interested_in": [{"Relevant_ref": "ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008)", "href": "#h.fxf9ychze3l3"}, {"Relevant_ref": "Ethnic and Racial minorities and socio-economic status (APA, 2017)", "href": "#h.pwgw2ot3vok"}, {"Relevant_ref": "Women and Socio-economic status (APA, 2010)", "href": "#h.5cve5vi76ant"}, {"Relevant_ref": "Disability and Socio-economic status (APA, 2010)", "href": "#h.fwgotxock7y7"}, {"Relevant_ref": "Lesbian, Gay, Bisexual and Transgender Persons & Socioeconomic Status (APA, 2010)", "href": "#h.75aqilh5muke"}]}, "summary_102": {"Title": "Ethnic and Racial minorities and socio-economic status (APA, 2017) \u25c8\u00a0\u233a", "Id": "h.pwgw2ot3vok", "Main_Takeaways": ["The relationship between SES, race and ethnicity is intimately intertwined. \u00a0Communities are segregated by SES, race and ethnicity. Low economic development, poor health conditions and low levels of educational attainment are shared in communities.", "Discrimination hinders social mobility to ethnic and racial minorities. In the US, 39% of African American children and adolescents, and 33% of Latino children and adolescents are living in poverty, which is more than double than the 14% poverty rate for non-Latino, White and Asian children and adolescents.", "Minority racial groups are more likely to experience multidimensional poverty than their White counterparts. American Indian/Alaska Native, Hispanic, Pacific Islander, and Native Hawaiian families are more likely than Caucasian and Asian families to live in poverty.", "Income of Asian American families have four to five family members working and performing above other minorities, African Americans (53%) and Latinos (43%) are more likely to receive high-cost mortgages than 18% Caucasians.", "African American unemployment rates are double of Caucasian Americans. African American men working full time earn only 72% of average earnings of comparable Caucasian men and 85% of earnings of Caucasian women.", "African Americans and Latinos are more likely to attend high-poverty schools than Asian Americans and Caucasians. From 2000 to 2013-dropout rates between racial groups narrowed significantly. \u00a0High school dropouts were highest for Latinos, followed by African Americans and Whites.", "High achieving African American students may be exposed to less rigorous curriculums, attend schools with fewer resources, and have teachers who expect less of them academically than similarly situated Caucasian students.", "12.% of African American college graduates were unemployed, which is more than double the rate of unemployment among all college graduates in the same age range. Racial and ethnic minorities have worse health than that of White Americans.", "Health disparities stem from economic determinants, education, geography, neighbourhood, environment, lower quality care, inadequate access to care, inability to navigate the system, provider ignorance or bias, and stress. Socio-economic status and race/ethnicity linked with avoidable procedures, hospitalisations and untreated disease.", "At each level of income or education, African American have worse outcomes than White resulting from adverse health effects of disadvantage or experiences linked to racial bias. Low birth weight linked to child health outcomes linked to lower socio-economic status and ethnic/minority status.", "In pre-retirement years, Hispanics and American Indians are much less likely than Whites, African Americans, and Asians have any health insurance. Negative net worth, zero net worth, and not owning a home in young adulthood are linked to depressive symptoms independent of other socio-economic indicators.", "Hispanics and African Americans report lower risk of psychiatric disorder relative to White counterparts, but those who become ill have more persistent disorders. African Americans, Hispanics, Asians, American Indians, and Native Hawaiians have higher rates of post-traumatic stress disorders than Whites, not explained by SES and history of psychiatric \u00a0disorders.", "American Indians are at heightened risk for post-traumatic stress disorder and alcohol dependence. Discrimination contribute to mental health disorders among Asian and African Americans. Relative to whites, African Americans are more frequently diagnosed with schizophrenia, a low prevalence but serious condition."], "Abstract": "Learn how socioeconomic status affects the lives of many racial and ethnic minorities.", "Reference": "APA (2017, July). Ethnic and Racial Minorities & Socioeconomic Status [Blog post]. Retrieved from https://www.apa.org/pi/ses/resources/publications/minorities", "You_may_also_be_interested_in": [{"Relevant_ref": "ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008)", "href": "#h.fxf9ychze3l3"}, {"Relevant_ref": "Education and Socio-economic status (APA, 2017b)", "href": "#h.d8en3o2spkj7"}, {"Relevant_ref": "Women and Socio-economic status (APA, 2010)", "href": "#h.5cve5vi76ant"}, {"Relevant_ref": "Disability and Socio-economic status (APA, 2010)", "href": "#h.fwgotxock7y7"}, {"Relevant_ref": "Lesbian, Gay, Bisexual and Transgender Persons & Socioeconomic Status (APA, 2010)", "href": "#h.75aqilh5muke"}]}, "summary_103": {"Title": "Faculty promotion must assess reproducibility (Flier, 2017) \u233a", "Id": "h.c3k3blbo0exl", "Main_Takeaways": ["Contributors besides intrinsic variability: inadequate training, increased competition, problems in peer review and publishing, and occasionally scientific misconduct.", "Diverse causes make finding solutions difficult, especially, as they must be implemented by independent constituencies including funders and publishers.", "Promotion criteria-impossible to advance to professor mainly to important papers with large and complex authorship instead of publishing papers as clear senior author.", "Committees now consider how well a candidate participate in team science.", "Clinical research, educational innovation and leadership have increased emphasis.", "We depend on imperfect metrics for judging research publications and ability to assess reliability and accuracy is underdeveloped.", "Reproducibility and robustness are under-emphasised when job applicants are evaluated and when faculty members are promoted.", "Reviewers assessed how a field would be different without a candidate\u2019s contributions, and survey a candidate\u2019s accomplishment, scholarship, and recognition.", "Processes should encourage evaluators to say whether they feel candidates\u2019 work is problematic or over-stated and whether it has been reproduced and broadly accepted.", "If not, they should say whether they believe widespread reproducibility is likely or whether work will advance the field.", "As a part of their application, to critically evaluate research, including unanswered questions, controversies and uncertainties. This signals the importance of assessment and creates a mechanism to judge a candidate\u2019s capacity for critical self-reflection.", "Evaluators should consider how candidates select or develop an animal model to generalise across species. They should be asked to consider how technical and statistical issues were handled. Research and discovery is not simple and unidirectional, and sceptical of candidates who oversimplify.", "Candidates are encouraged to make the case that their work is amazing. Scientists who demonstrate a deep understanding of limits of their approaches. If school leadership makes it clear these virtues are important, their role will be boosted.", "Institutions need to incentivise data sharing and transparency. Efforts are more urgent as increasingly interdisciplinary projects extend beyond individual investigators\u2019 expertise.", "Success will need creativity, pragmatism and diplomacy, because investigators bristle at any perceived imposition on their academic freedom."], "Quote": "\u201cOver time, efforts to increase the ratio of self-reflection to self-promotion may be the best way to improve science. It will be a slog, but if we don\u2019t take this on, formally and explicitly, nothing will change.\u201d (p.133)", "Abstract": "Research institutions should explicitly seek job candidates who can be frankly self-critical of their work, says Jeffrey Flier.", "Reference": "Flier J. (2017) Faculty promotion must assess reproducibility. Nature, 549(7671),133. https://doi.org/10.1038/549133a", "You_may_also_be_interested_in": [{"Relevant_ref": "Publication metrics and success on the academic job market (Van Dijk et al., 2014)", "href": "#h.ugiw8p46vedd"}, {"Relevant_ref": "Six principles for assessing scientists for hiring, promotion, and tenure (Naudet et al, 2018)", "href": "#h.1j8n5dtwa2fz"}]}, "summary_104": {"Title": "Women and Socio-economic status (APA, 2010)\u25c8\u00a0\u233a", "Id": "h.5cve5vi76ant", "Main_Takeaways": ["Socioeconomic status encompass quality of life attributes and opportunities and privileges afforded to people in society.", "Socio-economic status is a consistent and reliable predictor of outcomes across lifespan.", "Low socio-economic status and correlates affect society.", "Inequities in health distribution, resource distribution and quality of life are increasing in the US and globally.", "Socio-economic status contributes to the quality of life for women and affects the lives of children and families.", "Inequities in wealth and quality of life for women are long-standing and exist both locally and globally.", "Socio-economic status influences well-being and quality of life for women with women struggling more than men and more likely to live in poverty.", "Men are paid more than women despite similar levels of education and fields of occupation.", "Reduced income for women with longer life expectancy and increased responsibility to raise children, increase probabilities that women face economic disadvantages.", "Pay gap has narrowed over time but has plateaued.", "Women with a high school diploma are paid 80% of what men with the same qualifications are paid.", "Single mother families rely on earnings of one adult and more than 5 times as likely to live in poverty as married-couples families.", "Cost associated with pregnancy is higher for women than men. 46% of women believed they experienced gender discrimination.", "Pregnant women with low socio-economic status report more depressive symptoms, the third trimester may be more stressful for low-income women.", "At 2 and 3 months postpartum, women with low income have been found to experience more depressive symptoms than women with high-income.", "Women with insecure and low-status jobs with little to no decision-making authority experience higher-levels of negative life events, insecure housing tenure, more chronic stressors, and reduced social support.", "Depression and anxiety is higher for poor women in developing countries undergoing restructuring.", "Women with low income develop alcoholism and drug addiction influenced by social stressors linked to poverty.", "Improved balance in gender roles, obligations, pay equity, poverty reduction and renewed attention to maintenance of social capital redress the gender disparities in mental health.", "Women living with breast cancer are 11% more likely to die if they live in lower SES communities.", "Low-income women who have no insurance have lowest rates of mammography screening among women aged 40-64, increasing risk of death from breast cancer.", "Obesity and staying obese from adolescence to young adulthood linked to poverty among women.", "Relative to HIV-positive men, women with HIV are disproportionately low income in the US.", "Poverty is the most significant indicator of whether heterosexuals live in the inner city to develop the AIDS virus."], "Abstract": "Learn how socioeconomic status affects the lives of women.", "Reference": "APA. (2017, July). Ethnic and Racial Minorities & Socioeconomic Status [Blog post]. Retrieved from https://www.apa.org/pi/ses/resources/publications/women", "You_may_also_be_interested_in": [{"Relevant_ref": "ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008)", "href": "#h.fxf9ychze3l3"}, {"Relevant_ref": "Education and Socio-economic status (APA, 2017b)", "href": "#h.d8en3o2spkj7"}, {"Relevant_ref": "Disability and Socio-economic status (APA, 2010)", "href": "#h.fwgotxock7y7"}, {"Relevant_ref": "Ethnic and Racial minorities and socio-economic status (APA, 2017)", "href": "#h.pwgw2ot3vok"}, {"Relevant_ref": "Lesbian, Gay, Bisexual and Transgender Persons & Socioeconomic Status (APA, 2010)", "href": "#h.75aqilh5muke"}]}, "summary_105": {"Title": "The Gender Gap: Who Is (and Is Not) Included on Graduate-Level Syllabi in Social/Personality Psychology (Skitka et al., 2020) \u233a", "Id": "h.j2lw9gk12js0", "Main_Takeaways": ["One factor contributing gender gaps is whose work we choose to teach in graduate seminars.", "We hypothesise that one link in the broad chain of factors contributing to the eminence gender gap is female authors are likely to be under-represented on graduate course syllabi compared to their male peers (gender gap hypothesis).", "Female authors might be under-represented on course syllabi could be varied.", "Instructors may internalise cultural prejudices and biases favour men over women-preference may be greater for male over female-authored papers (i.e. bias hypothesis).", "The present study assesses whether benign accounts explains gender gaps included on graduate-level syllabus in social/personality psychology.", "If the classics hypothesis is true, instructors should prefer older over contemporary papers.", "In addition, male and female should be included on syllabi approaching equality as publication date of papers included on syllabi becomes more contemporary.", "It could also result from relative availability of publications authored by male versus female social/personality psychologists.", "There may not be gender equality because there are more male-authored papers to include than female-authored papers (i.e. availability hypothesis).", "The present study investigates whether there is a gender gap in representation on graduate level syllabi and whether it is explained by preference for classic over contemporary papers or relative availability of male- versus female-authored manuscripts.", "Method: The authors identified every social and/or personality PhD program in the US using the Social Psychology Network\u2019s PhD ranking list and Graduate Programs GeoSearch.", "120 programs were identified and a list of social/personality faculty names and email addresses for each program by going to psychology department websites.", "Method: We were interested in courses for first-year graduate students as an orientation to the field.", "Method: Inclusion criteria for syllabi were course name includes words: social or personality-72 syllabi combined produced 3415 assigned readings.", "Method: Gender of all authors, each author\u2019s h-index, total number of authors, journal article appeared, number of citations article received since publication and topic in social/personality psychology.", "Method: The present study obtained all names of authors, authorship order and year of publication in Journal of Social and Personality Psychology from 1965 to 2017-9799 papers.", "Method: From Personality and Social Psychology Bulletin from 1974 until April 2018-4291 papers. 75% of readings included on our syllabi were journal papers.", "These journals accounted for 33% of reading on sample course syllabi and formed benchmarks. We determined the gender of first authors for each comparison publication.", "Results: Less than 30% referenced on syllabi were female first authors, no greater inclusion of papers published by female first authors since 1980s.", "Results: Difference in inclusion rates of female first-authored paper could not be explained by preference for classic over contemporary papers in syllabi or relative availability of female first-authored papers in the published \u00a0literature.", "Gender gap in syllabi differed due to instructor gender and decade papers were published.", "Female instructors assigned more recently published papers at levels approaching gender equality, and female first-authored papers at levels higher than their male counterparts.", "Instructors of graduate level seminars in social/personality psychology showed preference for teaching contemporary over older publications.", "Availability hypothesis was not supported for the gender gap and the gender gap existed for all but one decade.", "Male and female authors were equally represented on graduate-level syllabi.", "Gender gap was much larger, with percentages of female first-authored papers between 0 and 15%. Do women publish less in some areas where the gender gap was greatest?", "Male and female-authored papers included on syllabi had similar citation rates, different h-index scores, proxies for article impact and author status.", "Article content matters more than total citations or author status, to the extent that these variables count in what gets included on graduate syllabi, instructors for most part appear using similar criteria irrespective of author gender.", "Increasing representation of female scholars\u2019 work on graduate course syllabi would have similarly beneficial consequences.", "Adding more citations to female scholars\u2019 work on syllabi may not solve gender gaps but improves gender parity in social/personality psychology."], "Abstract": "We contacted a random sample of social/personality psychologists in the United States and asked for copies of their graduate syllabi. We coded more than 3,400 papers referenced on these syllabi for gender of authors as well as other characteristics. Less than 30% of the papers referenced on these syllabi were written by female first authors, with no evidence of a trend toward greater inclusion of papers published by female first authors since the 1980s. The difference in inclusion rates of female first-authored papers could not be explained by a preference for including classic over contemporary papers in syllabi (there was evidence of a recency bias instead) or the relative availability of female first-authored papers in the published literature. Implications are discussed.", "Reference": "Skitka, L. J., Melton, Z. J., Mueller, A. B., & Wei, K. Y. (2020). The Gender Gap: Who Is (and Is Not) Included on Graduate-Level Syllabi in Social/Personality Psychology. Personality and Social Psychology Bulletin, 0146167220947326. https://doi.org/10.1177/0146167220947326", "You_may_also_be_interested_in": [{"Relevant_ref": "Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019)", "href": "#h.fbcbbt5lpe28"}, {"Relevant_ref": "Against Eminence (Vazire, 2017)", "href": "#h.ixq69pvnhnnt"}, {"Relevant_ref": "Scientific Eminence: Where Are the Women? (Eagly & Miller, 2016)", "href": "#h.fztee3dibdn9"}]}, "summary_106": {"Title": "Leveraging a collaborative consortium model of mentee/mentor training to foster career progression of under-represented post-doctoral researchers and promote institutional diversity and inclusion (Risner et al., 2020) \u233a", "Id": "h.um3ggunxziz6", "Main_Takeaways": ["The goal is to empower postdocs to be active participants in mentoring relationships and emphasise mentees\u2019 contributions to shape more productive interactions built to develop own skills as a future mentor.", "Inclusivity involves changing the environment in which trainees are embedded by creating equitable scientific communities that all scientists feel welcomed and valued.", "This paper reports a collaborative, multi-institutional approach by leveraging National Research Mentoring Network Academic Network model and metrics measured success of multi-year pilot study.", "Method: This paper reports results of a four-year of experience with a multi-institution model with follow-up implementation of mentor-training sessions on home campuses to foster capacity-building, dissemination and sustainability of good mentoring practices.", "Method: Establishing a functioning consortium needs buy-in and high-level cooperation from all partners, all potential institutional representatives set initial goals to address campus needs for mentor up skill development for post-doc and mentor facilitator training for staff and faculty.", "Method: Establish sustainable communities of practice for mentor training and develop mechanisms for central coordination, outreach to campus constituents, templates for recruitment of participants and strategies to sustain collaboration.", "Method: The seven Core Principles: Two-way communication, aligning expectations, assessing understanding, fostering independence, ethics, addressing equity and inclusion and promoting professional development.", "Method: Curriculum provide post-docs with opportunities of self-evaluation and reflection to become aware of their personal biases, attitudes and behaviours, explore strengths, weaknesses, and challenges in their interpersonal and professional relationships; understanding and learning how to use mentor principles and focus on cognitive processes leading to behavioural changes.", "Method: There are six key competencies for Entering mentoring: maintaining effective communication, establishing and aligning expectations, assessing understanding of scientific research, addressing diversity, fostering independence and promoting professional development.", "Method: All four post-doc cohorts measure whether mentor training affected career progression, influence postdoc relationship with Principal Investigators and components of mentor training implemented by postdoc mentees.", "Results: Post-doctoral students report improvements in mentoring proficiency and improved relationships with Principal Investigators.", "29% of post-doc respondents transitioned to faculty positions-85% were under-represented \u00a0and 75% were female.", "Results: 59 out of 120 faculty and administrators trained in the first three years provided mentor training on campuses to over 3000 undergraduate and graduate students, post-docs and faculty within the project period.", "Failure is most evident at transition from post-doctoral to faculty. Evidence indicates Mentoring-UP training for trainees for postdocs and Mentor Facilitator training for faculty contribute to mentee success individuals.", "Majority (70%) of post-docs indicate mentor training positively influenced relationships with their mentors in several important career-enhancing domains.", "Evidence-based mentoring up curriculum guided the majority of Big Ten Academic Alliance postdoc participants to better understand their mentoring needs, develop strategies to manage their mentoring relationships and empower them to make career decisions to remain in pursuit of an academic career (59%).", "Post-docs and early career scientists have more confidence to pursue an academic career (80%) after participating in workshops.", "Mentor training for postdocs significantly (88%) improved perceived proficiency as mentors, females (75%) and under-represented (62%) post-docs.", "Post-docs indicated mentor training benefitted career decisions. Integrating mentoring training into their institution\u2019s Presidential Postdoc Fellows program for both the advisors and postdocs; running a 10 week mentor training course for graduate students and postdocs and implementing faculty mentor training for four different departments at request of deans and department chairs."], "Abstract": "Changing institutional culture to be more diverse and inclusive within the biomedical academic community is difficult for many reasons. Herein we present evidence that a collaborative model involving multiple institutions of higher education can initiate and execute individual institutional change directed at enhancing diversity and inclusion at the postdoctoral researcher (postdoc) and junior faculty level by implementing evidence-based mentoring practices. A higher education consortium, the Big Ten Academic Alliance, invited individual member institutions to send participants to one of two types of annual mentor training: 1) \u201cMentoring-Up\u201d training for postdocs, a majority of whom were from underrepresented groups; 2) Mentor Facilitator training\u2014a train-the-trainer model\u2014for faculty and senior leadership. From 2016 to 2019, 102 postdocs and 160 senior faculty and administrative leaders participated. Postdocs reported improvements in their mentoring proficiency (87%) and improved relationships with their PIs (71%). 29% of postdoc respondents transitioned to faculty positions, and 85% of these were underrepresented and 75% were female. 59 out of the 120 faculty and administrators (49%) trained in the first three years provided mentor training on their campuses to over 3000 undergraduate and graduate students, postdocs and faculty within the project period. We conclude that early stage biomedical professionals as well as individual institutions of higher education benefited significantly from this collaborative mentee/mentor training model", "Reference": "Risner, L. E., Morin, X. K., Erenrich, E. S., Clifford, P. S., Franke, J., Hurley, I., & Schwartz, N. B. (2020). Leveraging a collaborative consortium model of mentee/mentor training to foster career progression of underrepresented postdoctoral researchers and promote institutional diversity and inclusion. PloS one, 15(9), e0238518. https://doi.org/10.1371/journal.pone.0238518", "You_may_also_be_interested_in": [{"Relevant_ref": "A cry for help (Nature, 2019)", "href": "#h.v7kmozf9h8f7"}, {"Relevant_ref": "Postdocs in crisis: science risks losing the next generation (Nature, 2020)", "href": "#h.68x0qk7jsz9j"}, {"Relevant_ref": "Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019)", "href": "#h.fbcbbt5lpe28"}, {"Relevant_ref": "Scientific Eminence: Where Are the Women? (Eagly & Miller, 2016)", "href": "#h.fztee3dibdn9"}]}, "summary_107": {"Title": "An index to quantify an individual\u2019s scientific research output (Hirsch, 2005)", "Id": "h.v4o766rkbx9r", "Main_Takeaways": ["Scientists who earn a Nobel prize have relevant research with a huge impact. \u00a0How do we quantify those others\u2019 impact and relevance?", "Number of publications, number of citations for each paper, journal where papers were published and impact parameters.", "H is a preferable index to evaluate scientific output to a researcher.", "Total number of papers: Benefit: measures productivity, yet does not measure importance of impact of papers.", "Citations measure total impact, yet hard to find and may be accentuated by \u201cbig hits\u201d that are not representative of individuals if they are co-authors.", "Total citations have an unfair advantage to review articles than original research contributions.", "Citations per paper allows comparison of scientists of different ages, yet hard to find, rewards low productivity and penalises high productivity.", "Number of significant papers defined as number of papers with y citations. It removes the difficulty to locate hard to find, punishes low productivity and rewards high productivity, whilst considering impact of papers and ignores big hits.", "However y is arbitrary and will favour or disfavour individuals randomly. It needs to be adjusted for levels of seniority.", "Number of citations to each of q most-cited papers.", "Overcomes many before, yet it is not a single number and difficult to obtain and compare. Higher the h, the more accomplished the scientist is. H should increase with time.", "All published papers contribute to h and will smoothly level off as the number of papers increases instead of a discontinuous change in slope. Papers with low citations will not contribute to a researcher\u2019s h if written late in career.", "H cannot decrease with time. Paper has exactly h citations at risk of being removed from individual\u2019s h count.", "Papers drop out and come back into h count- sleeping beauties. m = 1 (successful scientist with h = 20) m = 2 (outstanding scientist in research institutes and well-respected universities h = 40 higher) m = 3 (highly unique h = 60 after 2 years or 90 after 3.", "M parameter is useful if a scientist does not sustain productivity, whilst h parameter is useful for cumulative achievement continuing over time even after the end of the scientist\u2019s publication.", "Advancement to tenure and h = 18 is important for full professorship.", "A single number can never give more than an estimation to an individual\u2019s multi-faceted profile and many other factors need to be combined to evaluate an individual.", "Differences in typical h values in different fields like average number of papers produced by each scientist in field and size of field.", "More scientists share a larger number of citations. Scientists working in non-mainstream areas will not achieve the same high h values at the top level of those working in highly topical areas.", "High h is a reliable indicator of high accomplishment the opposite is not always not true.", "A few seminal papers with high citation counts and h index will not reflect scientist\u2019s accomplishment.", "Self citations need to be removed as it increases their h index.", "All self-citations to papers with h citations are irrelevant as are self-citations to papers with many more than h citations.", "Nobel prize winners have an h index of 30, meaning it did not occur in one stroke of luck but in a body of scientific work.", "Values of m are found often not high compared with other successful scientists.", "Overall h index of a group will be larger than each of members of group but smaller than sum of individual h indices as some papers offer to each individual\u2019s h will no longer contribute to group\u2019s h.", "H index could be important for rankings of groups or departments in the chosen area and administrators could be interested in this."], "Abstract": "I propose the index h, defined as the number of papers with citation number >h, as a useful index to characterize the scientific output of a researcher.", "Reference": "Hirsch, J. E. (2005). An index to quantify an individual's scientific research output. Proceedings of the National academy of Sciences, 102(46), 16569-16572. https://doi.org/10.1073/pnas.0507655102", "You_may_also_be_interested_in": [{"Relevant_ref": "High Impact =High Statistical Standards? Not Necessarily So (Tressoldi et al., 2013)", "href": "#h.bt5o53my0bmr"}]}, "summary_108": {"Title": "High Impact =High Statistical Standards? Not Necessarily So (Tressoldi et al., 2013)", "Id": "h.bt5o53my0bmr", "Main_Takeaways": ["Scientific papers publish in journals with the highest impact factor to measure scientific value and methodological quality.", "The present study investigated how often null hypothesis significance testing and alternative methods are used in leading scientific journals and compare frequencies with those of journals with lower impact factors.", "Null hypothesis significance testing focuses on rejection of H0 around 0.05. 2nd limitation-p value is different if experiment is repeated.", "The third limitation-there is a difference from 0. Null hypothesis is not correct. \u00a0H0 will be rejected with sample size, so results of Null hypothesis significance test says more about sample size as about any hypothesis.", "Null hypothesis significance testing does not give an estimate of difference from H0, yes there is a difference from 0.", "Fifth limitation-null hypothesis significance testing does not provide information about precision, meaning error in estimate of a parameter like mean, proportion or correlation.", "Any estimate based on physical, biological or behavioural measure will have errors. We need to know how large. \u00a0How many studies with the highest impact factors adopt these recommendations? Are they different from low impact?", "Method: \u00a06 Journals were chosen with high impact and 6 with low impact. The present study compared across journals using all relevant articles with many variables adding to any difference found.", "Results: Null hypothesis Significance Testing without any use of confidence intervals, effect size, prospective power and model estimation is prevalent statistical practice used in articles in Nature, 89% followed articles in Science.", "Results: In other journals both with high and low impact factor report confidence intervals and/or effect size measures.", "Confidence intervals and effect size were present in more than 80% of articles, while this percentage drops to less than 30% of articles in Science and in less than 11% of articles in Nature.", "Reporting confidence intervals and effect size do not guarantee researchers use them in the interpretation.", "We used a liberal approach in statistical practices for interpretation. \u00a0Many authors reported them, does not mean they were used for interpretation or referred in text.", "Most used only Null hypothesis significance testing. \u00a0We need to report and interpret effect size and confidence intervals. \u00a0Statistical practices differ across journals irrespective of strength of impact factor."], "Abstract": "What are the statistical practices of articles published in journals with a high impact factor? Are there differences compared with articles published in journals with a somewhat lower impact factor that have adopted editorial policies to reduce the impact of limitations of Null Hypothesis Significance Testing? To investigate these questions, the current study analyzed all articles related to psychological, neuropsychological and medical issues, published in 2011 in four journals with high impact factors: Science, Nature, The New England Journal of Medicine and The Lancet, and three journals with relatively lower impact factors: Neuropsychology, Journal of Experimental Psychology-Applied and the American Journal of Public Health. Results show that Null Hypothesis Significance Testing without any use of confidence intervals, effect size, prospective power and model estimation, is the prevalent statistical practice used in articles published in Nature, 89%, followed by articles published in Science, 42%. By contrast, in all other journals, both with high and lower impact factors, most articles report confidence intervals and/or effect size measures. We interpreted these differences as consequences of the editorial policies adopted by the journal editors, which are probably the most effective means to improve the statistical practices in journals with high or low impact factors.", "Reference": "Tressoldi, P. E., Giofr\u00e9, D., Sella, F., & Cumming, G. (2013). High impact= high statistical standards? Not necessarily so. PloS one, 8(2), e56180. https://doi.org/10.1371/journal.pone.0056180", "You_may_also_be_interested_in": [{"Relevant_ref": "An index to quantify an individual\u2019s scientific research output (Hirsch, 2005)", "href": "#h.v4o766rkbx9r"}]}, "summary_109": {"Title": "Disability and Socio-economic status (APA, 2010) \u25c8\u00a0\u233a", "Id": "h.fwgotxock7y7", "Main_Takeaways": ["There is the Disabilities Act that assures equal opportunities in education and employment for people with difficulties and prohibits discrimination on the basis of disability, people with disabilities remain over-represented among America\u2019s poor and under-educated.", "There is the Social Security Disability Insurance for workers who have become disabled and unable to work after paying Social Security taxes for at least 40 quarters.", "This program involves a higher income and produces higher Social Security Disability Insurance earnings.", "Supplemental Security Income is a welfare program for individuals with low income, fewer overall resources and no or an abbreviated work history.", "Current federal benefit for a single person using Supplemental Security Income is $735 a month.", "Supplemental Security income recipients qualify for Medicaid and people with disabilities are more likely to be unemployed and live in poverty.", "American Association of People with Disabilities estimates that two thirds of people with disabilities are of working age and want to work.", "There are disparities in median incomes for people with and without disabilities.", "A study surveyed human resources and project managers about perceptions of hiring persons with disabilities.", "Professionals held negative perceptions related to productivity, social maturity, interpersonal skills and psychological adjustment of persons with disabilities.", "Disparities in education have been ongoing for generations. 65 years and older, 20.9% of those without a disability failed to complete high school, relative to 25.1% and 38.6% of individuals with a non-severe or severe disability, who failed to complete high school.", "Great disparities exist when comparing attainment of higher degrees. 15.1% of the population aged 25 and over with disability obtain a bachelor\u2019s degree, whereas individuals in the same age category with no disability attain the same educational status."], "Abstract": "Learn how socioeconomic status affects individuals with disabilities.", "Reference": "APA (2010). Disability & Socioeconomic Status [Blog post]. Retrieved from https://www.apa.org/pi/ses/resources/publications/disability", "You_may_also_be_interested_in": [{"Relevant_ref": "ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008)", "href": "#h.fxf9ychze3l3"}, {"Relevant_ref": "Education and Socio-economic status (APA, 2017b)", "href": "#h.d8en3o2spkj7"}, {"Relevant_ref": "Ethnic and Racial minorities and socio-economic status (APA, 2017)", "href": "#h.pwgw2ot3vok"}, {"Relevant_ref": "Women and Socio-economic status (APA, 2010)", "href": "#h.5cve5vi76ant"}, {"Relevant_ref": "Lesbian, Gay, Bisexual and Transgender Persons & Socioeconomic Status (APA, 2010)", "href": "#h.75aqilh5muke"}]}, "summary_110": {"Title": "A cry for help (Nature, 2019)", "Id": "h.v7kmozf9h8f7", "Main_Takeaways": ["29% of 5700 respondents listed their mental health as an area of concern and under half sought help for anxiety or depression caused by their PhD study.", "Things get worse.", "6300 graduate students from around the world are satisfied with experience of research (71%) and 36% had help for anxiety or depression related to their PhD.", "How can graduate students be both broadly satisfied, but increasingly unwell. 1/5 of respondents report being bullied and experience harassment or discrimination.", "Could universities take more effective action? Yes, Are they? No. \u00bc said their institution provides support but 1/3 said they seek help elsewhere.", "Career success is measured by measurement including publications, citations, funding and impact.", "Early-career jobs tend to be precarious. \u00a0Researchers need to be hitting the right notes in regard to measures listed above.", "Most students embark on a PhD as a foundation of an academic career-career due to freedom and autonomy to discover and invent. Problems arise when autonomy in such matters is reduced or removed which is what happens when targets for funding, impact and publications become part of universities\u2019 formal monitoring and evaluation systems.", "Student\u2019s supervisors judge their success or failure, it is not surprising many feel unable to open up about vulnerabilities or mental-health concerns. Solutions not solely in institutions do more to provide on-campus mental health support-as essential as such actions are.", "Much has been written about how to overhaul the system and find a better way to define success in research, including promoting that many non-academic careers are open to researchers. System makes young people ill and needs our help."], "Abstract": "Without systemic change to research cultures, graduate-student mental health could worsen.", "Reference": "Nature. (2019). The mental health of PhD researchers demands urgent attention. Nature, 575, 257-258. 10.1038/d41586-019-03489-1", "You_may_also_be_interested_in": [{"Relevant_ref": "Leveraging a collaborative consortium model of mentee/mentor training to foster career progression of under-represented post-doctoral researchers and promote institutional diversity and inclusion (Risner et al., 2020)", "href": "#h.um3ggunxziz6"}, {"Relevant_ref": "A cry for help (Nature, 2019)", "href": "#h.v7kmozf9h8f7"}, {"Relevant_ref": "Postdocs in crisis: science risks losing the next generation (Nature, 2020)", "href": "#h.68x0qk7jsz9j"}, {"Relevant_ref": "Boosting research without supporting universities is wrong-headed \u00a0(Nature, 2020b)", "href": "#h.jt3augsu7xw8"}, {"Relevant_ref": "Seeking an exit plan (Woolston, 2020)", "href": "#h.nufoz6k4cnj7"}, {"Relevant_ref": "Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019)", "href": "#h.fbcbbt5lpe28"}]}, "summary_111": {"Title": "Postdocs in crisis: science risks losing the next generation (Nature, 2020)", "Id": "h.68x0qk7jsz9j", "Main_Takeaways": ["Post-doctoral researchers are going to be in career limbo and create immense anxiety and uncertainty.", "Pandemic adds to postdocs\u2019 distress. Pandemic worsened career prospects and supervisors have not done enough to support them during pandemic.", "51% leave active research due to work-related mental health concerns-tragic-as many early-career researchers are in distress and spell trouble for knowledge, discovery and inventions and many people conclude have no future in science.", "All efforts to help workers are welcome but on their own, small measures will not be enough to save many academic science careers.", "Universities cannot be expected to bear this extra cost.", "Pandemic is testing their finances for institutions dependent on income from international students\u2019 fees.", "Global student mobility will be much lower than usual in the coming academic year, and some institutions will lose a good fraction of their fee income as a result.", "Contract-research workers such as post-docs are most vulnerable to losing their jobs and will disproportionately influence women and people from minority groups who form a high share of post-doctoral workforce.", "Many post-docs are looking to leave their posts now, anticipating worse is to come and be champions for those who want to pursue fulfilling careers in science elsewhere.", "People find satisfying careers in science.", "Principal investigators show flexibility, patience and support for everyone in their group.", "They and their institutions must push harder than ever for accessible mental health services.", "Early career researchers faced pressures of continuous assessment and a more competitive and less secure working atmosphere than those who came before them."], "Abstract": "The pandemic has worsened the plight of postdoctoral researchers. Funders need to be offering more than moral support.", "Reference": "Nature. (2019). Postdocs in crisis: science cannot risk losing the next generation.\u00a0Nature, 580, 160. 10.1038/d41586-020-02541-9", "You_may_also_be_interested_in": [{"Relevant_ref": "Leveraging a collaborative consortium model of mentee/mentor training to foster career progression of under-represented post-doctoral researchers and promote institutional diversity and inclusion (Risner et al., 2020) \u233a", "href": "#h.um3ggunxziz6"}, {"Relevant_ref": "A cry for help (Nature, 2019)", "href": "#h.v7kmozf9h8f7"}, {"Relevant_ref": "Boosting research without supporting universities is wrong-headed \u00a0(Nature, 2020b)", "href": "#h.jt3augsu7xw8"}, {"Relevant_ref": "Seeking an exit plan (Woolston, 2020)", "href": "#h.nufoz6k4cnj7"}, {"Relevant_ref": "Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019)", "href": "#h.fbcbbt5lpe28"}]}, "summary_112": {"Title": "Boosting research without supporting universities is wrong-headed \u00a0(Nature, 2020b) \u233a", "Id": "h.jt3augsu7xw8", "Main_Takeaways": ["Coronavirus lockdowns have precipitated a crisis in university funding and academic morale.", "Universities all over the world closed their doors. Classes were moved and some research activity was moved online.", "Staff were given little or no time to prepare and few resources or training to help them.", "Fewer students are expected to enrol in the coming academic year, instead waiting until institutions open fully, young people will lose a year of their education and universities will lose out financially.", "Governments have plans to boost post-lockdown research but undermined if universities make job cuts and end up with staff shortages. Universities need support at this crucial time. Low- and middle-income countries face extra challenges from sudden transition to online learning.", "Students unable to access digital classrooms-those who live in areas without fast, reliable and affordable broadband or where students have no access to laptops, tablets, smartphones and other essential hardware.", "Teachers report students struggle to keep up since lockdown began. Students from poorer households in remote regions travel to the nearest city to access the Internet and pay commercial internet cafes to download course materials.", "It needs governments and funding bodies to accept students and universities should be eligible for the same kinds of temporary emergency funding as other industries are asking for.", "Airlines and tourism discuss bailout schemes with their governments so they can maintain workforce will be needed when they reopen.", "Governments have denied requests or delayed decisions. In high-income countries, partly because universities are functioning and less deserving of government help than businesses and professions have had no choice but to close.", "In poorer countries-public funding that universities depend on is under threat because economies have crashed during lockdowns.", "Governments need to realise the impact of decisions fail disproportionately on poorest students and on more vulnerable members of staff.", "Job cuts are more likely to affect people whose employment is less secure-fixed-term contracts. Staff, especially minority groups, will in turn are over-represented in contract staff.", "Students and staff on short-term contracts welcome more support from academic colleagues in senior positions and from others with permanent positions.", "Colleagues make the case for managers failing to provide more help to low-income students or cut the number of post-doctoral staff and teaching fellow harm to the next generation of researchers and teachers.", "It will reduce departments\u2019 capacity to teach and increase load on those who remain and take on teaching responsibilities of their former colleagues.", "Senior colleagues request assessments of how many planned redundancies will influence equality and diversity.", "Cutting back on scholarly capacity is unwise, but increasing spending on research and development is wrong-headed, slowing down economic recovery and jeopardising plans to make research more inclusive."], "Abstract": "Universities face a severe financial crisis, and some contract staff are hanging by a thread. Senior colleagues need to speak up now.", "Reference": "Nature. (2020). Boosting research without supporting universities is wrong-headed. Nature, 582, 313-314. 10.1038/d41586-020-01788-6", "You_may_also_be_interested_in": [{"Relevant_ref": "A cry for help (Nature, 2019)", "href": "#h.v7kmozf9h8f7"}, {"Relevant_ref": "Postdocs in crisis: science risks losing the next generation (Nature, 2020)", "href": "#h.68x0qk7jsz9j"}, {"Relevant_ref": "Seeking an exit plan (Woolston, 2020)", "href": "#h.nufoz6k4cnj7"}, {"Relevant_ref": "Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019)", "href": "#h.fbcbbt5lpe28"}]}, "summary_113": {"Title": "Seeking an exit plan (Woolston, 2020)", "Id": "h.nufoz6k4cnj7", "Main_Takeaways": ["Full impact of COVID-19 pandemic on scientific careers might not be known for years, but hiring freezes and other signs of turmoil at universities shake faith in academia as career options.", "A growing number of PhD students and other early-career researchers look at careers in industry, government and other sectors.", "It is unclear how many of these researchers leave academia out of choice or necessity but observers expect seismic readjustment in scientific careers.", "Where Hiring slowdowns and freezes at universities are less common in Germany, the last few months are \u00a0a time for reflection and rethinking.", "Shortage of tenured and tenure-track university positions deepen in coming years. In the United States- recession coincided with a strong shift towards gig or temporary work. Academic escapees emphasise skills developed in university careers."], "Abstract": "The pandemic is prompting some early-career researchers to rethink their hopes for a university post. By Chris Woolston.", "Reference": "Nature. (2020). Seeking an \u2018exit plan\u2019 for leaving academia amid coronavirus worries. Nature 583, 645-646. Doi: 10.1038/d41586-020-02029-6.", "You_may_also_be_interested_in": [{"Relevant_ref": "Leveraging a collaborative consortium model of mentee/mentor training to foster career progression of under-represented post-doctoral researchers and promote institutional diversity and inclusion (Risner et al., 2020) \u233a", "href": "#h.um3ggunxziz6"}, {"Relevant_ref": "Postdocs in crisis: science risks losing the next generation (Nature, 2020)", "href": "#h.68x0qk7jsz9j"}, {"Relevant_ref": "Boosting research without supporting universities is wrong-headed \u00a0(Nature, 2020b)", "href": "#h.jt3augsu7xw8"}]}, "summary_114": {"Title": "Lesbian, Gay, Bisexual and Transgender Persons & Socioeconomic Status (APA, 2010) \u25c8\u00a0\u233a", "Id": "h.75aqilh5muke", "Main_Takeaways": ["Individuals who identify as Lesbian, gay, bisexual and/or transgender are susceptible to socio-economic disadvantages.", "These are linked to rights, quality of life, and general well-being of Lesbian, Gay, Bisexual and/or transgender persons.", "Low income LGBT individuals and same-sex/gender couples have bene found more likely to receive cash assistance and food stamps benefits compared to heterosexual individuals or couples.", "Transgender adults were nearly 4 times more likely to have household income of less than $10,000 per year relative to general population.", "Raising the federal minimum wage benefits LGBT individuals and couples in the United States.", "Increase in minimum wage reduce poverty rate by 25%for same-sex/gender female couples and 30% for same-sex/gender male couples.", "Poverty rates would be projected to fall for the most vulnerable individuals in same-sex/gender couples, including African American, couples with children, people with disabilities, individuals under 24 years of age, people without high school diplomas or the equivalent, and those living in rural areas.", "The socio-economic position may be linked to experiences of discrimination.", "Gay and bisexual men who earned higher income were less likely to report discrimination relative to lower socio-economic position.", "Discrimination against and unfair treatment of LGBT persons remains legally permitted. 47% of transgender individuals report being discriminated against in hiring, firing and promotion, over 25% had lost a job due to discrimination based on gender identity.", "A lack of acceptance and fear of persecution lead many LGBT youth to leave their homes and live in transitional housing or on the street.", "Many LGBT youth may be rejected by their family of origin or caregivers and forced to leave home as minors.", "LGBT youth experience homeless at a disproportionate rate.", "LGBT homeless youth are more likely than their homeless heterosexual counterparts have poorer mental and physical health outcomes.", "Marriage licenses to same-sex couples and recognise same-sex unions have been legally performed in other states, legal barriers continue to exist.", "Workplace and housing discrimination result increasing socio-economic status disparities for LGBT persons and families. 20 states and District of Columbia prohibit discrimination in workplace based on sexual orientation and gender identity.", "18 states have no laws prohibiting workplace discrimination against LGBT people. 19% of transgender individuals refused a home or apartment and 11% report being evicted because of their gender identity or expression."], "Abstract": "This fact sheet explains the impact socioeconomic status has on gender identity and sexual orientation.", "Reference": "APA (2010). Sexual Orientation, Gender identity & Socioeconomic Status [Blog post]. Retrieved fromhttps://www.apa.org/pi/ses/resources/publications/lgbt", "You_may_also_be_interested_in": [{"Relevant_ref": "ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008)", "href": "#h.fxf9ychze3l3"}, {"Relevant_ref": "Education and Socio-economic status (APA, 2017b)", "href": "#h.d8en3o2spkj7"}, {"Relevant_ref": "Ethnic and Racial minorities and socio-economic status (APA, 2017)", "href": "#h.pwgw2ot3vok"}, {"Relevant_ref": "Women and Socio-economic status (APA, 2010)", "href": "#h.5cve5vi76ant"}, {"Relevant_ref": "Disability and Socio-economic status (APA, 2010)", "href": "#h.fwgotxock7y7"}]}, "summary_115": {"Title": "The Focus on Fame distorts Science (Innes-Ker, 2017) \u25c8\u00a0\u233a", "Id": "h.fiwxl1sbqyrw", "Main_Takeaways": ["Asking if you are famous is a wrong question, it focuses on the individual scientist, as if science is a lonely enterprise of hopeful geniuses.", "We should focus on ideas and knowledge and refining those ideas.", "Recording companies and Book publishers focus on the winners, but it is a crap-shoot but is also shown in science.", "A scientific enterprise looks for stars as opposed to results behaving similarly. H-index is not an objective measure.", "There is an assumption that peer review assures papers are solid and citations are a measure for quality.", "Science is argued to advance in an evolutionary manner-wealth of ideas is produced, some are selected and survive but depend on scientific merit and social process.", "Process involves production of papers, citations and engagement of groups of scientists.", "Ideas engage groups of scientists will grow and change and bring knowledge closer to the truth. Ideas that are not interacted with will die.", "This is far from focus on eminence and individual fame.", "Competition is a factor but cooperation is vital.", "For ideas to survive, multiple labs need to engage with them as champions or severe adversarial testers.", "If we focus on who may become eminent, we lose some power of the scientific process.", "Eminent scientists would be nowhere without collaborators and adversaries willing to engage with the ideas.", "Science is littered with sole ideas that went nowhere-they disappear like failed commercial products.", "The tendency to overwhelmingly publish only positive results with no clear avenue for publishing failures to confirm, means scientists we are not grappling with the real field.", "Recent work to improve methods, statistics and publishing practices is an example of collaboration.", "In science, scientific ideas need to be stress-tested, not scientists.", "We need to move away from the cultural market model of science focusing on individual instead robustness of ideas. Science is a low yield, high risk business.", "Assigning individual merit based on productivity and citation encourages poor scientific practices and discourages collaboration and argumentative engagement with ideas. It results in a waste of talent."], "Abstract": "The 2016 symposium on Scholarly Merit focused on individual eminence and fame. I argue, with some evidence, that the focus on individual merit distorts science. Instead we need to focus on the scientific ideas, and the creation of collaborative groups.", "Reference": "Innes-Ker, \u00c5. (2017). The Focus on Fame Distorts Science. https://psyarxiv.com/vyr3e/", "You_may_also_be_interested_in": [{"Relevant_ref": "Fame: I\u2019m Skeptical (Ferreira, 2017)", "href": "#h.vkb9jz1fj01l"}, {"Relevant_ref": "Let\u2019s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017)", "href": "#h.lr9irendycbv"}, {"Relevant_ref": "\u201cFame\u201d is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017)", "href": "#h.4h5yr3mehhqy"}, {"Relevant_ref": "Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017)", "href": "#h.1g4r1nf76afn"}, {"Relevant_ref": "Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016)", "href": "#h.6y2j1ai8fhzp"}, {"Relevant_ref": "Against Eminence (Vazire, 2017)", "href": "#h.ixq69pvnhnnt"}, {"Relevant_ref": "Giving Credit Where Credit\u2019s Due: Why It\u2019s So Hard to Do in Psychological Science (Simonton, 2016)", "href": "#h.gqzauu6c437o"}, {"Relevant_ref": "Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016)", "href": "#h.hx1ixz9w9khb"}, {"Relevant_ref": "Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016)", "href": "#h.xxdqanhbam8s"}, {"Relevant_ref": "Improving Departments of Psychology (Diener, 2016)", "href": "#h.plqv88khmyxu"}, {"Relevant_ref": "Varieties of Fame in Psychology (Roediger III, 2016)", "href": "#h.ubwvvcnua9qq"}, {"Relevant_ref": "Scientific Eminence: Where Are the Women? (Eagly & Miller, 2016)", "href": "#h.fztee3dibdn9"}, {"Relevant_ref": "Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016)", "href": "#h.q2na8s8kahet"}]}, "summary_116": {"Title": "Fame: I\u2019m Skeptical (Ferreira, 2017) \u25c8 \u233a", "Id": "h.vkb9jz1fj01l", "Main_Takeaways": ["Most of us believe we have the respect of our peers, acknowledge we wish to be admired and viewed as successful and important.", "No psychologist and no rational person would deny that evaluating people and the quality of their work is necessary and inevitable in any field, whether it be art, medicine or science.", "We admit most promising candidates to graduate programs, hire the best faculty, tenure only those who have long productive careers and reward scientists with prizes if they contributed more than most to uncover the nature of psychological processes.", "We must not conflate fame and scientific quality, integrity and impact on the other.", "All of us point to colleagues who completed excellent work but are barely known or who are not famous until long after their research careers have ended.", "Some scientists are well known because they have been called out for unethical practices, including data fabrication and other forms of cheating.", "We need to discriminate between what one must do to become famous and other is what leads a person to end up famous.", "It is an attempt to reconstruct what led a person to attain a specific status.", "Fame should not be a goal and valuing people or ideas because they are famous comes is risky.", "Fame should be viewed with caution and scepticism to avoid temptation to assume if someone is famous, their work is significant.", "Fame perpetuates discrimination and overlook excellent people and work. Fame reflects quality and puts up our guards a bit.", "We need to challenge conventional wisdom and population opinion-fame is a popular opinion among elite, educated and thoughtful individuals who are prone to the same biases as everyone.", "We should never hesitate to question the ideas of someone who is not famous or ideas that are true.", "We should not refuse to view the work of famous people positively or refuse to give it its due but we must be careful to think an idea is useful due to the person being famous."], "Abstract": "Fame is often deserved, emerging from a person\u2019s significant and timely contributions to science. It is also true that fame and quality clearly sometimes diverge: many people who do excellent work are barely known, and some people are famous even though their work is mediocre. Reliance on fame and name recognition when identifying psychologists as candidates for honors and awards helps to perpetuate a range of stereotypes and prevents us from broadening participation in our field, particularly from women and underrepresented groups. The pursuit of fame may also be contributing to the current crisis in psychology concerning research integrity, because it incentivizes quantity and speed in publishing. The right attitude towards fame is to use it wisely if it happens to come, but to focus our efforts on conducting excellent research and nurturing talent in others.", "Reference": "Ferreira, F. (2017). Fernanda Ferreira--Fame: I'm Skeptical (2017). \u00a0https://psyarxiv.com/6zb4f/", "You_may_also_be_interested_in": [{"Relevant_ref": "The Focus on Fame distorts Science (Innes-Ker, 2017)", "href": "#h.fiwxl1sbqyrw"}, {"Relevant_ref": "Let\u2019s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017)", "href": "#h.lr9irendycbv"}, {"Relevant_ref": "\u201cFame\u201d is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017)", "href": "#h.4h5yr3mehhqy"}, {"Relevant_ref": "Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017)", "href": "#h.1g4r1nf76afn"}, {"Relevant_ref": "Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016)", "href": "#h.6y2j1ai8fhzp"}, {"Relevant_ref": "Against Eminence (Vazire, 2017)", "href": "#h.ixq69pvnhnnt"}, {"Relevant_ref": "Giving Credit Where Credit\u2019s Due: Why It\u2019s So Hard to Do in Psychological Science (Simonton, 2016)", "href": "#h.gqzauu6c437o"}, {"Relevant_ref": "Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016)", "href": "#h.hx1ixz9w9khb"}, {"Relevant_ref": "Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016)", "href": "#h.xxdqanhbam8s"}, {"Relevant_ref": "Improving Departments of Psychology (Diener, 2016)", "href": "#h.plqv88khmyxu"}, {"Relevant_ref": "Varieties of Fame in Psychology (Roediger III, 2016)", "href": "#h.ubwvvcnua9qq"}, {"Relevant_ref": "Scientific Eminence: Where Are the Women? (Eagly & Miller, 2016)", "href": "#h.fztee3dibdn9"}, {"Relevant_ref": "Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016)", "href": "#h.q2na8s8kahet"}]}, "summary_117": {"Title": "Let\u2019s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017) \u25c8 \u233a", "Id": "h.lr9irendycbv", "Main_Takeaways": ["Why do we care about judging scientific merit? There is some need to have a system to determine whether to award tenure and promotion to faculty members, leading to development of criteria and judge and measure the scholarly merit of individuals.", "Science is a collective enterprise whose goal is to explain and understand the natural world and to build knowledge.", "Science cares about advancements and discoveries, not about individuals.", "Individual scientists are valued to the extent further goals of the collective system.", "Science comprises lab workers, scientists, institutions, agencies and broader society is embedded.", "The supply of researchers-number, age and creativity, and organisation of research programs.", "At organisation level, features facilitate scientific discovery-organisational autonomy, organisational flexibility, moderate scientific diversity and frequent and intense interaction among scientists with different viewpoints.", "An individual scientist contributed to scientific discovery through their own scientific products or indirectly by positively affecting other aspects of the system.", "More senior graduate students train incoming graduate students-good at this role and output of the entire lab skyrocket as a result.", "Graduate students not only conduct their own personal research but presence in the lab facilitates scientific progress of others.", "Scientists promote productivity of other scientists by reviewing manuscripts, sharing data, creating and serving scientific organisations, developing scientific tools and paradigms used by others.", "Individual research scientists do not have resources to create large research centres, organise conferences and symposia, create and contribute to scientific discussion platforms and make their research protocols and data easily shareable."], "Abstract": "When judging scientific merit, the traditional method has been to use measures that assess the quality and/or quantity of an individual\u2019s research program. In today\u2019s academic world, a meritorious scholar is one who publishes high quality work that is frequently cited, who receives plentiful funding and scientific awards, and who is well regarded among his or her peers. In other words, merit is defined by how successful the scholar has been in terms of promoting his or her own career. In this commentary, I argue that there has been an overemphasis on measuring individual career outcomes and that we should be more concerned with the effect that scholars have on the scientific system in which they are embedded. Put simply, the question we should be asking is whether and to what extent a scholar has advanced the scientific discipline and moved the field forward collectively.", "Reference": "Pickett, C. (2017). Let's Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit. https://psyarxiv.com/tv6nb/", "You_may_also_be_interested_in": [{"Relevant_ref": "The Focus on Fame distorts Science (Innes-Ker, 2017)", "href": "#h.fiwxl1sbqyrw"}, {"Relevant_ref": "Fame: I\u2019m Skeptical (Ferreira, 2017)", "href": "#h.vkb9jz1fj01l"}, {"Relevant_ref": "\u201cFame\u201d is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017)", "href": "#h.4h5yr3mehhqy"}, {"Relevant_ref": "Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017)", "href": "#h.1g4r1nf76afn"}, {"Relevant_ref": "Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016)", "href": "#h.6y2j1ai8fhzp"}, {"Relevant_ref": "Against Eminence (Vazire, 2017)", "href": "#h.ixq69pvnhnnt"}, {"Relevant_ref": "Giving Credit Where Credit\u2019s Due: Why It\u2019s So Hard to Do in Psychological Science (Simonton, 2016)", "href": "#h.gqzauu6c437o"}, {"Relevant_ref": "Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016)", "href": "#h.hx1ixz9w9khb"}, {"Relevant_ref": "Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016)", "href": "#h.xxdqanhbam8s"}, {"Relevant_ref": "Improving Departments of Psychology (Diener, 2016)", "href": "#h.plqv88khmyxu"}, {"Relevant_ref": "Varieties of Fame in Psychology (Roediger III, 2016)", "href": "#h.ubwvvcnua9qq"}, {"Relevant_ref": "Scientific Eminence: Where Are the Women? (Eagly & Miller, 2016)", "href": "#h.fztee3dibdn9"}, {"Relevant_ref": "Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016)", "href": "#h.q2na8s8kahet"}]}, "summary_118": {"Title": "\u201cFame\u201d is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017)\u25c8 \u233a", "Id": "h.4h5yr3mehhqy", "Main_Takeaways": ["Fame is about visibility \u2013 who is seen. Ample evidence documents the influence of heuristics in determining who is visible, and whose contribution is considered important.", "Explicit and implicit beliefs about competence influence peer review when methodological quality or potential impact is ambiguous.", "The author is sceptical about the extent fame is shaped by the quality of one\u2019s work instead of confidence, dominance, persistence and demographics.", "Pace of academic life accelerates, pressure to depend on shortcuts in gatekeeping and evaluation continue to grow.", "We cannot remove implicit biases, there are ways to deflect impact.", "Reviews of submitted work should be blind to identity and demographics, letting the quality of the product stand on its own.", "We specify criteria for good science flexibly but explicitly and in detail.", "This includes: thorough and accurate contextualisation in relevant previous work, methodological rigour; innovation and problem solving and implications for theory, future research and/or intervention.", "We should insist on diversity in career stage, gender, ethnicity and perspective instead of inviting first people who come to mind.", "We can resist temptation to track women and minorities into high profile, high-demand services roles, thinking that this solves problems of diversity in science. We should ask what we should value and reward instead."], "Abstract": "To be famous is to be widely known, and honored for one\u2019s achievements. The process by which researchers achieve fame or eminence is skewed by heuristics that influence visibility; implications of these heuristics are magnified by a snowball effect, in which current fame leads to bias in ostensibly objective metrics of merit, including the distribution of resources that support future excellence. This effect may disproportionately hurt women and minorities, who struggle with both external and internalized implicit biases regarding competence and worth. While some solutions to this problem are available, they will not address the deeper problems of defining what it means for research to \u201cmake a difference\u201d in our field and in society, and consistently holding our work to that criterion.", "Reference": "Shiota, M. N. (2017) \u201cFame\u201d is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science. https://psyarxiv.com/4kwuq", "You_may_also_be_interested_in": [{"Relevant_ref": "The Focus on Fame distorts Science (Innes-Ker, 2017)", "href": "#h.fiwxl1sbqyrw"}, {"Relevant_ref": "Fame: I\u2019m Skeptical (Ferreira, 2017)", "href": "#h.vkb9jz1fj01l"}, {"Relevant_ref": "Let\u2019s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017)", "href": "#h.lr9irendycbv"}, {"Relevant_ref": "Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017)", "href": "#h.1g4r1nf76afn"}, {"Relevant_ref": "Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016)", "href": "#h.6y2j1ai8fhzp"}, {"Relevant_ref": "Against Eminence (Vazire, 2017)", "href": "#h.ixq69pvnhnnt"}, {"Relevant_ref": "Giving Credit Where Credit\u2019s Due: Why It\u2019s So Hard to Do in Psychological Science (Simonton, 2016)", "href": "#h.gqzauu6c437o"}, {"Relevant_ref": "Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016)", "href": "#h.hx1ixz9w9khb"}, {"Relevant_ref": "Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016)", "href": "#h.xxdqanhbam8s"}, {"Relevant_ref": "Improving Departments of Psychology (Diener, 2016)", "href": "#h.plqv88khmyxu"}, {"Relevant_ref": "Varieties of Fame in Psychology (Roediger III, 2016)", "href": "#h.ubwvvcnua9qq"}, {"Relevant_ref": "Scientific Eminence: Where Are the Women? (Eagly & Miller, 2016)", "href": "#h.fztee3dibdn9"}, {"Relevant_ref": "Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016)", "href": "#h.q2na8s8kahet"}]}, "summary_119": {"Title": "Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017) \u25c8 \u233a", "Id": "h.1g4r1nf76afn", "Main_Takeaways": ["What makes a science of knowing special?", "Why do we accord knowledge derived from scientific method a privileged position relative to common sense, appeals to authority figures, or other forms of rhetoric?", "If scientists depend on their own expertise as justification to prioritise their claims, we are not better to make truth-claims than religious, political and other leaders.", "Universalism means scientists reject claims of special authority, it matters far less who did the research than how it was done.", "How do we square scientific ideals with scientific culture that fetishizes lone scientific genius?", "Working scientists know this is not an accurate job depiction.", "We need to recognise the methods used to produce a scientific claim are more important than eminence of a person who produced it.", "Focusing primarily on the individual researcher excellence hurts psychological science, as eminence reflects values that are counterproductive to maximise scientific knowledge.", "Current system privileges quantity over quality, outcome of research instead of the process itself.", "Systematic biases affect how we identify who qualifies as eminent under status quo.", "Gender, nationality, race or institution should not matter to measure research quality.", "Structural changes should be initiated to help researchers reward and evaluate quality research.", "We can do a much better job to recognise and reward many activities for researchers that support scientific discovery beyond publishing peer reviewed articles (e.g. develop scientific software, generate large datasets, write data analytic code and construct tutorials to teach others to use it).", "We need to re-evaluate ways to measure researchers\u2019 excellence in light of value and promise of team-driven research.", "Science is a communal endeavour-scientific community that works together to understand the natural and social world.", "To combat structural and systematic problems linked to recognising eminence, double blind peer reviews were seen as standard practice for journal publication, grant funding and awards committee.", "Technological solutions are developed to allow departments to blind in at an early stage of faculty hiring."], "Abstract": "The scientific method has been used to eradicate polio, send humans to the moon, and enrich understanding of human cognition and behavior. It produced these accomplishments not through magic or appeals to authority, but through open, detailed, and reproducible methods. To call something \u201cscience\u201d means there are clear ways to independently and empirically evaluate research claims. There is no need to simply trust an information source. Scientific values thus prioritize transparency and universalism, emphasizing that it matters less who has made a discovery than how it was done. Yet, scientific reward systems are based on identifying individual eminence. The current paper contrasts this focus on individual eminence with reforms to scientific rewards systems that help these systems better align with scientific values.", "Reference": "Corker, K. S. (2017). Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values. https://psyarxiv.com/yqfrd", "You_may_also_be_interested_in": [{"Relevant_ref": "The Focus on Fame distorts Science (Innes-Ker, 2017)", "href": "#h.fiwxl1sbqyrw"}, {"Relevant_ref": "Fame: I\u2019m Skeptical (Ferreira, 2017)", "href": "#h.vkb9jz1fj01l"}, {"Relevant_ref": "Let\u2019s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017)", "href": "#h.lr9irendycbv"}, {"Relevant_ref": "\u201cFame\u201d is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017)", "href": "#h.4h5yr3mehhqy"}, {"Relevant_ref": "Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016)", "href": "#h.6y2j1ai8fhzp"}, {"Relevant_ref": "Against Eminence (Vazire, 2017)", "href": "#h.ixq69pvnhnnt"}, {"Relevant_ref": "Giving Credit Where Credit\u2019s Due: Why It\u2019s So Hard to Do in Psychological Science (Simonton, 2016)", "href": "#h.gqzauu6c437o"}, {"Relevant_ref": "Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016)", "href": "#h.hx1ixz9w9khb"}, {"Relevant_ref": "Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016)", "href": "#h.xxdqanhbam8s"}, {"Relevant_ref": "Improving Departments of Psychology (Diener, 2016)", "href": "#h.plqv88khmyxu"}, {"Relevant_ref": "Varieties of Fame in Psychology (Roediger III, 2016)", "href": "#h.ubwvvcnua9qq"}, {"Relevant_ref": "Scientific Eminence: Where Are the Women? (Eagly & Miller, 2016)", "href": "#h.fztee3dibdn9"}, {"Relevant_ref": "Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016)", "href": "#h.q2na8s8kahet"}]}, "summary_120": {"Title": "Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) \u233a", "Id": "h.fbcbbt5lpe28", "Main_Takeaways": ["COVID-19 pandemic disrupted scientific enterprise.", "Policymakers and institutional leaders respond to reduce influences of pandemic on researchers. They reached US- and Europe-based scientists across institutions, career stages and demographic backgrounds.", "The present paper solicited information about working hours and how time allocations changed since the onset of pandemic and asked scientists to report the range of individual and family properties, as these feature moderate effects of pandemic.", "They found a decline in total working hours with average dropping from 61 pre-pandemic to 54 hours at time of survey.", "5% of scientists report they worked 42 hours or less before the pandemic and share increased sixfold to 30% during the pandemic.", "Scientists perform many different types of work: research, fundraising and teaching and other tasks.", "Time devoted to research has changed most during pandemic. Total working hours decreased by 11% on average, but research declined by 24%.", "Scientists working in fields rely on physical laboratories and time sensitive experiments report largest declines in research time in the range of 30-40% below pre-pandemic levels.", "Fields that are less equipment intensive report lowest declines in research time. The difference can be as large as fourfold.", "There are differences between male and female respondents in how pandemic influenced their work.", "Female scientists and scientists with young dependent report ability to devote time to their research have been influenced and effects are additive, most impact was for female scientists with young dependents.", "Researchers best explain changes in time devoted to research during pandemic.", "Career stage and facility closures contributed no role in changes to time allocated to research when everything else is held constant, gender and young dependents contributed major roles.", "Female scientists reported 5% larger decline in research time but scientists with at least one child 5 years old or young experienced 17% larger decline in research time.", "Multiple dependents was linked to further 3% reduction in time spent on research, the same was observed for 6-11 years.", "This indicates gender discrepancy can be due to female scientists more likely to have young children as dependents. Scientists who feel strongly about sharing situations, whether they experience large positive or negative changes, apply to US and Europe-based academic researchers.", "There are disparities in how pandemic influences the scientific workforce. Pandemic influences members of the scientific community differently.", "Shelter at home is not the same as work from home, when dependents are also at home and need care.", "Unless adequate childcare services are available, researchers with young children continue to be affected irrespective of reopening plans of institutions.", "The need to care for dependents is not unique to the scientific workforce.", "Pandemic will likely have longer-term impacts that are important to monitor and address disparities and further efforts to track effects of pandemic on the scientific workforce need to consider household circumstances.", "These uniform policies do not consider individual circumstances, while welcoming may have unintended consequences and worsen pre-existing inequalities.", "The disparities may worsen, as institutions begin the process of reopening as different priorities for bench sciences versus work with human subjects or field-work travel may lead to new disparities across scientists.", "Funders seeking to support high-impact programs adopt a similar approach, favouring proposals that are more resilient to uncertain future scenarios.", "Senior researchers have incentives to avoid in-person interactions facilitating mentoring and hands-on training of junior researchers.", "Impact of changes on individuals and groups of scientists could be large in short- and long-term, worsening negative impacts among those at a disadvantage.", "We need to consider consequences of policies adopted to respond to pandemic, as they may disadvantage under-represented minorities and worsen existing disparities."], "Quote": "\u201cThe disparities we observe and the likely surfacing of new impacts in the coming months and years argue for targeted and nuanced approaches as the world-wide research enterprise rebuilds.\u201d (p.882)", "Abstract": "COVID-19 has not affected all scientists equally. A survey of principal investigators indicates that female scientists, those in the \u2018bench sciences\u2019 and, especially, scientists with young children experienced a substantial decline in time devoted to research. This could have important short- and longer-term effects on their careers, which institution leaders and funders need to address carefully.", "Reference": "Myers, K. R., Tham, W. Y., Yin, Y., Cohodes, N., Thursby, J. G., Thursby, M. C., ... & Wang, D. (2020). Unequal effects of the COVID-19 pandemic on scientists. Nature Human Behaviour, 4, 880-883. https://doi.org/10.1038/s41562-020-0921-y", "You_may_also_be_interested_in": [{"Relevant_ref": "The Gender Gap: Who Is (and Is Not) Included on Graduate-Level Syllabi in Social/Personality Psychology (Skitka et al., 2020)", "href": "#h.j2lw9gk12js0"}, {"Relevant_ref": "Leveraging a collaborative consortium model of mentee/mentor training to foster career progression of under-represented post-doctoral researchers and promote institutional diversity and inclusion (Risner et al., 2020) \u233a", "href": "#h.um3ggunxziz6"}, {"Relevant_ref": "A cry for help (Nature, 2019)", "href": "#h.v7kmozf9h8f7"}, {"Relevant_ref": "Postdocs in crisis: science risks losing the next generation (Nature, 2020)", "href": "#h.68x0qk7jsz9j"}, {"Relevant_ref": "Boosting research without supporting universities is wrong-headed \u00a0(Nature, 2020b)", "href": "#h.jt3augsu7xw8"}, {"Relevant_ref": "Against Eminence (Vazire, 2017)", "href": "#h.ixq69pvnhnnt"}]}, "summary_121": {"Title": "Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016)", "Id": "h.6y2j1ai8fhzp", "Main_Takeaways": ["We go to get a PhD, time of hiring, time of reappointment, time of tenure and time of promotion to full professor. All are stressful times, we wonder whether criteria is fair and applied fairly.", "Quality, productivity, visibility and impact matter in departmental judgment of scientific merit.", "But evaluations are highly subjective. Letters from distinguished referees provide useful qualitative evaluations of candidates.", "They go beyond mere numbers why it is or not important and the referee's assessment of future potential.", "Candidates who are working in faddish areas and have larger networks-more likely to attend prestigious graduate school, prestigious post-doc or well-known adviser at an advantage.", "Letter writers are prone to fads, biases and personal idiosyncrasies anyone of us possess.", "Quantity of publication is a reasonable measure of productivity.", "Most articles get published somewhere if one does not worry about the prestige of the journal. One can control for impact factor-how much on average articles are cited.", "Number of citations-applies to individuals over a career and not a journal for which most of impact data come from people other than the candidate. H index is the number of publications cited at least h times. I10 is the number of publications cited at least 10 times.", "Grants and contracts show scholars have systematically and valued proposed programs of research.", "Editorship shows scholar\u2019s work is recognised in their field. Invited service on a grant panel is another recognition of success in one\u2019s professional endeavours.", "Awards are a useful measure of recognition by peers and measure quality of work instead of citation to work.", "Honorary doctorates are recognitions by broader academic audiences of merit of a scholar\u2019s work.", "Some would say shift represents a natural progression as the field becomes more and more of a natural science.", "The big thinkers of yesterday might be taken aback by the amount of work done in modern times.", "The use of neuroimaging, behavioural experiments importance shrinks towards small-scale psychology without theory but with a large theory, they contribute to larger theory. Big thinking pays off."], "Abstract": "The purpose of this symposium is to consider new ways of judging merit in academia, especially with respect to research in psychological science. First, I discuss the importance of merit-based evaluation and the purpose of this symposium. Next, I review some previous ideas about judging merit\u2014especially creative merit\u2014and I describe some of the main criteria used by institutions today for judging the quality of research in psychological science. Finally, I suggest a new criterion that institutions and individuals might use and draw some conclusions.", "Reference": "Sternberg, R. J. (2016). \u201cAm I famous yet?\u201d Judging scholarly merit in psychological science: An introduction. Perspectives on Psychological Science, 11(6), 877-881. https://doi.org/10.1177/1745691616661777", "You_may_also_be_interested_in": [{"Relevant_ref": "The Focus on Fame distorts Science (Innes-Ker, 2017)", "href": "#h.fiwxl1sbqyrw"}, {"Relevant_ref": "Fame: I\u2019m Skeptical (Ferreira, 2017)", "href": "#h.vkb9jz1fj01l"}, {"Relevant_ref": "Let\u2019s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017)", "href": "#h.lr9irendycbv"}, {"Relevant_ref": "\u201cFame\u201d is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017)", "href": "#h.4h5yr3mehhqy"}, {"Relevant_ref": "Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017)", "href": "#h.1g4r1nf76afn"}, {"Relevant_ref": "Against Eminence (Vazire, 2017)", "href": "#h.ixq69pvnhnnt"}, {"Relevant_ref": "Giving Credit Where Credit\u2019s Due: Why It\u2019s So Hard to Do in Psychological Science (Simonton, 2016)", "href": "#h.gqzauu6c437o"}, {"Relevant_ref": "Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016)", "href": "#h.hx1ixz9w9khb"}, {"Relevant_ref": "Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016)", "href": "#h.xxdqanhbam8s"}, {"Relevant_ref": "Improving Departments of Psychology (Diener, 2016)", "href": "#h.plqv88khmyxu"}, {"Relevant_ref": "Varieties of Fame in Psychology (Roediger III, 2016)", "href": "#h.ubwvvcnua9qq"}, {"Relevant_ref": "Scientific Eminence: Where Are the Women? (Eagly & Miller, 2016)", "href": "#h.fztee3dibdn9"}, {"Relevant_ref": "Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016)", "href": "#h.q2na8s8kahet"}]}, "summary_122": {"Title": "Against Eminence (Vazire, 2017) \u25c8 \u233a", "Id": "h.ixq69pvnhnnt", "Main_Takeaways": ["The kind of change we are seeing now is more profound than typical scientific development. Transparency makes it possible for scientists to discriminate robust from shaky findings.", "The Replicability crisis shows a system without transparency does not work.", "How did we get here? Who let this happen? Those in charge of setting scientific norms and standards should strive to increase transparency, bolster our confidence that we trust published research.", "Many high level decisions in science are made with a different goal in mind to increase impact.", "Professional societies and journals prioritise publishing attention-grabbing findings to boost visibility and prestige.", "Seeking eminence is at odds with scientific value and affects scientific gatekeepers\u2019 decisions.", "Editors influenced by the status of submitting authors or prestige of institutions violate the basic premise of science. Science work should be evaluated on its own merit, irrespective of the source.", "Excluding status bias out of scientific evaluation is not easy.", "Lack of transparency in science is a direct consequence of corrupting the influence of eminence seeking.", "If journals and societies are motivated by boosting impact and publish sexiest findings by most famous authors.", "There is tension between eminence and rigour at all levels of science.", "Gatekeepers as they control incentive structures that shape individual researchers\u2019 behaviour and uphold scientific values and most power to erode those values.", "Individual researchers\u2019 desire for eminence threatens the integrity of the research process and keeps it in check.", "All researchers are human and desire recognition for their work and a bleak existence if we removed this source of motivation for fame and status.", "There is no good reason to amplify human drive and encourage scientists to seek fame.", "Scientists struggle with tension between human desire and scientific ideals.", "Why make it harder for them to give human desires an even stronger platform.", "Glorification of eminence is that it reinforces inequalities in science.", "If scientists are evaluated based on ability to attract attention, those with the most prestige will be heard the loudest. Certain groups are over-represented at a high level of status.", "Eminence propagates privilege and raises barriers to entry for others.", "How should scientific merit be evaluated? What does this mean for committees to select one or few winners?", "Admit that a larger number of scientists meet the objective criteria for these recognitions-do sound science.", "Admit selection of one or few individuals from this group is not based on merit but on preference or partiality.", "It is fine for groups to select or recognise members who exemplify their values but not be confused with exceptional scientific merit.", "Whenever possible-for tenure, promotion and when journal space or grant fund permits, reward scientists whose work reaches a more objective threshold of scientific rigour or soundness instead of selecting cream of the crop."], "Abstract": "The drive for eminence is inherently at odds with scientific values, and insufficient attention to this problem is partly responsible for the recent crisis of confidence in psychology and other sciences. The replicability crisis has shown that a system without transparency doesn\u2019t work. The lack of transparency in science is a direct consequence of the corrupting influence of eminence-seeking. If journals and societies are primarily motivated by boosting their impact, their most effective strategy will be to publish the sexiest findings by the most famous authors. Humans will always care about eminence. Scientific institutions and gatekeepers should be a bulwark against the corrupting influence of the drive for eminence, and help researchers maintain integrity and uphold scientific values in the face of internal and external pressures to compromise. One implication for evaluating scientific merit is that gatekeepers should attempt to reward all scientists whose work reaches a more objective threshold of scientific rigor or soundness, rather than attempting to select the cream of the crop (i.e., identify the most \u201ceminent\u201d).", "Reference": "Vazire, S. (2017). Against eminence. https://doi.org/10.31234/osf.io/djbcw", "You_may_also_be_interested_in": [{"Relevant_ref": "The Gender Gap: Who Is (and Is Not) Included on Graduate-Level Syllabi in Social/Personality Psychology (Skitka et al., 2020)", "href": "#h.j2lw9gk12js0"}, {"Relevant_ref": "The Focus on Fame distorts Science (Innes-Ker, 2017)", "href": "#h.fiwxl1sbqyrw"}, {"Relevant_ref": "Fame: I\u2019m Skeptical (Ferreira, 2017)", "href": "#h.vkb9jz1fj01l"}, {"Relevant_ref": "Let\u2019s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017)", "href": "#h.lr9irendycbv"}, {"Relevant_ref": "\u201cFame\u201d is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017)", "href": "#h.4h5yr3mehhqy"}, {"Relevant_ref": "Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017)", "href": "#h.1g4r1nf76afn"}, {"Relevant_ref": "Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016)", "href": "#h.6y2j1ai8fhzp"}, {"Relevant_ref": "Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019)", "href": "#h.fbcbbt5lpe28"}, {"Relevant_ref": "Giving Credit Where Credit\u2019s Due: Why It\u2019s So Hard to Do in Psychological Science (Simonton, 2016)", "href": "#h.gqzauu6c437o"}, {"Relevant_ref": "Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016)", "href": "#h.hx1ixz9w9khb"}, {"Relevant_ref": "Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016)", "href": "#h.xxdqanhbam8s"}, {"Relevant_ref": "Improving Departments of Psychology (Diener, 2016)", "href": "#h.plqv88khmyxu"}, {"Relevant_ref": "Varieties of Fame in Psychology (Roediger III, 2016)", "href": "#h.ubwvvcnua9qq"}, {"Relevant_ref": "Scientific Eminence: Where Are the Women? (Eagly & Miller, 2016)", "href": "#h.fztee3dibdn9"}, {"Relevant_ref": "Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016)", "href": "#h.q2na8s8kahet"}]}, "summary_123": {"Title": "Giving Credit Where Credit\u2019s Due: Why It\u2019s So Hard to Do in Psychological Science (Simonton, 2016)", "Id": "h.gqzauu6c437o", "Main_Takeaways": ["Eminence in any scientific disciplines will be directly proportional to actual contributions.", "Scientists make inferences based on empirical fact and logical reasoning.", "Not only would peer assessments prove highly objective but scientists' self-assessments of their own contributions should depart little from colleagues in the best position to evaluate their work.", "One specific manifestation of this consensus would appear in awards and honours bestowed on those scientists who have devoted a whole career to produce high-impact work.", "Lifetime career awards-scientist\u2019s cumulative record span decades should be more reliable than early career awards, founded on a much smaller and less representative sample of their entire career.", "Best article recognition is lower than the reliability of any career award. Best article awards may not predict citations that the article later receives. It makes most sense to concentrate on assessment of lifetime contributions to psychological science.", "Mass producers who are high in output but low in impact are rare, perfectionists who are low in output but high in impact. Mass producers and perfectionists exist but represent exceptions to general pattern-outliers in scatter plot.", "Invitation to write a definitive handbook chapter is a sure sign-a scientist is a widely recognised expert on an important topic.", "Scientific impact reliably assessed by simple count of total citations, impact measured by several alternative citation measures.", "Psychologists receive contradictory reviews-one might recommend minor revisions but another support reject. Episodes are not only frustrating to the author but discomfiting for the editor and prove discipline lacks a strong consensus on what counts as a contribution to science.", "Lack of agreement in peer evaluations of single papers should be ameliorated if evaluators can operate with a larger sample of contributions that hold lifetime career awards. Psychologists argue that disciplinary deficits compared to physical and biological sciences might be rectified so that our field boasts a stronger consensus."], "Abstract": "More than a century of scientific research has shed considerable light on how a scientist\u2019s contributions to psychological science might be best assessed and duly recognized. This brief overview of that empirical evidence concentrates on recognition for lifetime career achievements in psychological science. After discussing both productivity and citation indicators, the treatment turns to critical precautions in the application of these indicators to psychologists. These issues concern both predictive validity and interjudge reliability. In the former case, not only are the predictive validities for standard indicators relatively small, but the indicators can exhibit important non-merit-based biases that undermine validity. In the latter case, peer consensus in the evaluation of scientific contributions is appreciably lower in psychology than in the natural sciences, a fact that has consequences for citation measures as well. Psychologists must therefore exercise considerable care in judging achievements in psychological science\u2014both their own and those of others.", "Reference": "Simonton, D. K. (2016). Giving credit where credit\u2019s due: Why it\u2019s so hard to do in psychological science. Perspectives on Psychological Science, 11(6), 888-892. https://doi.org/10.1177/1745691616660155", "You_may_also_be_interested_in": [{"Relevant_ref": "The Focus on Fame distorts Science (Innes-Ker, 2017)", "href": "#h.fiwxl1sbqyrw"}, {"Relevant_ref": "Fame: I\u2019m Skeptical (Ferreira, 2017)", "href": "#h.vkb9jz1fj01l"}, {"Relevant_ref": "Let\u2019s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017)", "href": "#h.lr9irendycbv"}, {"Relevant_ref": "\u201cFame\u201d is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017)", "href": "#h.4h5yr3mehhqy"}, {"Relevant_ref": "Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017)", "href": "#h.1g4r1nf76afn"}, {"Relevant_ref": "Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016)", "href": "#h.6y2j1ai8fhzp"}, {"Relevant_ref": "Against Eminence (Vazire, 2017)", "href": "#h.ixq69pvnhnnt"}, {"Relevant_ref": "Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016)", "href": "#h.hx1ixz9w9khb"}, {"Relevant_ref": "Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016)", "href": "#h.xxdqanhbam8s"}, {"Relevant_ref": "Improving Departments of Psychology (Diener, 2016)", "href": "#h.plqv88khmyxu"}, {"Relevant_ref": "Varieties of Fame in Psychology (Roediger III, 2016)", "href": "#h.ubwvvcnua9qq"}, {"Relevant_ref": "Scientific Eminence: Where Are the Women? (Eagly & Miller, 2016)", "href": "#h.fztee3dibdn9"}, {"Relevant_ref": "Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016)", "href": "#h.q2na8s8kahet"}]}, "summary_124": {"Title": "Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016)", "Id": "h.hx1ixz9w9khb", "Main_Takeaways": ["Deep eminence is the omniscient self knows and future generations will know the candidate is doing work moving some part of the discipline toward \u201ccapital-T Truth\u201d.", "We must assess whether the candidate is moving or has a good chance to move the subfield forward based on a small sample.", "Surface eminence is the basis for our mere-mortal judgment of tenurability-we take solace what else can you scratch but the surface. The teaching assignments in research universities are lower than they were in the days when modern experimental psychology took off.", "It is not unusual to see less productive scholars with teaching assignments involve larger sections of undergraduates and other service activities. It is still possible for successful grantees to buy out some of their teaching time.", "A university is to develop human capital of society and keep faith with funders of universities, we should be leery of allowing mission to slip too low in our goal hierarchy.", "Citation data predicts early prominence at least at extremes of citation distribution.", "There will be a problem with longitudinal studies-if a person does not get tenure, likelihood next job is at an institution with fewer resources in support of research. Some publications-impact on me are not driven by quantity but quality of the work.", "During the interview, it is asked what is the big problem you care about? Six years from now what will you be known for as your contribution to working on that problem. People who have answers to that question have their eye on the big picture, will have their work cited and have a good shot to be on the path to eminence."], "Abstract": "In this article, I review, comment upon, and assess some of the suggestions for evaluating scientific merit as suggested by contributors to this symposium. I ask the reader to take the perspective of the individual who has the final say in making a tenure, promotion, or hiring decision. I also ask that one imagine the difference between the fallible human state we are in on such an occasion and what it would be like to be omniscient when making such decisions. After adopting the terminology of \u201cdeep\u201d and \u201csurface\u201d eminence, I consider what an omniscient being would take into account to determine eminence and to guide decision-making. After discussing how some proposed improvements in assessing merit might move us closer to wise decisions, I conclude by noting that both data and judgment are, and will continue to be, necessary. A clerk cannot determine eminence.", "Reference": "Foss, D. J. (2016). Eminence and omniscience: Statistical and clinical prediction of merit. Perspectives on Psychological Science, 11(6), 913-916. https://doi.org/10.1177/1745691616662440", "You_may_also_be_interested_in": [{"Relevant_ref": "The Focus on Fame distorts Science (Innes-Ker, 2017)", "href": "#h.fiwxl1sbqyrw"}, {"Relevant_ref": "Fame: I\u2019m Skeptical (Ferreira, 2017)", "href": "#h.vkb9jz1fj01l"}, {"Relevant_ref": "Let\u2019s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017)", "href": "#h.lr9irendycbv"}, {"Relevant_ref": "\u201cFame\u201d is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017)", "href": "#h.4h5yr3mehhqy"}, {"Relevant_ref": "Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017)", "href": "#h.1g4r1nf76afn"}, {"Relevant_ref": "Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016)", "href": "#h.6y2j1ai8fhzp"}, {"Relevant_ref": "Against Eminence (Vazire, 2017)", "href": "#h.ixq69pvnhnnt"}, {"Relevant_ref": "Giving Credit Where Credit\u2019s Due: Why It\u2019s So Hard to Do in Psychological Science (Simonton, 2016)", "href": "#h.gqzauu6c437o"}, {"Relevant_ref": "Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016)", "href": "#h.xxdqanhbam8s"}, {"Relevant_ref": "Improving Departments of Psychology (Diener, 2016)", "href": "#h.plqv88khmyxu"}, {"Relevant_ref": "Varieties of Fame in Psychology (Roediger III, 2016)", "href": "#h.ubwvvcnua9qq"}, {"Relevant_ref": "Scientific Eminence: Where Are the Women? (Eagly & Miller, 2016)", "href": "#h.fztee3dibdn9"}, {"Relevant_ref": "Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016)", "href": "#h.q2na8s8kahet"}]}, "summary_125": {"Title": "Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016)", "Id": "h.xxdqanhbam8s", "Main_Takeaways": ["Hiring, tenure, promotion, research funding and honours are based on scientific impact measured via citation measures.", "Citations offer advantages, as it tracks the influence of articles in a more fine-grained manner than coarse measures like impact factor.", "One highly cited publication is impressive but less so than sustained production of many influential works.", "Many citation measures are robust to outliers and easy to calculate with h index. Also, it is easy to understand and robust to outliers or missing data.", "H index measures more than productivity because scholars earn recognition through citations-transparent, reproducible, and objective measures of scholarly impact. This reduces many kinds of bias that influence judgments and decisions.", "H index provides promise for the assessment of scholarly impact. Several factors affect h or its interpretation must be carefully considered.", "An individual\u2019s h index depends on what database is used to retrieve citations, when, and how effectively the search is performed.", "Advantages include free access, ease of use, speed and comprehensive coverage.", "Google scholar is automated processing that can misidentify authors and provide messy data. H index takes integer values, so tied scores will be common.", "It is important not to make too much of a small difference on any measures and accept important decisions will be made in the margin of error.", "There is concern about awarding credit for self-citations, as self-citations are strongly related to total citations.", "Self-citation is done strategically and needs impressive foresight about which scholarly works occupy positions in the long run.", "Publications by citations array evolves in unpredictable ways. There are similar concerns about the merits of awarding full credit for citations of works with shared authorships.", "Citations accumulate over time, individuals at later career stages would be predicted to have larger h indices.", "One can control for career stage by dividing h by number of years since first publication. H involves differences across scientific disciplines or subdisciplines or specialty areas.", "The number of people working in an area, extent to which that work reaches beyond specialists, size of research teams and typical number of references for publications in a certain field of study. Comparing individuals who work in different areas can be unfair.", "H index does not involve all relevant information and will not solve all decision-making dilemmas-too much to ask of any measure.", "H index is transparent, objective, and based on wealth of information usefully augments what can be gleaned from strictly subjective review of a scholarly record.", "Familiarity with and use of h index accelerates rapidly. Knowing h index will be evaluated one or more times during career and have implications for how to go about your work. \u00a0Aim for sustained production of influential work.", "We leave behind days of mindlessly counting the number of publications on CV. \u00a0In early stages of an academic career, it may remain tempting to publish as much as possible, but as the career unfolds you will have the luxury of caring as much about impact about productivity.", "H index helps to assess success with which both are attained and its use might help to steer scholars toward a more laudable goal of scientific impact.", "H index quantifies scholarly impact and identifies academic eminence but cannot tell you how heavy an emphasis you should place on its pursuit."], "Abstract": "Professional decisions about hiring, tenure, promotion, funding, and honors are informed by assessments of scholarly impact. As a measure of influence, citations are produced by experts but accessible to nonexperts. The h index is the largest number h such that an individual has published at least h works cited at least h times apiece. This is easy to understand and calculate, as or more reliable and valid than alternative citation measures, and highly robust to missing or messy data. Striving for a large h index requires both productivity and influence, which provides healthy incentives for researchers striving for eminence through scientific impact. A number of factors that can influence h are discussed to promote the mindful use of what might otherwise be an ambiguous or misleading measure. The h index adds a transparent, objective component to assessments of scholarly impact, and even academic eminence, that merits at least two cheers.", "Reference": "Ruscio, J. (2016). Taking advantage of citation measures of scholarly impact: Hip Hip h Index!. Perspectives on Psychological Science, 11(6), 905-908. https://doi.org/10.1177/1745691616664436", "You_may_also_be_interested_in": [{"Relevant_ref": "An index to quantify an individual\u2019s scientific research output (Hirsch, 2005)", "href": "#h.v4o766rkbx9r"}, {"Relevant_ref": "High Impact =High Statistical Standards? Not Necessarily So (Tressoldi et al., 2013)", "href": "#h.bt5o53my0bmr"}, {"Relevant_ref": "The Focus on Fame distorts Science (Innes-Ker, 2017)", "href": "#h.fiwxl1sbqyrw"}, {"Relevant_ref": "Fame: I\u2019m Skeptical (Ferreira, 2017)", "href": "#h.vkb9jz1fj01l"}, {"Relevant_ref": "Let\u2019s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017)", "href": "#h.lr9irendycbv"}, {"Relevant_ref": "\u201cFame\u201d is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017)", "href": "#h.4h5yr3mehhqy"}, {"Relevant_ref": "Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017)", "href": "#h.1g4r1nf76afn"}, {"Relevant_ref": "Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016)", "href": "#h.6y2j1ai8fhzp"}, {"Relevant_ref": "Against Eminence (Vazire, 2017)", "href": "#h.ixq69pvnhnnt"}, {"Relevant_ref": "Giving Credit Where Credit\u2019s Due: Why It\u2019s So Hard to Do in Psychological Science (Simonton, 2016)", "href": "#h.gqzauu6c437o"}, {"Relevant_ref": "Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016)", "href": "#h.hx1ixz9w9khb"}, {"Relevant_ref": "Improving Departments of Psychology (Diener, 2016)", "href": "#h.plqv88khmyxu"}, {"Relevant_ref": "Varieties of Fame in Psychology (Roediger III, 2016)", "href": "#h.ubwvvcnua9qq"}, {"Relevant_ref": "Scientific Eminence: Where Are the Women? (Eagly & Miller, 2016)", "href": "#h.fztee3dibdn9"}, {"Relevant_ref": "Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016)", "href": "#h.q2na8s8kahet"}]}, "summary_126": {"Title": "Improving Departments of Psychology (Diener, 2016)", "Id": "h.plqv88khmyxu", "Main_Takeaways": ["We have excellent universities, our selection-based approach to talent and productivity is incomplete for creating the very best departments.", "There is a drop in research productivity after people are granted tenure.", "Most articles are cited very little. What all organisation citizens should ask. What can things be improved?", "Current approach to excellence in scholarship rests largely on hiring the right individuals who have the right talent and motivation based on activities during graduate school and early careers.", "The judgments are cloudy due to confounding variables due to differences between mentors of young scholars. We know that our judgments are correct but off base.", "Many psychologists stress the importance of situations affecting behaviour but depend on personnel selection to achieve excellence in their organisation.", "Good selection is important, albeit hard to do well, immense energy and time is spent selecting professors and students, they are turned loose to do their own thing. There are limitations to depend on personnel selection.", "A candidate who ends up being a star at another institution or hires someone who looks promising but turns out to be disappointing.", "We trust our own human judgment, all the while warning people of the shortcomings of human judgment.", "These numbers from economics seem to generalise to psychology but spent much time hiring right people but do not carefully implement systems to ensure scholarly excellence.", "We need to coordinate selection with continuing education, performance appraisals, tracking of talent, and rewards and compensation for excellence.", "Most departments have forms of continuing education for faculty, but activities are unsystematic and not incentivised. Scholars attend conferences but see old friends.", "The talks attend conferences and colloquia focus on a single person\u2019s latest research and do not educate about advances in various areas of psychology.", "Few faculty members take advanced courses to further their knowledge and their skills in statistics grow very outmoded over time.", "Efforts at continuing education in terms of travel money to conferences, publication of annual reviews present updates of field, and departmental colloquia.", "Focus would not be on a single individual\u2019s research but instead on the most important advances in the area of psychology.", "All faculty would be predicted or needed to attend, not help but benefit scholarship and teaching.", "Once researchers are hired, they teach and research that is unsupervised.", "A large amount of the raise pool is devoted to standard raises for everyone and a chunk is given to those with competing offers.", "Friends in other departments and those willing to play competing offer games get the largest raises.", "Often only a small amount of the raise pool is awarded for merit.", "Other incentives are possible, such as reductions in committee work and reductions in teaching for an excellent research record.", "All departments use the model of professors to do everything.", "One must be a good researcher to be a good teacher and vice versa, and all faculty should do all activities.", "Professors are needed to perform university services.", "Specialisation of activities is a major force in advance of civilisation and no reason scholarship would not profit from it.", "Some professors love research and are extremely productive. Difference between research stars and other faculty grows huge.", "A small percentage of scientists are responsible for a vastly disproportionate number of publications and citations.", "Most productive scholars had lower teaching loads, albeit departments allowed for some course buyouts from teaching with grant money.", "Most productive scholars in each department teach much less and lowest professors in terms of scholarly output teach more, scholarly productivity and impact of department would increase dramatically, not help but raise overall scholarly output of departments.", "Some professors are spectacular teachers and some seem to thrive on service and enjoy it, while others loathe it.", "Why burden superstars in each domain with many activities from other domains? Few departments now have tenured teaching tracks, but relatively rare.", "Hiring new PhD\u2019s produces uncertain results, institutions hire more scholars at midcareer and senior levels.", "All organisations heavily emphasise hiring new PhDs, individuals who might be good at somewhat different tasks, as individuals will publish modestly and whose teaching and mentoring are adequate but not outstanding.", "If one wants to hire outstanding teachers or researchers, a department looks more frequently to hiring associates or full professors who have a proven track record independent graduate school mentors.", "Hiring across universities will move the most productive to institutions with most resources for research and this will benefit the field.", "There will be added incentives for better teaching and research if university psychologists move more easily.", "A helpful way to obtain a higher salary is to receive competing offers from other universities, this is available to only a small percentage of highly accomplished individuals working in popular fields.", "Hiring talented individuals from other institutions becomes an even greater source of talent for departments to attract them."], "Abstract": "Our procedures for creating excellent departments of psychology are based largely on selection\u2014hiring and promoting the best people. I argue that these procedures have been successful, but I suggest the implementation of policies that I believe will further improve departments in the behavioral and brain sciences. I recommend that we institute more faculty development programs attached to incentives to guarantee continuing education and scholarly activities after the Ph.D. degree. I also argue that we would do a much better job if we more strongly stream our faculty into research, education, or service and not expect all faculty members to carry equal responsibility for each of these. Finally, I argue that more hiring should occur at advanced levels, where scholars have a proven track record of independent scholarship. Although these practices will be a challenge to implement, institutions do ossify over time and thus searching for ways to improve our departments should be a key element of faculty governance.", "Reference": "Diener, E. (2016). Improving departments of psychology. Perspectives on Psychological Science, 11(6), 909-912. https://doi.org/10.1177/1745691616662865", "You_may_also_be_interested_in": [{"Relevant_ref": "The Focus on Fame distorts Science (Innes-Ker, 2017)", "href": "#h.fiwxl1sbqyrw"}, {"Relevant_ref": "Fame: I\u2019m Skeptical (Ferreira, 2017)", "href": "#h.vkb9jz1fj01l"}, {"Relevant_ref": "Let\u2019s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017)", "href": "#h.lr9irendycbv"}, {"Relevant_ref": "\u201cFame\u201d is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017)", "href": "#h.4h5yr3mehhqy"}, {"Relevant_ref": "Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017)", "href": "#h.1g4r1nf76afn"}, {"Relevant_ref": "Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016)", "href": "#h.6y2j1ai8fhzp"}, {"Relevant_ref": "Against Eminence (Vazire, 2017)", "href": "#h.ixq69pvnhnnt"}, {"Relevant_ref": "Giving Credit Where Credit\u2019s Due: Why It\u2019s So Hard to Do in Psychological Science (Simonton, 2016)", "href": "#h.gqzauu6c437o"}, {"Relevant_ref": "Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016)", "href": "#h.hx1ixz9w9khb"}, {"Relevant_ref": "Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016)", "href": "#h.xxdqanhbam8s"}, {"Relevant_ref": "Varieties of Fame in Psychology (Roediger III, 2016)", "href": "#h.ubwvvcnua9qq"}, {"Relevant_ref": "Scientific Eminence: Where Are the Women? (Eagly & Miller, 2016)", "href": "#h.fztee3dibdn9"}, {"Relevant_ref": "Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016)", "href": "#h.q2na8s8kahet"}]}, "summary_127": {"Title": "Varieties of Fame in Psychology (Roediger III, 2016)", "Id": "h.ubwvvcnua9qq", "Main_Takeaways": ["How do we determine the quality and impact of an individual and their research in psychological science?", "We use progression of fame to indicate how the system works. Researchers who go on to fame have the first step and publish other papers as graduate students.", "The curriculum vitae of successful job candidates look like people getting tenure at research universities 40 years ago.", "Once a researcher succeeds in graduate school, obtained a job in academia, industry or research institute, time to move with one\u2019s research career.", "Write integrative reviews, theoretical articles and book chapters that are read.", "Work needs to be cited, so you need to write well and report interesting findings.", "If you are doing well, you are asked to serve on editorial boards and grant panels.", "Attract good graduate students if you are in a research-intensive university.", "You are productive, your work is becoming more cited, you are being asked to serve in various ways and get grants.", "If you are asked to be an associate editor or editor of a journal, it is a high mark of respect in the discipline of psychology.", "How does one get known outside of one\u2019s primary field?", "How does one become a famous psychologist?", "The main criterion is to discover important truths about human behaviours and tell the story in an interesting and accessible way.", "Writing for a wide audience-writing articles that are accessible and assigned and taught in course.", "Some are gifted for writing in a technical audience but for the whole field and for the general public.", "High publication rates are a given, high citation rates indicate fame, few papers with remarkably high citation rates and receipts of psychology-wide awards and qualities are highly correlated.", "How does one achieve these criteria? Strong, exciting publications widely cited lead to awards and both lead to fame in these fields.", "Few psychologists achieve a state of being widely known outside psychology by the general public or highly educated public. Those who win the National Medal of Science are rarely known by the educated public.", "Many people receive information from few media outlets, more general fame for an academic could be possible.", "Intellectual life becomes increasingly fragmented, as we live in our small bubbles of information.", "Widespread fame is harder to come by outside of politics, sports and entertainment.", "Fame is more local than ever in academia, with the many fractionations of our fields. Being famous is a highly specialised business.", "How to attain eminence or fame-there are guidelines/strategies.", "1. Study interesting and important phenomena in your area of expertise. Why not study something that is big and powerful and that matters?", "2. Write well and tell a compelling story. People want to read your papers as a pleasure, not a duty.", "3. Develop one or more interesting theories that involve several phenomena. Theoretical and review articles are much more highly cited than empirical articles.", "4. Write synthetic reviews in appropriate journals or book chapters.", "5. Give powerful talks that tell an interesting study. Adapt your study but do not imitate someone when your skillset is not comparable.", "6. As you gain experience, write a book-only hope for fame that is more than fleeting. Prizes will follow because citations are as good an index as we have of fame.", "7. We still have an excellent mental image of the painting after seeing it hundreds of times.", "If you doubt the claim that fame in psychology is fleeting, how often have you read papers from 100 years in the past 2 years? The answer is probably 0 or close to it.", "We can predict that a century from now, current publications from our era will be read at the same rate."], "Quote": "\u201cFame is local, both by area and by time. This point has been made by scholars in other contexts (usually politics or other historical figures), but it is as true of psychology as of any other field. As with other writers in this series, the best advice is to do the research, the writing, and the teaching that you are passionate about. Fame may or may not come for a time, but should not be an all-consuming concern. Even if it comes, it will soon fade away\u201d \u00a0(p.887)", "Abstract": "Fame in psychology, as in all arenas, is a local phenomenon. Psychologists (and probably academics in all fields) often first become well known for studying a subfield of an area (say, the study of attention in cognitive psychology, or even certain tasks used to study attention). Later, the researcher may become famous within cognitive psychology. In a few cases, researchers break out of a discipline to become famous across psychology and (more rarely still) even outside the confines of academe. The progression is slow and uneven. Fame is also temporally constricted. The most famous psychologists today will be forgotten in less than a century, just as the greats from the era of World War I are rarely read or remembered today. Freud and a few others represent exceptions to the rule, but generally fame is fleeting and each generation seems to dispense with the lessons learned by previous ones to claim their place in the sun.", "Reference": "Roediger III, H. L. (2016). Varieties of fame in psychology. Perspectives on Psychological Science, 11(6), 882-887. https://doi.org/10.1177/1745691616662457", "You_may_also_be_interested_in": [{"Relevant_ref": "The Focus on Fame distorts Science (Innes-Ker, 2017)", "href": "#h.fiwxl1sbqyrw"}, {"Relevant_ref": "Fame: I\u2019m Skeptical (Ferreira, 2017)", "href": "#h.vkb9jz1fj01l"}, {"Relevant_ref": "Let\u2019s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017)", "href": "#h.lr9irendycbv"}, {"Relevant_ref": "\u201cFame\u201d is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017)", "href": "#h.4h5yr3mehhqy"}, {"Relevant_ref": "Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017)", "href": "#h.1g4r1nf76afn"}, {"Relevant_ref": "Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016)", "href": "#h.6y2j1ai8fhzp"}, {"Relevant_ref": "Against Eminence (Vazire, 2017)", "href": "#h.ixq69pvnhnnt"}, {"Relevant_ref": "Giving Credit Where Credit\u2019s Due: Why It\u2019s So Hard to Do in Psychological Science (Simonton, 2016)", "href": "#h.gqzauu6c437o"}, {"Relevant_ref": "Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016)", "href": "#h.hx1ixz9w9khb"}, {"Relevant_ref": "Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016)", "href": "#h.xxdqanhbam8s"}, {"Relevant_ref": "Improving Departments of Psychology (Diener, 2016)", "href": "#h.plqv88khmyxu"}, {"Relevant_ref": "Scientific Eminence: Where Are the Women? (Eagly & Miller, 2016)", "href": "#h.fztee3dibdn9"}, {"Relevant_ref": "Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016)", "href": "#h.q2na8s8kahet"}]}, "summary_128": {"Title": "Scientific Eminence: Where Are the Women? (Eagly & Miller, 2016) \u00a0\u233a", "Id": "h.fztee3dibdn9", "Main_Takeaways": ["Women\u2019s scientific contributions in psychology may not be as numerous or influential as those of men.", "What is the magnitude of the current eminence gender gap?", "Women\u2019s modest inroads into this list of eminent psychologists deserve respect, given this lag between obtaining a doctorate and attaining eminence and formidable barriers that women once faced in pursuing scientific careers.", "Psychologists judge eminence by observing signs such as memberships in selective societies, career scientific achievement awards and honorary degrees.", "Whether men exceed women on both quantity and impact of their publication underlies h index.", "Whether these metrics are tainted by unfair bias against women.", "Whether h index identifies potential socio-cultural and individual causes of the eminence gap.", "Women\u2019s publications are cited less than men. This gap was larger in psychology.", "Women received 20% fewer in psychology varying across subfields.", "Gender gap on h index and similar metrics has two sources: women publish less than men and articles receive fewer citations.", "Metrics assessing scientific eminence may be tainted by prejudicial bias against female scientists in obtaining grant support, publishing papers, or gaining citations of published papers.", "If psychologists are disadvantaged in publishing their work, bias may be limited to culturally masculine topics or male-dominated research areas.", "Such topics and are no doubt becoming rarer in psychology, given women receive most US doctorates.", "Men\u2019s greater overall citations reflect higher rates of self-citation, women self-cite less often.", "This reflects men\u2019s larger corpus of their own citable papers.", "Prejudicial gender bias is limited and presents ambiguity given most studies are correlational instead of experimental.", "Little is known about possible gender bias in awards for scientific eminence such as science prizes and honorary degrees, which are imperfect indicators of the importance of scientists\u2019 contributions.", "Female scientists\u2019 lesser rates of publication and citation reflect causes other than biases.", "Broader socio-cultural factors shape individual identities and motivations.", "Nature and nurture affects role occupancies so men and women are differently distributed into social roles.", "Women excel in communal qualities of warmth and concern for others and for men to excel in agentic qualities of assertiveness and mastery.", "Women are over-represented in less research intensive but more in teaching-intensive ranks and part-time positions.", "Gender norms discourage female agency may disadvantage to gain status in departmental and disciplinary networks and garner resources.", "Stereotypes erode women\u2019s confidence in ability to become highly successful scientists.", "Eminence gender gaps in psychology and other sciences shrink further over time as new cohorts of scientists advance in their careers.", "Women\u2019s representation among PhD earners has increased dramatically over recent decades."], "Abstract": "Women are sparsely represented among psychologists honored for scientific eminence. However, most currently eminent psychologists started their careers when far fewer women pursued training in psychological science. Now that women earn the majority of psychology Ph.D.\u2019s, will they predominate in the next generation\u2019s cadre of eminent psychologists? Comparing currently active female and male psychology professors on publication metrics such as the h index provides clues for answering this question. Men outperform women on the h index and its two components: scientific productivity and citations of contributions. To interpret these gender gaps, we first evaluate whether publication metrics are affected by gender bias in obtaining grant support, publishing papers, or gaining citations of published papers. We also consider whether women\u2019s chances of attaining eminence are compromised by two intertwined sets of influences: (a) gender bias stemming from social norms pertaining to gender and to science and (b) the choices that individual psychologists make in pursuing their careers.", "Reference": "Eagly, A. H., & Miller, D. I. (2016). Scientific eminence: Where are the women?. Perspectives on Psychological Science, 11(6), 899-904. https://doi.org/10.1177/1745691616663918", "You_may_also_be_interested_in": [{"Relevant_ref": "The Gender Gap: Who Is (and Is Not) Included on Graduate-Level Syllabi in Social/Personality Psychology (Skitka et al., 2020)", "href": "#h.j2lw9gk12js0"}, {"Relevant_ref": "Leveraging a collaborative consortium model of mentee/mentor training to foster career progression of under-represented post-doctoral researchers and promote institutional diversity and inclusion (Risner et al., 2020)", "href": "#h.um3ggunxziz6"}, {"Relevant_ref": "The Focus on Fame distorts Science (Innes-Ker, 2017)", "href": "#h.fiwxl1sbqyrw"}, {"Relevant_ref": "Fame: I\u2019m Skeptical (Ferreira, 2017)", "href": "#h.vkb9jz1fj01l"}, {"Relevant_ref": "Let\u2019s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017)", "href": "#h.lr9irendycbv"}, {"Relevant_ref": "\u201cFame\u201d is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017)", "href": "#h.4h5yr3mehhqy"}, {"Relevant_ref": "Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017)", "href": "#h.1g4r1nf76afn"}, {"Relevant_ref": "Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016)", "href": "#h.6y2j1ai8fhzp"}, {"Relevant_ref": "Against Eminence (Vazire, 2017)", "href": "#h.ixq69pvnhnnt"}, {"Relevant_ref": "Varieties of Fame in Psychology (Roediger III, 2016)", "href": "#h.ubwvvcnua9qq"}, {"Relevant_ref": "Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016)", "href": "#h.hx1ixz9w9khb"}, {"Relevant_ref": "Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016)", "href": "#h.xxdqanhbam8s"}, {"Relevant_ref": "Improving Departments of Psychology (Diener, 2016)", "href": "#h.plqv88khmyxu"}, {"Relevant_ref": "Giving Credit Where Credit\u2019s Due: Why It\u2019s So Hard to Do in Psychological Science (Simonton, 2016)", "href": "#h.gqzauu6c437o"}, {"Relevant_ref": "Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016)", "href": "#h.q2na8s8kahet"}]}, "summary_129": {"Title": "Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016)", "Id": "h.q2na8s8kahet", "Main_Takeaways": ["Fame and desire for a legacy sees meaning in one\u2019s existence.", "Scientists are not the only group driven by a desire to be famous. What does it mean to be famous in science and how do we measure scientific fame?", "There is intrinsic motivation to follow one\u2019s interest, curiosity, gut and intuition for important and undiscovered topics, while there is extrinsic motivation to follow money, grants and/or what is being published in top-tier journals.", "There is a continuum of fame: mundane or imitative science-someone conducts a replication or slight advance of already published research.", "Its impact is little more than personal, influencing the person conducting it but few other people might happen in an undergraduate research method class.", "Normal science is when one takes an idea or theory from within an existing theoretical paradigm and tests it.", "Most scientific research falls in the normal category. Its impact is regional and/or narrowly national.", "We have creative science-moderate to high impact science, heavily cited by other scholars in the field and sometimes garner regional, national or even international awards.", "Finally, there is rare transformational/revolutionary science that changes the entire field and whose impact is both internal and historic.", "If a peer-reviewed article is the currency of scientific career, funding is its bread and butter. Research is not possible without finding increasing amounts for most scientists.", "Generative publications are not only highly cited themselves but also generate other words that are highly cited.", "If they generate enough new works of high impact, they can be transformative.", "Skewed distributions of creative achievement confirm what we know: Creative output is a rare commodity.", "Once published, articles are either ignored or exert some kind of influence on the field.", "Publication and citation counts are reliable and robust measures of creative output in science.", "Scientists cite any and all work affected their current research and this appears to not be the case. Only about 30% of influences are cited.", "Papers with several authors are more likely to be cited due to greater exposure.", "Each author brings in their own network of scientific relations, and this paper is available to a wider network of researchers.", "Other more integrated measures of productivity have been developed to correct some problems. H index: when an author of N articles has h number of publications cited at least h number of times and rest of articles receive no more than citations.", "Traditional and citation-based metrics of impact is time lag between when an article is published and when citation indexes catch up.", "Altmetrics measures impact derived from online and social media data concerning articles. Altmetrics assess article outcomes such as the number of times an article is viewed, liked, downloaded, discussed, saved, cited, tweeted, blogged, or recommended.", "Altmetric data is faster than traditional citation count and h index. Altmetric data is counted immediately upon publication with real-time updates at any given time.", "Publications are necessary but not sufficient conditions for citations, those who publish the most are cited the most.", "It is important to remember there are individuals who publish a lot but not get cited, and those who publish not much but are heavily cited.", "One can do very good work but the field may or may not pay much attention to it.", "Many heavily cited papers make a methodological or statistical advance and are of practical, not theoretical, importance.", "Psychologists would better understand the difference between individual success and disciplinary success. What is good for one\u2019s career is not always what is good for science.", "Psychology may be too quick to accept findings of single studies as true. Researchers have begun to make recommendations to authors, editors, and instructors of research methodology to increase replicability such as pre-registering predictions by increasing transparency and clearly justify sample size and publish raw data.", "Most psychological scientists find a way to marry their intrinsic interests with its extrinsic reward and impact."], "Quote": "\u201cFinding that sweet spot between the two extremes of joy and recognition may be the best definition of success in science that we can come up with. So if I were to recommend a strategy for up and coming scientists it might be this: develop a research program that combines intrinsic fascination and interest with extrinsic recognition and career advancement. Follow your heart and your head. Explore and develop the riskier, more potentially transformative and creative lines of research at the same time that you develop the safer, more fundable ideas. This might occur by developing two separate lines of research, or better yet, by finding one research program that is both intrinsically motivated and then other people also recognize, appreciate, and reward you for it. If you can do both of these, you stand the best chance of surviving, succeeding, and maybe even becoming famous in the competitive world of academic psychological science\u201d (p.897)", "Abstract": "In this article, I argue that scientific fame and impact exists on a continuum from the mundane to the transformative/ revolutionary. Ideally, one achieves fame and impact in science by synthesizing two extreme career prototypes: intrinsic and extrinsic research. The former is guided by interest, curiosity, passion, gut, and intuition for important untapped topics. The latter is guided by money, grants, and/or what is being published in top-tier journals. Assessment of fame and impact in science ultimately rests on productivity (publication) and some variation of its impact (citations). In addition to those traditional measures of impact, there are some relatively new metrics (e.g., the h index and altmetrics). If psychology is to achieve consensual cumulative progress and better rates of replication, I propose that upcoming psychologists would do well to understand that success is not equal to fame and that individual career success is not necessarily the same as disciplinary success. Finally, if one is to have a successful and perhaps even famous career in psychological science, a good strategy would be to synthesize intrinsic and extrinsic motives for one\u2019s research.", "Reference": "Feist, G. J. (2016). Intrinsic and extrinsic science: A dialectic of scientific fame. Perspectives on Psychological Science, 11(6), 893-898. https://doi.org/10.1177/1745691616660535", "You_may_also_be_interested_in": [{"Relevant_ref": "The Focus on Fame distorts Science (Innes-Ker, 2017)", "href": "#h.fiwxl1sbqyrw"}, {"Relevant_ref": "Fame: I\u2019m Skeptical (Ferreira, 2017)", "href": "#h.vkb9jz1fj01l"}, {"Relevant_ref": "Let\u2019s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017)", "href": "#h.lr9irendycbv"}, {"Relevant_ref": "\u201cFame\u201d is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017)", "href": "#h.4h5yr3mehhqy"}, {"Relevant_ref": "Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017)", "href": "#h.1g4r1nf76afn"}, {"Relevant_ref": "Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016)", "href": "#h.6y2j1ai8fhzp"}, {"Relevant_ref": "Against Eminence (Vazire, 2017)", "href": "#h.ixq69pvnhnnt"}, {"Relevant_ref": "Giving Credit Where Credit\u2019s Due: Why It\u2019s So Hard to Do in Psychological Science (Simonton, 2016)", "href": "#h.gqzauu6c437o"}, {"Relevant_ref": "Scientific Eminence: Where Are the Women? (Eagly & Miller, 2016)", "href": "#h.fztee3dibdn9"}, {"Relevant_ref": "Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016)", "href": "#h.hx1ixz9w9khb"}, {"Relevant_ref": "Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016)", "href": "#h.xxdqanhbam8s"}, {"Relevant_ref": "Varieties of Fame in Psychology (Roediger III, 2016)", "href": "#h.ubwvvcnua9qq"}, {"Relevant_ref": "Improving Departments of Psychology (Diener, 2016)", "href": "#h.plqv88khmyxu"}]}}